{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb831a3-5afc-4f55-83a6-e666ce20ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b56ec9c0-453d-49f1-a1de-051139c3acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmengine.evaluator import BaseMetric\n",
    "from mmengine.model import BaseModel\n",
    "from mmengine.runner import Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "0b8670d4-8a86-4033-a482-a3e6928744e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef2404a7ff14c61be53564a9ceeadf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar10/cifar-10-python.tar.gz to data/cifar10\n",
      "Files already downloaded and verified\n",
      "11/24 00:27:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 2146494974\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda-11.4\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.100\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.10.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.9.0\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 2146494974\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "11/24 00:28:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "11/24 00:28:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "11/24 00:28:00 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "11/24 00:28:00 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The prefix is not set in metric class Accuracy.\n",
      "11/24 00:28:00 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "11/24 00:28:02 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "11/24 00:28:02 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "11/24 00:28:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/user/mmpretrain/work_dir.\n",
      "11/24 00:28:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  10/1563]  lr: 1.0000e-03  eta: 1:08:01  time: 0.5230  data_time: 0.0199  memory: 2032  loss: 6.8924\n",
      "11/24 00:28:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  20/1563]  lr: 1.0000e-03  eta: 0:35:29  time: 0.0235  data_time: 0.0072  memory: 2032  loss: 6.7177\n",
      "11/24 00:28:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  30/1563]  lr: 1.0000e-03  eta: 0:24:43  time: 0.0250  data_time: 0.0084  memory: 2032  loss: 4.7876\n",
      "11/24 00:28:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  40/1563]  lr: 1.0000e-03  eta: 0:19:18  time: 0.0243  data_time: 0.0073  memory: 2032  loss: 3.8443\n",
      "11/24 00:28:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  50/1563]  lr: 1.0000e-03  eta: 0:16:01  time: 0.0232  data_time: 0.0069  memory: 2032  loss: 3.0960\n",
      "11/24 00:28:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  60/1563]  lr: 1.0000e-03  eta: 0:13:48  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.6734\n",
      "11/24 00:28:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  70/1563]  lr: 1.0000e-03  eta: 0:12:14  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.5127\n",
      "11/24 00:28:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  80/1563]  lr: 1.0000e-03  eta: 0:11:05  time: 0.0246  data_time: 0.0078  memory: 2032  loss: 2.4758\n",
      "11/24 00:28:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  90/1563]  lr: 1.0000e-03  eta: 0:10:09  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.4397\n",
      "11/24 00:28:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 100/1563]  lr: 1.0000e-03  eta: 0:09:25  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.4320\n",
      "11/24 00:28:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 110/1563]  lr: 1.0000e-03  eta: 0:08:48  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3972\n",
      "11/24 00:28:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 120/1563]  lr: 1.0000e-03  eta: 0:08:18  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.4398\n",
      "11/24 00:28:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 130/1563]  lr: 1.0000e-03  eta: 0:07:52  time: 0.0233  data_time: 0.0069  memory: 2032  loss: 2.3890\n",
      "11/24 00:28:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 140/1563]  lr: 1.0000e-03  eta: 0:07:30  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3821\n",
      "11/24 00:28:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 150/1563]  lr: 1.0000e-03  eta: 0:07:11  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.4228\n",
      "11/24 00:28:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 160/1563]  lr: 1.0000e-03  eta: 0:06:54  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.4100\n",
      "11/24 00:28:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 170/1563]  lr: 1.0000e-03  eta: 0:06:39  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.4223\n",
      "11/24 00:28:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 180/1563]  lr: 1.0000e-03  eta: 0:06:25  time: 0.0218  data_time: 0.0060  memory: 2032  loss: 2.4246\n",
      "11/24 00:28:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 190/1563]  lr: 1.0000e-03  eta: 0:06:14  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3967\n",
      "11/24 00:28:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 200/1563]  lr: 1.0000e-03  eta: 0:06:03  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3782\n",
      "11/24 00:28:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 210/1563]  lr: 1.0000e-03  eta: 0:05:53  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.3889\n",
      "11/24 00:28:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 220/1563]  lr: 1.0000e-03  eta: 0:05:44  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3973\n",
      "11/24 00:28:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 230/1563]  lr: 1.0000e-03  eta: 0:05:36  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3916\n",
      "11/24 00:28:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 240/1563]  lr: 1.0000e-03  eta: 0:05:28  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3913\n",
      "11/24 00:28:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 250/1563]  lr: 1.0000e-03  eta: 0:05:22  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.4753\n",
      "11/24 00:28:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 260/1563]  lr: 1.0000e-03  eta: 0:05:15  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.4256\n",
      "11/24 00:28:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 270/1563]  lr: 1.0000e-03  eta: 0:05:09  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.4249\n",
      "11/24 00:28:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 280/1563]  lr: 1.0000e-03  eta: 0:05:04  time: 0.0233  data_time: 0.0068  memory: 2032  loss: 2.4379\n",
      "11/24 00:28:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 290/1563]  lr: 1.0000e-03  eta: 0:04:59  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.4200\n",
      "11/24 00:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 300/1563]  lr: 1.0000e-03  eta: 0:04:54  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.4136\n",
      "11/24 00:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 310/1563]  lr: 1.0000e-03  eta: 0:04:50  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3965\n",
      "11/24 00:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 320/1563]  lr: 1.0000e-03  eta: 0:04:46  time: 0.0227  data_time: 0.0067  memory: 2032  loss: 2.3519\n",
      "11/24 00:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 330/1563]  lr: 1.0000e-03  eta: 0:04:41  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3813\n",
      "11/24 00:28:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 340/1563]  lr: 1.0000e-03  eta: 0:04:38  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3839\n",
      "11/24 00:28:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 350/1563]  lr: 1.0000e-03  eta: 0:04:34  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3583\n",
      "11/24 00:28:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 360/1563]  lr: 1.0000e-03  eta: 0:04:31  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3655\n",
      "11/24 00:28:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 370/1563]  lr: 1.0000e-03  eta: 0:04:27  time: 0.0227  data_time: 0.0068  memory: 2032  loss: 2.3571\n",
      "11/24 00:28:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 380/1563]  lr: 1.0000e-03  eta: 0:04:24  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3645\n",
      "11/24 00:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 390/1563]  lr: 1.0000e-03  eta: 0:04:21  time: 0.0223  data_time: 0.0065  memory: 2032  loss: 2.3648\n",
      "11/24 00:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 400/1563]  lr: 1.0000e-03  eta: 0:04:19  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3980\n",
      "11/24 00:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 410/1563]  lr: 1.0000e-03  eta: 0:04:16  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.4083\n",
      "11/24 00:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 420/1563]  lr: 1.0000e-03  eta: 0:04:13  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.3778\n",
      "11/24 00:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 430/1563]  lr: 1.0000e-03  eta: 0:04:11  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3951\n",
      "11/24 00:28:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 440/1563]  lr: 1.0000e-03  eta: 0:04:09  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3967\n",
      "11/24 00:28:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 450/1563]  lr: 1.0000e-03  eta: 0:04:07  time: 0.0234  data_time: 0.0069  memory: 2032  loss: 2.3975\n",
      "11/24 00:28:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 460/1563]  lr: 1.0000e-03  eta: 0:04:05  time: 0.0244  data_time: 0.0077  memory: 2032  loss: 2.3766\n",
      "11/24 00:28:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 470/1563]  lr: 1.0000e-03  eta: 0:04:03  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3705\n",
      "11/24 00:28:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 480/1563]  lr: 1.0000e-03  eta: 0:04:01  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.4095\n",
      "11/24 00:28:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 490/1563]  lr: 1.0000e-03  eta: 0:03:59  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3785\n",
      "11/24 00:28:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 500/1563]  lr: 1.0000e-03  eta: 0:03:57  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3850\n",
      "11/24 00:28:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 510/1563]  lr: 1.0000e-03  eta: 0:03:55  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3611\n",
      "11/24 00:28:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 520/1563]  lr: 1.0000e-03  eta: 0:03:53  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3502\n",
      "11/24 00:28:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 530/1563]  lr: 1.0000e-03  eta: 0:03:52  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3743\n",
      "11/24 00:28:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 540/1563]  lr: 1.0000e-03  eta: 0:03:50  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3612\n",
      "11/24 00:28:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 550/1563]  lr: 1.0000e-03  eta: 0:03:48  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3948\n",
      "11/24 00:28:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 560/1563]  lr: 1.0000e-03  eta: 0:03:47  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3649\n",
      "11/24 00:28:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 570/1563]  lr: 1.0000e-03  eta: 0:03:45  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.3860\n",
      "11/24 00:28:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 580/1563]  lr: 1.0000e-03  eta: 0:03:44  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3946\n",
      "11/24 00:28:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 590/1563]  lr: 1.0000e-03  eta: 0:03:43  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3660\n",
      "11/24 00:28:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 600/1563]  lr: 1.0000e-03  eta: 0:03:41  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3782\n",
      "11/24 00:28:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 610/1563]  lr: 1.0000e-03  eta: 0:03:40  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3573\n",
      "11/24 00:28:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 620/1563]  lr: 1.0000e-03  eta: 0:03:39  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3624\n",
      "11/24 00:28:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 630/1563]  lr: 1.0000e-03  eta: 0:03:37  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3726\n",
      "11/24 00:28:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 640/1563]  lr: 1.0000e-03  eta: 0:03:36  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3661\n",
      "11/24 00:28:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 650/1563]  lr: 1.0000e-03  eta: 0:03:35  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3707\n",
      "11/24 00:28:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 660/1563]  lr: 1.0000e-03  eta: 0:03:34  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3641\n",
      "11/24 00:28:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 670/1563]  lr: 1.0000e-03  eta: 0:03:33  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3542\n",
      "11/24 00:28:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 680/1563]  lr: 1.0000e-03  eta: 0:03:32  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3618\n",
      "11/24 00:28:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 690/1563]  lr: 1.0000e-03  eta: 0:03:31  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3879\n",
      "11/24 00:28:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 700/1563]  lr: 1.0000e-03  eta: 0:03:29  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3555\n",
      "11/24 00:28:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 710/1563]  lr: 1.0000e-03  eta: 0:03:28  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3431\n",
      "11/24 00:28:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 720/1563]  lr: 1.0000e-03  eta: 0:03:27  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3528\n",
      "11/24 00:28:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 730/1563]  lr: 1.0000e-03  eta: 0:03:26  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3755\n",
      "11/24 00:28:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 740/1563]  lr: 1.0000e-03  eta: 0:03:25  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3676\n",
      "11/24 00:28:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 750/1563]  lr: 1.0000e-03  eta: 0:03:25  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3436\n",
      "11/24 00:28:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 760/1563]  lr: 1.0000e-03  eta: 0:03:24  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3255\n",
      "11/24 00:28:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 770/1563]  lr: 1.0000e-03  eta: 0:03:23  time: 0.0235  data_time: 0.0074  memory: 2032  loss: 2.3663\n",
      "11/24 00:28:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 780/1563]  lr: 1.0000e-03  eta: 0:03:22  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3332\n",
      "11/24 00:28:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 790/1563]  lr: 1.0000e-03  eta: 0:03:21  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3858\n",
      "11/24 00:28:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 800/1563]  lr: 1.0000e-03  eta: 0:03:20  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3758\n",
      "11/24 00:28:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 810/1563]  lr: 1.0000e-03  eta: 0:03:19  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3513\n",
      "11/24 00:28:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 820/1563]  lr: 1.0000e-03  eta: 0:03:19  time: 0.0227  data_time: 0.0064  memory: 2032  loss: 2.3672\n",
      "11/24 00:28:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 830/1563]  lr: 1.0000e-03  eta: 0:03:18  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3740\n",
      "11/24 00:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 840/1563]  lr: 1.0000e-03  eta: 0:03:17  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3481\n",
      "11/24 00:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 850/1563]  lr: 1.0000e-03  eta: 0:03:16  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3528\n",
      "11/24 00:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 860/1563]  lr: 1.0000e-03  eta: 0:03:16  time: 0.0231  data_time: 0.0066  memory: 2032  loss: 2.3457\n",
      "11/24 00:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 870/1563]  lr: 1.0000e-03  eta: 0:03:15  time: 0.0243  data_time: 0.0077  memory: 2032  loss: 2.3473\n",
      "11/24 00:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 880/1563]  lr: 1.0000e-03  eta: 0:03:14  time: 0.0245  data_time: 0.0077  memory: 2032  loss: 2.3595\n",
      "11/24 00:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 890/1563]  lr: 1.0000e-03  eta: 0:03:14  time: 0.0226  data_time: 0.0067  memory: 2032  loss: 2.3892\n",
      "11/24 00:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 900/1563]  lr: 1.0000e-03  eta: 0:03:13  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3628\n",
      "11/24 00:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 910/1563]  lr: 1.0000e-03  eta: 0:03:12  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3515\n",
      "11/24 00:28:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 920/1563]  lr: 1.0000e-03  eta: 0:03:12  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3627\n",
      "11/24 00:28:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 930/1563]  lr: 1.0000e-03  eta: 0:03:11  time: 0.0236  data_time: 0.0073  memory: 2032  loss: 2.3814\n",
      "11/24 00:28:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 940/1563]  lr: 1.0000e-03  eta: 0:03:10  time: 0.0245  data_time: 0.0079  memory: 2032  loss: 2.3508\n",
      "11/24 00:28:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 950/1563]  lr: 1.0000e-03  eta: 0:03:10  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3618\n",
      "11/24 00:28:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 960/1563]  lr: 1.0000e-03  eta: 0:03:09  time: 0.0244  data_time: 0.0078  memory: 2032  loss: 2.3401\n",
      "11/24 00:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 970/1563]  lr: 1.0000e-03  eta: 0:03:09  time: 0.0249  data_time: 0.0082  memory: 2032  loss: 2.3593\n",
      "11/24 00:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 980/1563]  lr: 1.0000e-03  eta: 0:03:08  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3625\n",
      "11/24 00:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 990/1563]  lr: 1.0000e-03  eta: 0:03:08  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3091\n",
      "11/24 00:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1000/1563]  lr: 1.0000e-03  eta: 0:03:07  time: 0.0230  data_time: 0.0069  memory: 2032  loss: 2.3797\n",
      "11/24 00:28:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1010/1563]  lr: 1.0000e-03  eta: 0:03:06  time: 0.0241  data_time: 0.0077  memory: 2032  loss: 2.3293\n",
      "11/24 00:28:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1020/1563]  lr: 1.0000e-03  eta: 0:03:06  time: 0.0231  data_time: 0.0069  memory: 2032  loss: 2.3482\n",
      "11/24 00:28:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1030/1563]  lr: 1.0000e-03  eta: 0:03:05  time: 0.0226  data_time: 0.0064  memory: 2032  loss: 2.3439\n",
      "11/24 00:28:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1040/1563]  lr: 1.0000e-03  eta: 0:03:05  time: 0.0233  data_time: 0.0068  memory: 2032  loss: 2.3907\n",
      "11/24 00:28:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1050/1563]  lr: 1.0000e-03  eta: 0:03:04  time: 0.0244  data_time: 0.0074  memory: 2032  loss: 2.3630\n",
      "11/24 00:28:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1060/1563]  lr: 1.0000e-03  eta: 0:03:04  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3983\n",
      "11/24 00:28:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1070/1563]  lr: 1.0000e-03  eta: 0:03:03  time: 0.0231  data_time: 0.0070  memory: 2032  loss: 2.3570\n",
      "11/24 00:28:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1080/1563]  lr: 1.0000e-03  eta: 0:03:03  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3469\n",
      "11/24 00:28:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1090/1563]  lr: 1.0000e-03  eta: 0:03:02  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3477\n",
      "11/24 00:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1100/1563]  lr: 1.0000e-03  eta: 0:03:01  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3416\n",
      "11/24 00:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1110/1563]  lr: 1.0000e-03  eta: 0:03:01  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3707\n",
      "11/24 00:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1120/1563]  lr: 1.0000e-03  eta: 0:03:00  time: 0.0239  data_time: 0.0074  memory: 2032  loss: 2.3745\n",
      "11/24 00:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1130/1563]  lr: 1.0000e-03  eta: 0:03:00  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3777\n",
      "11/24 00:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1140/1563]  lr: 1.0000e-03  eta: 0:02:59  time: 0.0241  data_time: 0.0073  memory: 2032  loss: 2.3493\n",
      "11/24 00:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1150/1563]  lr: 1.0000e-03  eta: 0:02:59  time: 0.0249  data_time: 0.0077  memory: 2032  loss: 2.3563\n",
      "11/24 00:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1160/1563]  lr: 1.0000e-03  eta: 0:02:58  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3359\n",
      "11/24 00:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1170/1563]  lr: 1.0000e-03  eta: 0:02:58  time: 0.0230  data_time: 0.0067  memory: 2032  loss: 2.3386\n",
      "11/24 00:28:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1180/1563]  lr: 1.0000e-03  eta: 0:02:58  time: 0.0261  data_time: 0.0085  memory: 2032  loss: 2.3516\n",
      "11/24 00:28:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1190/1563]  lr: 1.0000e-03  eta: 0:02:57  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3700\n",
      "11/24 00:28:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1200/1563]  lr: 1.0000e-03  eta: 0:02:57  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3296\n",
      "11/24 00:28:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1210/1563]  lr: 1.0000e-03  eta: 0:02:56  time: 0.0253  data_time: 0.0083  memory: 2032  loss: 2.3537\n",
      "11/24 00:28:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1220/1563]  lr: 1.0000e-03  eta: 0:02:56  time: 0.0227  data_time: 0.0066  memory: 2032  loss: 2.3439\n",
      "11/24 00:28:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1230/1563]  lr: 1.0000e-03  eta: 0:02:55  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3720\n",
      "11/24 00:28:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1240/1563]  lr: 1.0000e-03  eta: 0:02:55  time: 0.0246  data_time: 0.0078  memory: 2032  loss: 2.3129\n",
      "11/24 00:28:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1250/1563]  lr: 1.0000e-03  eta: 0:02:54  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3494\n",
      "11/24 00:28:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1260/1563]  lr: 1.0000e-03  eta: 0:02:54  time: 0.0229  data_time: 0.0066  memory: 2032  loss: 2.3779\n",
      "11/24 00:28:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1270/1563]  lr: 1.0000e-03  eta: 0:02:54  time: 0.0252  data_time: 0.0082  memory: 2032  loss: 2.3618\n",
      "11/24 00:28:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1280/1563]  lr: 1.0000e-03  eta: 0:02:53  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3547\n",
      "11/24 00:28:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1290/1563]  lr: 1.0000e-03  eta: 0:02:53  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3312\n",
      "11/24 00:28:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1300/1563]  lr: 1.0000e-03  eta: 0:02:52  time: 0.0243  data_time: 0.0076  memory: 2032  loss: 2.3338\n",
      "11/24 00:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1310/1563]  lr: 1.0000e-03  eta: 0:02:52  time: 0.0234  data_time: 0.0071  memory: 2032  loss: 2.3377\n",
      "11/24 00:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1320/1563]  lr: 1.0000e-03  eta: 0:02:51  time: 0.0227  data_time: 0.0067  memory: 2032  loss: 2.3512\n",
      "11/24 00:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1330/1563]  lr: 1.0000e-03  eta: 0:02:51  time: 0.0247  data_time: 0.0079  memory: 2032  loss: 2.3496\n",
      "11/24 00:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1340/1563]  lr: 1.0000e-03  eta: 0:02:51  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3651\n",
      "11/24 00:28:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1350/1563]  lr: 1.0000e-03  eta: 0:02:50  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3777\n",
      "11/24 00:28:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1360/1563]  lr: 1.0000e-03  eta: 0:02:50  time: 0.0233  data_time: 0.0069  memory: 2032  loss: 2.3453\n",
      "11/24 00:28:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1370/1563]  lr: 1.0000e-03  eta: 0:02:50  time: 0.0281  data_time: 0.0099  memory: 2032  loss: 2.3668\n",
      "11/24 00:28:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1380/1563]  lr: 1.0000e-03  eta: 0:02:49  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3348\n",
      "11/24 00:28:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1390/1563]  lr: 1.0000e-03  eta: 0:02:49  time: 0.0226  data_time: 0.0067  memory: 2032  loss: 2.3585\n",
      "11/24 00:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1400/1563]  lr: 1.0000e-03  eta: 0:02:48  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3384\n",
      "11/24 00:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1410/1563]  lr: 1.0000e-03  eta: 0:02:48  time: 0.0243  data_time: 0.0074  memory: 2032  loss: 2.3653\n",
      "11/24 00:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1420/1563]  lr: 1.0000e-03  eta: 0:02:47  time: 0.0233  data_time: 0.0068  memory: 2032  loss: 2.3295\n",
      "11/24 00:28:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1430/1563]  lr: 1.0000e-03  eta: 0:02:47  time: 0.0233  data_time: 0.0070  memory: 2032  loss: 2.3254\n",
      "11/24 00:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1440/1563]  lr: 1.0000e-03  eta: 0:02:47  time: 0.0236  data_time: 0.0070  memory: 2032  loss: 2.3449\n",
      "11/24 00:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1450/1563]  lr: 1.0000e-03  eta: 0:02:46  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3416\n",
      "11/24 00:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1460/1563]  lr: 1.0000e-03  eta: 0:02:46  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3381\n",
      "11/24 00:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1470/1563]  lr: 1.0000e-03  eta: 0:02:45  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3394\n",
      "11/24 00:28:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1480/1563]  lr: 1.0000e-03  eta: 0:02:45  time: 0.0229  data_time: 0.0067  memory: 2032  loss: 2.3267\n",
      "11/24 00:28:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1490/1563]  lr: 1.0000e-03  eta: 0:02:45  time: 0.0247  data_time: 0.0078  memory: 2032  loss: 2.3357\n",
      "11/24 00:28:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1500/1563]  lr: 1.0000e-03  eta: 0:02:44  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3284\n",
      "11/24 00:28:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1510/1563]  lr: 1.0000e-03  eta: 0:02:44  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3324\n",
      "11/24 00:28:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1520/1563]  lr: 1.0000e-03  eta: 0:02:43  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3492\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1530/1563]  lr: 1.0000e-03  eta: 0:02:43  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3818\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1540/1563]  lr: 1.0000e-03  eta: 0:02:42  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3457\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1550/1563]  lr: 1.0000e-03  eta: 0:02:42  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3652\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1560/1563]  lr: 1.0000e-03  eta: 0:02:42  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3306\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:28:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "11/24 00:28:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "11/24 00:28:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 10/313]    eta: 0:01:26  time: 0.2859  data_time: 0.1951  memory: 2032  \n",
      "11/24 00:28:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 20/313]    eta: 0:00:44  time: 0.0167  data_time: 0.0135  memory: 1619  \n",
      "11/24 00:28:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 30/313]    eta: 0:00:29  time: 0.0067  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:28:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 40/313]    eta: 0:00:21  time: 0.0075  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:28:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 50/313]    eta: 0:00:17  time: 0.0084  data_time: 0.0048  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 60/313]    eta: 0:00:14  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 70/313]    eta: 0:00:11  time: 0.0076  data_time: 0.0047  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 80/313]    eta: 0:00:10  time: 0.0066  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 90/313]    eta: 0:00:08  time: 0.0068  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][100/313]    eta: 0:00:07  time: 0.0068  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][110/313]    eta: 0:00:06  time: 0.0070  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][120/313]    eta: 0:00:06  time: 0.0071  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][130/313]    eta: 0:00:05  time: 0.0079  data_time: 0.0049  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][140/313]    eta: 0:00:04  time: 0.0084  data_time: 0.0054  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][150/313]    eta: 0:00:04  time: 0.0072  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][160/313]    eta: 0:00:03  time: 0.0067  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][170/313]    eta: 0:00:03  time: 0.0066  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][180/313]    eta: 0:00:03  time: 0.0077  data_time: 0.0046  memory: 1619  \n",
      "11/24 00:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][190/313]    eta: 0:00:02  time: 0.0083  data_time: 0.0051  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][200/313]    eta: 0:00:02  time: 0.0109  data_time: 0.0069  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][210/313]    eta: 0:00:02  time: 0.0079  data_time: 0.0049  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][220/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][230/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][240/313]    eta: 0:00:01  time: 0.0083  data_time: 0.0047  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][250/313]    eta: 0:00:01  time: 0.0096  data_time: 0.0057  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][260/313]    eta: 0:00:00  time: 0.0089  data_time: 0.0054  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][270/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][280/313]    eta: 0:00:00  time: 0.0084  data_time: 0.0051  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][290/313]    eta: 0:00:00  time: 0.0079  data_time: 0.0046  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][300/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][310/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][313/313]    accuracy: 10.0000  data_time: 0.0110  time: 0.0168\n",
      "11/24 00:29:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  10/1563]  lr: 1.0000e-03  eta: 0:02:44  time: 0.0953  data_time: 0.0329  memory: 2032  loss: 2.3403\n",
      "11/24 00:29:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  20/1563]  lr: 1.0000e-03  eta: 0:02:44  time: 0.0229  data_time: 0.0070  memory: 2032  loss: 2.3476\n",
      "11/24 00:29:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  30/1563]  lr: 1.0000e-03  eta: 0:02:43  time: 0.0232  data_time: 0.0069  memory: 2032  loss: 2.3568\n",
      "11/24 00:29:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  40/1563]  lr: 1.0000e-03  eta: 0:02:43  time: 0.0255  data_time: 0.0082  memory: 2032  loss: 2.3444\n",
      "11/24 00:29:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  50/1563]  lr: 1.0000e-03  eta: 0:02:43  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.3293\n",
      "11/24 00:29:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  60/1563]  lr: 1.0000e-03  eta: 0:02:42  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3186\n",
      "11/24 00:29:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  70/1563]  lr: 1.0000e-03  eta: 0:02:42  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3629\n",
      "11/24 00:29:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  80/1563]  lr: 1.0000e-03  eta: 0:02:41  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3579\n",
      "11/24 00:29:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  90/1563]  lr: 1.0000e-03  eta: 0:02:41  time: 0.0237  data_time: 0.0070  memory: 2032  loss: 2.3576\n",
      "11/24 00:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 100/1563]  lr: 1.0000e-03  eta: 0:02:41  time: 0.0242  data_time: 0.0076  memory: 2032  loss: 2.3299\n",
      "11/24 00:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 110/1563]  lr: 1.0000e-03  eta: 0:02:40  time: 0.0234  data_time: 0.0073  memory: 2032  loss: 2.3592\n",
      "11/24 00:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 120/1563]  lr: 1.0000e-03  eta: 0:02:40  time: 0.0226  data_time: 0.0064  memory: 2032  loss: 2.3576\n",
      "11/24 00:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 130/1563]  lr: 1.0000e-03  eta: 0:02:40  time: 0.0242  data_time: 0.0076  memory: 2032  loss: 2.3453\n",
      "11/24 00:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 140/1563]  lr: 1.0000e-03  eta: 0:02:39  time: 0.0245  data_time: 0.0076  memory: 2032  loss: 2.3363\n",
      "11/24 00:29:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 150/1563]  lr: 1.0000e-03  eta: 0:02:39  time: 0.0235  data_time: 0.0072  memory: 2032  loss: 2.3365\n",
      "11/24 00:29:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 160/1563]  lr: 1.0000e-03  eta: 0:02:39  time: 0.0232  data_time: 0.0068  memory: 2032  loss: 2.3146\n",
      "11/24 00:29:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 170/1563]  lr: 1.0000e-03  eta: 0:02:38  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3195\n",
      "11/24 00:29:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 180/1563]  lr: 1.0000e-03  eta: 0:02:38  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.3366\n",
      "11/24 00:29:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 190/1563]  lr: 1.0000e-03  eta: 0:02:37  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3641\n",
      "11/24 00:29:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 200/1563]  lr: 1.0000e-03  eta: 0:02:37  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3298\n",
      "11/24 00:29:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 210/1563]  lr: 1.0000e-03  eta: 0:02:37  time: 0.0235  data_time: 0.0072  memory: 2032  loss: 2.3460\n",
      "11/24 00:29:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 220/1563]  lr: 1.0000e-03  eta: 0:02:36  time: 0.0261  data_time: 0.0088  memory: 2032  loss: 2.3263\n",
      "11/24 00:29:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 230/1563]  lr: 1.0000e-03  eta: 0:02:36  time: 0.0239  data_time: 0.0072  memory: 2032  loss: 2.3156\n",
      "11/24 00:29:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 240/1563]  lr: 1.0000e-03  eta: 0:02:36  time: 0.0245  data_time: 0.0077  memory: 2032  loss: 2.3737\n",
      "11/24 00:29:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 250/1563]  lr: 1.0000e-03  eta: 0:02:35  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3465\n",
      "11/24 00:29:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 260/1563]  lr: 1.0000e-03  eta: 0:02:35  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3378\n",
      "11/24 00:29:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 270/1563]  lr: 1.0000e-03  eta: 0:02:35  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3568\n",
      "11/24 00:29:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 280/1563]  lr: 1.0000e-03  eta: 0:02:34  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3360\n",
      "11/24 00:29:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 290/1563]  lr: 1.0000e-03  eta: 0:02:34  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3296\n",
      "11/24 00:29:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 300/1563]  lr: 1.0000e-03  eta: 0:02:33  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3712\n",
      "11/24 00:29:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 310/1563]  lr: 1.0000e-03  eta: 0:02:33  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3125\n",
      "11/24 00:29:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 320/1563]  lr: 1.0000e-03  eta: 0:02:33  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3567\n",
      "11/24 00:29:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 330/1563]  lr: 1.0000e-03  eta: 0:02:32  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3664\n",
      "11/24 00:29:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 340/1563]  lr: 1.0000e-03  eta: 0:02:32  time: 0.0231  data_time: 0.0068  memory: 2032  loss: 2.3531\n",
      "11/24 00:29:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 350/1563]  lr: 1.0000e-03  eta: 0:02:32  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3315\n",
      "11/24 00:29:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 360/1563]  lr: 1.0000e-03  eta: 0:02:31  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3357\n",
      "11/24 00:29:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 370/1563]  lr: 1.0000e-03  eta: 0:02:31  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3231\n",
      "11/24 00:29:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 380/1563]  lr: 1.0000e-03  eta: 0:02:30  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3617\n",
      "11/24 00:29:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 390/1563]  lr: 1.0000e-03  eta: 0:02:30  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3480\n",
      "11/24 00:29:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 400/1563]  lr: 1.0000e-03  eta: 0:02:30  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3423\n",
      "11/24 00:29:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 410/1563]  lr: 1.0000e-03  eta: 0:02:29  time: 0.0249  data_time: 0.0079  memory: 2032  loss: 2.3519\n",
      "11/24 00:29:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 420/1563]  lr: 1.0000e-03  eta: 0:02:29  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3377\n",
      "11/24 00:29:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 430/1563]  lr: 1.0000e-03  eta: 0:02:29  time: 0.0225  data_time: 0.0066  memory: 2032  loss: 2.3462\n",
      "11/24 00:29:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:29:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 440/1563]  lr: 1.0000e-03  eta: 0:02:28  time: 0.0252  data_time: 0.0082  memory: 2032  loss: 2.3379\n",
      "11/24 00:29:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 450/1563]  lr: 1.0000e-03  eta: 0:02:28  time: 0.0229  data_time: 0.0069  memory: 2032  loss: 2.3332\n",
      "11/24 00:29:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 460/1563]  lr: 1.0000e-03  eta: 0:02:28  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3630\n",
      "11/24 00:29:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 470/1563]  lr: 1.0000e-03  eta: 0:02:27  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3448\n",
      "11/24 00:29:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 480/1563]  lr: 1.0000e-03  eta: 0:02:27  time: 0.0236  data_time: 0.0071  memory: 2032  loss: 2.3353\n",
      "11/24 00:29:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 490/1563]  lr: 1.0000e-03  eta: 0:02:27  time: 0.0254  data_time: 0.0084  memory: 2032  loss: 2.3257\n",
      "11/24 00:29:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 500/1563]  lr: 1.0000e-03  eta: 0:02:27  time: 0.0240  data_time: 0.0074  memory: 2032  loss: 2.3430\n",
      "11/24 00:29:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 510/1563]  lr: 1.0000e-03  eta: 0:02:26  time: 0.0244  data_time: 0.0077  memory: 2032  loss: 2.3341\n",
      "11/24 00:29:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 520/1563]  lr: 1.0000e-03  eta: 0:02:26  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3305\n",
      "11/24 00:29:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 530/1563]  lr: 1.0000e-03  eta: 0:02:26  time: 0.0230  data_time: 0.0069  memory: 2032  loss: 2.3421\n",
      "11/24 00:29:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 540/1563]  lr: 1.0000e-03  eta: 0:02:25  time: 0.0234  data_time: 0.0071  memory: 2032  loss: 2.3289\n",
      "11/24 00:29:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 550/1563]  lr: 1.0000e-03  eta: 0:02:25  time: 0.0229  data_time: 0.0068  memory: 2032  loss: 2.3360\n",
      "11/24 00:29:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 560/1563]  lr: 1.0000e-03  eta: 0:02:25  time: 0.0231  data_time: 0.0067  memory: 2032  loss: 2.3408\n",
      "11/24 00:29:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 570/1563]  lr: 1.0000e-03  eta: 0:02:24  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3344\n",
      "11/24 00:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 580/1563]  lr: 1.0000e-03  eta: 0:02:24  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3281\n",
      "11/24 00:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 590/1563]  lr: 1.0000e-03  eta: 0:02:24  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3488\n",
      "11/24 00:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 600/1563]  lr: 1.0000e-03  eta: 0:02:23  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3168\n",
      "11/24 00:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 610/1563]  lr: 1.0000e-03  eta: 0:02:23  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3324\n",
      "11/24 00:29:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 620/1563]  lr: 1.0000e-03  eta: 0:02:23  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3402\n",
      "11/24 00:29:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 630/1563]  lr: 1.0000e-03  eta: 0:02:22  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3492\n",
      "11/24 00:29:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 640/1563]  lr: 1.0000e-03  eta: 0:02:22  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3495\n",
      "11/24 00:29:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 650/1563]  lr: 1.0000e-03  eta: 0:02:22  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3285\n",
      "11/24 00:29:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 660/1563]  lr: 1.0000e-03  eta: 0:02:21  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3147\n",
      "11/24 00:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 670/1563]  lr: 1.0000e-03  eta: 0:02:21  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3620\n",
      "11/24 00:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 680/1563]  lr: 1.0000e-03  eta: 0:02:21  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3353\n",
      "11/24 00:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 690/1563]  lr: 1.0000e-03  eta: 0:02:20  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3418\n",
      "11/24 00:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 700/1563]  lr: 1.0000e-03  eta: 0:02:20  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3388\n",
      "11/24 00:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 710/1563]  lr: 1.0000e-03  eta: 0:02:20  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3565\n",
      "11/24 00:29:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 720/1563]  lr: 1.0000e-03  eta: 0:02:19  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3703\n",
      "11/24 00:29:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 730/1563]  lr: 1.0000e-03  eta: 0:02:19  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3436\n",
      "11/24 00:29:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 740/1563]  lr: 1.0000e-03  eta: 0:02:19  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3406\n",
      "11/24 00:29:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 750/1563]  lr: 1.0000e-03  eta: 0:02:18  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3376\n",
      "11/24 00:29:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 760/1563]  lr: 1.0000e-03  eta: 0:02:18  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3437\n",
      "11/24 00:29:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 770/1563]  lr: 1.0000e-03  eta: 0:02:18  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3432\n",
      "11/24 00:29:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 780/1563]  lr: 1.0000e-03  eta: 0:02:17  time: 0.0244  data_time: 0.0080  memory: 2032  loss: 2.3463\n",
      "11/24 00:29:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 790/1563]  lr: 1.0000e-03  eta: 0:02:17  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.2970\n",
      "11/24 00:29:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 800/1563]  lr: 1.0000e-03  eta: 0:02:17  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3491\n",
      "11/24 00:29:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 810/1563]  lr: 1.0000e-03  eta: 0:02:16  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3265\n",
      "11/24 00:29:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 820/1563]  lr: 1.0000e-03  eta: 0:02:16  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3275\n",
      "11/24 00:29:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 830/1563]  lr: 1.0000e-03  eta: 0:02:16  time: 0.0229  data_time: 0.0067  memory: 2032  loss: 2.3409\n",
      "11/24 00:29:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 840/1563]  lr: 1.0000e-03  eta: 0:02:15  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3552\n",
      "11/24 00:29:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 850/1563]  lr: 1.0000e-03  eta: 0:02:15  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3497\n",
      "11/24 00:29:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 860/1563]  lr: 1.0000e-03  eta: 0:02:15  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3132\n",
      "11/24 00:29:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 870/1563]  lr: 1.0000e-03  eta: 0:02:14  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3466\n",
      "11/24 00:29:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 880/1563]  lr: 1.0000e-03  eta: 0:02:14  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3351\n",
      "11/24 00:29:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 890/1563]  lr: 1.0000e-03  eta: 0:02:14  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3169\n",
      "11/24 00:29:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 900/1563]  lr: 1.0000e-03  eta: 0:02:13  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3244\n",
      "11/24 00:29:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 910/1563]  lr: 1.0000e-03  eta: 0:02:13  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3740\n",
      "11/24 00:29:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 920/1563]  lr: 1.0000e-03  eta: 0:02:13  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3239\n",
      "11/24 00:29:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 930/1563]  lr: 1.0000e-03  eta: 0:02:13  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3469\n",
      "11/24 00:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 940/1563]  lr: 1.0000e-03  eta: 0:02:12  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3377\n",
      "11/24 00:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 950/1563]  lr: 1.0000e-03  eta: 0:02:12  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3639\n",
      "11/24 00:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 960/1563]  lr: 1.0000e-03  eta: 0:02:12  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3264\n",
      "11/24 00:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 970/1563]  lr: 1.0000e-03  eta: 0:02:11  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3307\n",
      "11/24 00:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 980/1563]  lr: 1.0000e-03  eta: 0:02:11  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3603\n",
      "11/24 00:29:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 990/1563]  lr: 1.0000e-03  eta: 0:02:11  time: 0.0231  data_time: 0.0066  memory: 2032  loss: 2.3225\n",
      "11/24 00:29:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1000/1563]  lr: 1.0000e-03  eta: 0:02:10  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3289\n",
      "11/24 00:29:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1010/1563]  lr: 1.0000e-03  eta: 0:02:10  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3616\n",
      "11/24 00:29:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1020/1563]  lr: 1.0000e-03  eta: 0:02:10  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3179\n",
      "11/24 00:29:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1030/1563]  lr: 1.0000e-03  eta: 0:02:10  time: 0.0243  data_time: 0.0082  memory: 2032  loss: 2.3302\n",
      "11/24 00:29:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1040/1563]  lr: 1.0000e-03  eta: 0:02:09  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3522\n",
      "11/24 00:29:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1050/1563]  lr: 1.0000e-03  eta: 0:02:09  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3226\n",
      "11/24 00:29:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1060/1563]  lr: 1.0000e-03  eta: 0:02:09  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3538\n",
      "11/24 00:29:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1070/1563]  lr: 1.0000e-03  eta: 0:02:08  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3331\n",
      "11/24 00:29:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1080/1563]  lr: 1.0000e-03  eta: 0:02:08  time: 0.0233  data_time: 0.0072  memory: 2032  loss: 2.3455\n",
      "11/24 00:29:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1090/1563]  lr: 1.0000e-03  eta: 0:02:08  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3513\n",
      "11/24 00:29:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1100/1563]  lr: 1.0000e-03  eta: 0:02:07  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3418\n",
      "11/24 00:29:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1110/1563]  lr: 1.0000e-03  eta: 0:02:07  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3319\n",
      "11/24 00:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1120/1563]  lr: 1.0000e-03  eta: 0:02:07  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3434\n",
      "11/24 00:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1130/1563]  lr: 1.0000e-03  eta: 0:02:07  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3278\n",
      "11/24 00:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1140/1563]  lr: 1.0000e-03  eta: 0:02:06  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3019\n",
      "11/24 00:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1150/1563]  lr: 1.0000e-03  eta: 0:02:06  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3607\n",
      "11/24 00:29:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1160/1563]  lr: 1.0000e-03  eta: 0:02:06  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3419\n",
      "11/24 00:29:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1170/1563]  lr: 1.0000e-03  eta: 0:02:05  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3495\n",
      "11/24 00:29:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1180/1563]  lr: 1.0000e-03  eta: 0:02:05  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3255\n",
      "11/24 00:29:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1190/1563]  lr: 1.0000e-03  eta: 0:02:05  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3467\n",
      "11/24 00:29:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1200/1563]  lr: 1.0000e-03  eta: 0:02:04  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3028\n",
      "11/24 00:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1210/1563]  lr: 1.0000e-03  eta: 0:02:04  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3145\n",
      "11/24 00:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1220/1563]  lr: 1.0000e-03  eta: 0:02:04  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3359\n",
      "11/24 00:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1230/1563]  lr: 1.0000e-03  eta: 0:02:04  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3190\n",
      "11/24 00:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1240/1563]  lr: 1.0000e-03  eta: 0:02:03  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3570\n",
      "11/24 00:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1250/1563]  lr: 1.0000e-03  eta: 0:02:03  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3223\n",
      "11/24 00:29:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1260/1563]  lr: 1.0000e-03  eta: 0:02:03  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3348\n",
      "11/24 00:29:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1270/1563]  lr: 1.0000e-03  eta: 0:02:02  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3530\n",
      "11/24 00:29:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1280/1563]  lr: 1.0000e-03  eta: 0:02:02  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3364\n",
      "11/24 00:29:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1290/1563]  lr: 1.0000e-03  eta: 0:02:02  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3317\n",
      "11/24 00:29:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1300/1563]  lr: 1.0000e-03  eta: 0:02:02  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.2885\n",
      "11/24 00:29:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1310/1563]  lr: 1.0000e-03  eta: 0:02:01  time: 0.0218  data_time: 0.0060  memory: 2032  loss: 2.3149\n",
      "11/24 00:29:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1320/1563]  lr: 1.0000e-03  eta: 0:02:01  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3370\n",
      "11/24 00:29:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1330/1563]  lr: 1.0000e-03  eta: 0:02:01  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3049\n",
      "11/24 00:29:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1340/1563]  lr: 1.0000e-03  eta: 0:02:00  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3545\n",
      "11/24 00:29:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1350/1563]  lr: 1.0000e-03  eta: 0:02:00  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3461\n",
      "11/24 00:29:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1360/1563]  lr: 1.0000e-03  eta: 0:02:00  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3223\n",
      "11/24 00:29:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1370/1563]  lr: 1.0000e-03  eta: 0:02:00  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3782\n",
      "11/24 00:29:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1380/1563]  lr: 1.0000e-03  eta: 0:01:59  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3389\n",
      "11/24 00:29:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1390/1563]  lr: 1.0000e-03  eta: 0:01:59  time: 0.0232  data_time: 0.0069  memory: 2032  loss: 2.3556\n",
      "11/24 00:29:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1400/1563]  lr: 1.0000e-03  eta: 0:01:59  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3523\n",
      "11/24 00:29:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1410/1563]  lr: 1.0000e-03  eta: 0:01:58  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3496\n",
      "11/24 00:29:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1420/1563]  lr: 1.0000e-03  eta: 0:01:58  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3415\n",
      "11/24 00:29:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1430/1563]  lr: 1.0000e-03  eta: 0:01:58  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3293\n",
      "11/24 00:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1440/1563]  lr: 1.0000e-03  eta: 0:01:58  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3546\n",
      "11/24 00:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1450/1563]  lr: 1.0000e-03  eta: 0:01:57  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3208\n",
      "11/24 00:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1460/1563]  lr: 1.0000e-03  eta: 0:01:57  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3124\n",
      "11/24 00:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1470/1563]  lr: 1.0000e-03  eta: 0:01:57  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3166\n",
      "11/24 00:29:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1480/1563]  lr: 1.0000e-03  eta: 0:01:56  time: 0.0236  data_time: 0.0076  memory: 2032  loss: 2.3448\n",
      "11/24 00:29:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1490/1563]  lr: 1.0000e-03  eta: 0:01:56  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3274\n",
      "11/24 00:29:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1500/1563]  lr: 1.0000e-03  eta: 0:01:56  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3402\n",
      "11/24 00:29:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1510/1563]  lr: 1.0000e-03  eta: 0:01:56  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3336\n",
      "11/24 00:29:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1520/1563]  lr: 1.0000e-03  eta: 0:01:55  time: 0.0229  data_time: 0.0069  memory: 2032  loss: 2.3270\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1530/1563]  lr: 1.0000e-03  eta: 0:01:55  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3569\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1540/1563]  lr: 1.0000e-03  eta: 0:01:55  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3343\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1550/1563]  lr: 1.0000e-03  eta: 0:01:54  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3460\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1560/1563]  lr: 1.0000e-03  eta: 0:01:54  time: 0.0230  data_time: 0.0066  memory: 2032  loss: 2.3331\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:29:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "11/24 00:29:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 10/313]    eta: 0:00:02  time: 0.0069  data_time: 0.0040  memory: 2032  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 20/313]    eta: 0:00:04  time: 0.0206  data_time: 0.0171  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 30/313]    eta: 0:00:03  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 40/313]    eta: 0:00:02  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 50/313]    eta: 0:00:02  time: 0.0143  data_time: 0.0115  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 60/313]    eta: 0:00:02  time: 0.0089  data_time: 0.0051  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 70/313]    eta: 0:00:02  time: 0.0073  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 80/313]    eta: 0:00:02  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 90/313]    eta: 0:00:02  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][100/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][110/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][120/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][130/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][140/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][150/313]    eta: 0:00:01  time: 0.0087  data_time: 0.0050  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][160/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][170/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][180/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][190/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][200/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][210/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][220/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][230/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][240/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][250/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][260/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:29:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][270/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][280/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][290/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][300/313]    eta: 0:00:00  time: 0.0082  data_time: 0.0049  memory: 1619  \n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][310/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][313/313]    accuracy: 10.0000  data_time: 0.0048  time: 0.0078\n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  10/1563]  lr: 1.0000e-03  eta: 0:01:54  time: 0.0234  data_time: 0.0063  memory: 2032  loss: 2.3150\n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  20/1563]  lr: 1.0000e-03  eta: 0:01:54  time: 0.0223  data_time: 0.0061  memory: 2032  loss: 2.3469\n",
      "11/24 00:29:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  30/1563]  lr: 1.0000e-03  eta: 0:01:53  time: 0.0231  data_time: 0.0071  memory: 2032  loss: 2.3408\n",
      "11/24 00:29:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  40/1563]  lr: 1.0000e-03  eta: 0:01:53  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3279\n",
      "11/24 00:29:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  50/1563]  lr: 1.0000e-03  eta: 0:01:53  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3108\n",
      "11/24 00:29:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  60/1563]  lr: 1.0000e-03  eta: 0:01:53  time: 0.0222  data_time: 0.0061  memory: 2032  loss: 2.3442\n",
      "11/24 00:29:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  70/1563]  lr: 1.0000e-03  eta: 0:01:52  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.3137\n",
      "11/24 00:29:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  80/1563]  lr: 1.0000e-03  eta: 0:01:52  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3248\n",
      "11/24 00:29:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][  90/1563]  lr: 1.0000e-03  eta: 0:01:52  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3376\n",
      "11/24 00:29:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 100/1563]  lr: 1.0000e-03  eta: 0:01:51  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3281\n",
      "11/24 00:29:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 110/1563]  lr: 1.0000e-03  eta: 0:01:51  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3393\n",
      "11/24 00:29:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 120/1563]  lr: 1.0000e-03  eta: 0:01:51  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3330\n",
      "11/24 00:29:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 130/1563]  lr: 1.0000e-03  eta: 0:01:51  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3418\n",
      "11/24 00:29:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 140/1563]  lr: 1.0000e-03  eta: 0:01:50  time: 0.0238  data_time: 0.0072  memory: 2032  loss: 2.3558\n",
      "11/24 00:29:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 150/1563]  lr: 1.0000e-03  eta: 0:01:50  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3407\n",
      "11/24 00:29:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 160/1563]  lr: 1.0000e-03  eta: 0:01:50  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3573\n",
      "11/24 00:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 170/1563]  lr: 1.0000e-03  eta: 0:01:50  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3285\n",
      "11/24 00:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 180/1563]  lr: 1.0000e-03  eta: 0:01:49  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3188\n",
      "11/24 00:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 190/1563]  lr: 1.0000e-03  eta: 0:01:49  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3321\n",
      "11/24 00:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 200/1563]  lr: 1.0000e-03  eta: 0:01:49  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3395\n",
      "11/24 00:29:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 210/1563]  lr: 1.0000e-03  eta: 0:01:48  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3267\n",
      "11/24 00:29:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 220/1563]  lr: 1.0000e-03  eta: 0:01:48  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3333\n",
      "11/24 00:29:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 230/1563]  lr: 1.0000e-03  eta: 0:01:48  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3402\n",
      "11/24 00:29:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 240/1563]  lr: 1.0000e-03  eta: 0:01:48  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3321\n",
      "11/24 00:29:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 250/1563]  lr: 1.0000e-03  eta: 0:01:47  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3307\n",
      "11/24 00:29:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 260/1563]  lr: 1.0000e-03  eta: 0:01:47  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3335\n",
      "11/24 00:29:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 270/1563]  lr: 1.0000e-03  eta: 0:01:47  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3170\n",
      "11/24 00:29:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 280/1563]  lr: 1.0000e-03  eta: 0:01:47  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3474\n",
      "11/24 00:29:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 290/1563]  lr: 1.0000e-03  eta: 0:01:46  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3472\n",
      "11/24 00:29:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 300/1563]  lr: 1.0000e-03  eta: 0:01:46  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3394\n",
      "11/24 00:29:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 310/1563]  lr: 1.0000e-03  eta: 0:01:46  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3330\n",
      "11/24 00:29:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 320/1563]  lr: 1.0000e-03  eta: 0:01:45  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3229\n",
      "11/24 00:29:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 330/1563]  lr: 1.0000e-03  eta: 0:01:45  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3110\n",
      "11/24 00:29:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 340/1563]  lr: 1.0000e-03  eta: 0:01:45  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3384\n",
      "11/24 00:29:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 350/1563]  lr: 1.0000e-03  eta: 0:01:45  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3248\n",
      "11/24 00:29:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 360/1563]  lr: 1.0000e-03  eta: 0:01:44  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3260\n",
      "11/24 00:29:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 370/1563]  lr: 1.0000e-03  eta: 0:01:44  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3396\n",
      "11/24 00:29:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 380/1563]  lr: 1.0000e-03  eta: 0:01:44  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3330\n",
      "11/24 00:29:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 390/1563]  lr: 1.0000e-03  eta: 0:01:44  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3349\n",
      "11/24 00:29:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 400/1563]  lr: 1.0000e-03  eta: 0:01:43  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3439\n",
      "11/24 00:29:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 410/1563]  lr: 1.0000e-03  eta: 0:01:43  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3410\n",
      "11/24 00:29:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 420/1563]  lr: 1.0000e-03  eta: 0:01:43  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3211\n",
      "11/24 00:29:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 430/1563]  lr: 1.0000e-03  eta: 0:01:43  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3337\n",
      "11/24 00:29:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 440/1563]  lr: 1.0000e-03  eta: 0:01:42  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3330\n",
      "11/24 00:29:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 450/1563]  lr: 1.0000e-03  eta: 0:01:42  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3384\n",
      "11/24 00:29:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 460/1563]  lr: 1.0000e-03  eta: 0:01:42  time: 0.0230  data_time: 0.0067  memory: 2032  loss: 2.3302\n",
      "11/24 00:29:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 470/1563]  lr: 1.0000e-03  eta: 0:01:41  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3370\n",
      "11/24 00:29:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 480/1563]  lr: 1.0000e-03  eta: 0:01:41  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3290\n",
      "11/24 00:29:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 490/1563]  lr: 1.0000e-03  eta: 0:01:41  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3214\n",
      "11/24 00:29:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 500/1563]  lr: 1.0000e-03  eta: 0:01:41  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3358\n",
      "11/24 00:29:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 510/1563]  lr: 1.0000e-03  eta: 0:01:40  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3412\n",
      "11/24 00:29:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 520/1563]  lr: 1.0000e-03  eta: 0:01:40  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3314\n",
      "11/24 00:29:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 530/1563]  lr: 1.0000e-03  eta: 0:01:40  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3336\n",
      "11/24 00:29:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 540/1563]  lr: 1.0000e-03  eta: 0:01:40  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3346\n",
      "11/24 00:29:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 550/1563]  lr: 1.0000e-03  eta: 0:01:39  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3399\n",
      "11/24 00:29:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 560/1563]  lr: 1.0000e-03  eta: 0:01:39  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3308\n",
      "11/24 00:29:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 570/1563]  lr: 1.0000e-03  eta: 0:01:39  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3174\n",
      "11/24 00:30:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 580/1563]  lr: 1.0000e-03  eta: 0:01:39  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3507\n",
      "11/24 00:30:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 590/1563]  lr: 1.0000e-03  eta: 0:01:38  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3191\n",
      "11/24 00:30:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 600/1563]  lr: 1.0000e-03  eta: 0:01:38  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3221\n",
      "11/24 00:30:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 610/1563]  lr: 1.0000e-03  eta: 0:01:38  time: 0.0230  data_time: 0.0066  memory: 2032  loss: 2.3238\n",
      "11/24 00:30:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 620/1563]  lr: 1.0000e-03  eta: 0:01:38  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3282\n",
      "11/24 00:30:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 630/1563]  lr: 1.0000e-03  eta: 0:01:37  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3597\n",
      "11/24 00:30:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 640/1563]  lr: 1.0000e-03  eta: 0:01:37  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3263\n",
      "11/24 00:30:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 650/1563]  lr: 1.0000e-03  eta: 0:01:37  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3511\n",
      "11/24 00:30:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 660/1563]  lr: 1.0000e-03  eta: 0:01:36  time: 0.0222  data_time: 0.0061  memory: 2032  loss: 2.3292\n",
      "11/24 00:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 670/1563]  lr: 1.0000e-03  eta: 0:01:36  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3260\n",
      "11/24 00:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 680/1563]  lr: 1.0000e-03  eta: 0:01:36  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3216\n",
      "11/24 00:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 690/1563]  lr: 1.0000e-03  eta: 0:01:36  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3262\n",
      "11/24 00:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 700/1563]  lr: 1.0000e-03  eta: 0:01:35  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3436\n",
      "11/24 00:30:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 710/1563]  lr: 1.0000e-03  eta: 0:01:35  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3321\n",
      "11/24 00:30:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 720/1563]  lr: 1.0000e-03  eta: 0:01:35  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3218\n",
      "11/24 00:30:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 730/1563]  lr: 1.0000e-03  eta: 0:01:35  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3711\n",
      "11/24 00:30:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 740/1563]  lr: 1.0000e-03  eta: 0:01:34  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3082\n",
      "11/24 00:30:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 750/1563]  lr: 1.0000e-03  eta: 0:01:34  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3359\n",
      "11/24 00:30:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 760/1563]  lr: 1.0000e-03  eta: 0:01:34  time: 0.0232  data_time: 0.0069  memory: 2032  loss: 2.3171\n",
      "11/24 00:30:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 770/1563]  lr: 1.0000e-03  eta: 0:01:34  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3356\n",
      "11/24 00:30:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 780/1563]  lr: 1.0000e-03  eta: 0:01:33  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3342\n",
      "11/24 00:30:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 790/1563]  lr: 1.0000e-03  eta: 0:01:33  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3376\n",
      "11/24 00:30:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 800/1563]  lr: 1.0000e-03  eta: 0:01:33  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3378\n",
      "11/24 00:30:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 810/1563]  lr: 1.0000e-03  eta: 0:01:33  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3355\n",
      "11/24 00:30:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 820/1563]  lr: 1.0000e-03  eta: 0:01:32  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3373\n",
      "11/24 00:30:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 830/1563]  lr: 1.0000e-03  eta: 0:01:32  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3186\n",
      "11/24 00:30:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 840/1563]  lr: 1.0000e-03  eta: 0:01:32  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3376\n",
      "11/24 00:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 850/1563]  lr: 1.0000e-03  eta: 0:01:32  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3387\n",
      "11/24 00:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 860/1563]  lr: 1.0000e-03  eta: 0:01:31  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3425\n",
      "11/24 00:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 870/1563]  lr: 1.0000e-03  eta: 0:01:31  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3409\n",
      "11/24 00:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 880/1563]  lr: 1.0000e-03  eta: 0:01:31  time: 0.0225  data_time: 0.0066  memory: 2032  loss: 2.3363\n",
      "11/24 00:30:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 890/1563]  lr: 1.0000e-03  eta: 0:01:31  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3304\n",
      "11/24 00:30:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 900/1563]  lr: 1.0000e-03  eta: 0:01:30  time: 0.0234  data_time: 0.0072  memory: 2032  loss: 2.3149\n",
      "11/24 00:30:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 910/1563]  lr: 1.0000e-03  eta: 0:01:30  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3235\n",
      "11/24 00:30:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 920/1563]  lr: 1.0000e-03  eta: 0:01:30  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3346\n",
      "11/24 00:30:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 930/1563]  lr: 1.0000e-03  eta: 0:01:30  time: 0.0226  data_time: 0.0067  memory: 2032  loss: 2.3406\n",
      "11/24 00:30:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 940/1563]  lr: 1.0000e-03  eta: 0:01:29  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3253\n",
      "11/24 00:30:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 950/1563]  lr: 1.0000e-03  eta: 0:01:29  time: 0.0218  data_time: 0.0060  memory: 2032  loss: 2.3366\n",
      "11/24 00:30:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 960/1563]  lr: 1.0000e-03  eta: 0:01:29  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3383\n",
      "11/24 00:30:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 970/1563]  lr: 1.0000e-03  eta: 0:01:28  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3184\n",
      "11/24 00:30:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 980/1563]  lr: 1.0000e-03  eta: 0:01:28  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3438\n",
      "11/24 00:30:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][ 990/1563]  lr: 1.0000e-03  eta: 0:01:28  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3226\n",
      "11/24 00:30:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1000/1563]  lr: 1.0000e-03  eta: 0:01:28  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3401\n",
      "11/24 00:30:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1010/1563]  lr: 1.0000e-03  eta: 0:01:27  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3132\n",
      "11/24 00:30:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1020/1563]  lr: 1.0000e-03  eta: 0:01:27  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3389\n",
      "11/24 00:30:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1030/1563]  lr: 1.0000e-03  eta: 0:01:27  time: 0.0225  data_time: 0.0066  memory: 2032  loss: 2.3497\n",
      "11/24 00:30:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1040/1563]  lr: 1.0000e-03  eta: 0:01:27  time: 0.0231  data_time: 0.0068  memory: 2032  loss: 2.3282\n",
      "11/24 00:30:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1050/1563]  lr: 1.0000e-03  eta: 0:01:26  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3293\n",
      "11/24 00:30:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1060/1563]  lr: 1.0000e-03  eta: 0:01:26  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3048\n",
      "11/24 00:30:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1070/1563]  lr: 1.0000e-03  eta: 0:01:26  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3246\n",
      "11/24 00:30:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1080/1563]  lr: 1.0000e-03  eta: 0:01:26  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3314\n",
      "11/24 00:30:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1090/1563]  lr: 1.0000e-03  eta: 0:01:25  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3493\n",
      "11/24 00:30:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1100/1563]  lr: 1.0000e-03  eta: 0:01:25  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3244\n",
      "11/24 00:30:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1110/1563]  lr: 1.0000e-03  eta: 0:01:25  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3393\n",
      "11/24 00:30:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1120/1563]  lr: 1.0000e-03  eta: 0:01:25  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3399\n",
      "11/24 00:30:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1130/1563]  lr: 1.0000e-03  eta: 0:01:24  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3386\n",
      "11/24 00:30:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1140/1563]  lr: 1.0000e-03  eta: 0:01:24  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3428\n",
      "11/24 00:30:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1150/1563]  lr: 1.0000e-03  eta: 0:01:24  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3487\n",
      "11/24 00:30:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1160/1563]  lr: 1.0000e-03  eta: 0:01:24  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3364\n",
      "11/24 00:30:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1170/1563]  lr: 1.0000e-03  eta: 0:01:23  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3180\n",
      "11/24 00:30:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1180/1563]  lr: 1.0000e-03  eta: 0:01:23  time: 0.0235  data_time: 0.0071  memory: 2032  loss: 2.3236\n",
      "11/24 00:30:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1190/1563]  lr: 1.0000e-03  eta: 0:01:23  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3373\n",
      "11/24 00:30:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1200/1563]  lr: 1.0000e-03  eta: 0:01:23  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3392\n",
      "11/24 00:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1210/1563]  lr: 1.0000e-03  eta: 0:01:22  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3422\n",
      "11/24 00:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1220/1563]  lr: 1.0000e-03  eta: 0:01:22  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3301\n",
      "11/24 00:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1230/1563]  lr: 1.0000e-03  eta: 0:01:22  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3612\n",
      "11/24 00:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1240/1563]  lr: 1.0000e-03  eta: 0:01:22  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3339\n",
      "11/24 00:30:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1250/1563]  lr: 1.0000e-03  eta: 0:01:21  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3277\n",
      "11/24 00:30:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1260/1563]  lr: 1.0000e-03  eta: 0:01:21  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3349\n",
      "11/24 00:30:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1270/1563]  lr: 1.0000e-03  eta: 0:01:21  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3230\n",
      "11/24 00:30:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1280/1563]  lr: 1.0000e-03  eta: 0:01:21  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3511\n",
      "11/24 00:30:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1290/1563]  lr: 1.0000e-03  eta: 0:01:20  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3288\n",
      "11/24 00:30:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1300/1563]  lr: 1.0000e-03  eta: 0:01:20  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3502\n",
      "11/24 00:30:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1310/1563]  lr: 1.0000e-03  eta: 0:01:20  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3269\n",
      "11/24 00:30:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1320/1563]  lr: 1.0000e-03  eta: 0:01:20  time: 0.0231  data_time: 0.0068  memory: 2032  loss: 2.3288\n",
      "11/24 00:30:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1330/1563]  lr: 1.0000e-03  eta: 0:01:19  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3050\n",
      "11/24 00:30:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1340/1563]  lr: 1.0000e-03  eta: 0:01:19  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3507\n",
      "11/24 00:30:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1350/1563]  lr: 1.0000e-03  eta: 0:01:19  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3415\n",
      "11/24 00:30:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1360/1563]  lr: 1.0000e-03  eta: 0:01:19  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3175\n",
      "11/24 00:30:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1370/1563]  lr: 1.0000e-03  eta: 0:01:18  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3426\n",
      "11/24 00:30:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1380/1563]  lr: 1.0000e-03  eta: 0:01:18  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3224\n",
      "11/24 00:30:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1390/1563]  lr: 1.0000e-03  eta: 0:01:18  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3413\n",
      "11/24 00:30:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1400/1563]  lr: 1.0000e-03  eta: 0:01:18  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3446\n",
      "11/24 00:30:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1410/1563]  lr: 1.0000e-03  eta: 0:01:17  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3366\n",
      "11/24 00:30:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1420/1563]  lr: 1.0000e-03  eta: 0:01:17  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3148\n",
      "11/24 00:30:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1430/1563]  lr: 1.0000e-03  eta: 0:01:17  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3315\n",
      "11/24 00:30:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1440/1563]  lr: 1.0000e-03  eta: 0:01:17  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3252\n",
      "11/24 00:30:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1450/1563]  lr: 1.0000e-03  eta: 0:01:16  time: 0.0233  data_time: 0.0068  memory: 2032  loss: 2.3215\n",
      "11/24 00:30:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1460/1563]  lr: 1.0000e-03  eta: 0:01:16  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3159\n",
      "11/24 00:30:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1470/1563]  lr: 1.0000e-03  eta: 0:01:16  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3306\n",
      "11/24 00:30:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1480/1563]  lr: 1.0000e-03  eta: 0:01:16  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3339\n",
      "11/24 00:30:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1490/1563]  lr: 1.0000e-03  eta: 0:01:15  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3233\n",
      "11/24 00:30:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1500/1563]  lr: 1.0000e-03  eta: 0:01:15  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3144\n",
      "11/24 00:30:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1510/1563]  lr: 1.0000e-03  eta: 0:01:15  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3334\n",
      "11/24 00:30:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1520/1563]  lr: 1.0000e-03  eta: 0:01:15  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3223\n",
      "11/24 00:30:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1530/1563]  lr: 1.0000e-03  eta: 0:01:14  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3200\n",
      "11/24 00:30:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1540/1563]  lr: 1.0000e-03  eta: 0:01:14  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3105\n",
      "11/24 00:30:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1550/1563]  lr: 1.0000e-03  eta: 0:01:14  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3274\n",
      "11/24 00:30:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [3][1560/1563]  lr: 1.0000e-03  eta: 0:01:14  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3338\n",
      "11/24 00:30:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:30:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 10/313]    eta: 0:00:02  time: 0.0074  data_time: 0.0040  memory: 2032  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 20/313]    eta: 0:00:04  time: 0.0230  data_time: 0.0203  memory: 1619  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 30/313]    eta: 0:00:03  time: 0.0075  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 40/313]    eta: 0:00:03  time: 0.0071  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 50/313]    eta: 0:00:02  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 60/313]    eta: 0:00:02  time: 0.0185  data_time: 0.0148  memory: 1619  \n",
      "11/24 00:30:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 70/313]    eta: 0:00:02  time: 0.0073  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 80/313]    eta: 0:00:02  time: 0.0071  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][ 90/313]    eta: 0:00:02  time: 0.0089  data_time: 0.0056  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][100/313]    eta: 0:00:02  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][110/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][120/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][130/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][140/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][150/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][160/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][170/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][180/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][190/313]    eta: 0:00:01  time: 0.0085  data_time: 0.0049  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][200/313]    eta: 0:00:00  time: 0.0076  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][210/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][220/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][230/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][240/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][250/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][260/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][270/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][280/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][290/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][300/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][310/313]    eta: 0:00:00  time: 0.0089  data_time: 0.0051  memory: 1619  \n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][313/313]    accuracy: 10.0000  data_time: 0.0051  time: 0.0081\n",
      "11/24 00:30:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  10/1563]  lr: 1.0000e-03  eta: 0:01:13  time: 0.0230  data_time: 0.0064  memory: 2032  loss: 2.2936\n",
      "11/24 00:30:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  20/1563]  lr: 1.0000e-03  eta: 0:01:13  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3335\n",
      "11/24 00:30:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  30/1563]  lr: 1.0000e-03  eta: 0:01:13  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3351\n",
      "11/24 00:30:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  40/1563]  lr: 1.0000e-03  eta: 0:01:13  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3464\n",
      "11/24 00:30:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  50/1563]  lr: 1.0000e-03  eta: 0:01:12  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3386\n",
      "11/24 00:30:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  60/1563]  lr: 1.0000e-03  eta: 0:01:12  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3260\n",
      "11/24 00:30:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  70/1563]  lr: 1.0000e-03  eta: 0:01:12  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3306\n",
      "11/24 00:30:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  80/1563]  lr: 1.0000e-03  eta: 0:01:12  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3269\n",
      "11/24 00:30:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][  90/1563]  lr: 1.0000e-03  eta: 0:01:11  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3222\n",
      "11/24 00:30:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 100/1563]  lr: 1.0000e-03  eta: 0:01:11  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3169\n",
      "11/24 00:30:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 110/1563]  lr: 1.0000e-03  eta: 0:01:11  time: 0.0238  data_time: 0.0069  memory: 2032  loss: 2.3221\n",
      "11/24 00:30:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 120/1563]  lr: 1.0000e-03  eta: 0:01:11  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3074\n",
      "11/24 00:30:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 130/1563]  lr: 1.0000e-03  eta: 0:01:10  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3328\n",
      "11/24 00:30:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 140/1563]  lr: 1.0000e-03  eta: 0:01:10  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3258\n",
      "11/24 00:30:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 150/1563]  lr: 1.0000e-03  eta: 0:01:10  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3265\n",
      "11/24 00:30:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 160/1563]  lr: 1.0000e-03  eta: 0:01:10  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3121\n",
      "11/24 00:30:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 170/1563]  lr: 1.0000e-03  eta: 0:01:09  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3177\n",
      "11/24 00:30:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 180/1563]  lr: 1.0000e-03  eta: 0:01:09  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.3299\n",
      "11/24 00:30:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 190/1563]  lr: 1.0000e-03  eta: 0:01:09  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3159\n",
      "11/24 00:30:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 200/1563]  lr: 1.0000e-03  eta: 0:01:09  time: 0.0222  data_time: 0.0064  memory: 2032  loss: 2.3176\n",
      "11/24 00:30:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 210/1563]  lr: 1.0000e-03  eta: 0:01:08  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3059\n",
      "11/24 00:30:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 220/1563]  lr: 1.0000e-03  eta: 0:01:08  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3396\n",
      "11/24 00:30:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 230/1563]  lr: 1.0000e-03  eta: 0:01:08  time: 0.0234  data_time: 0.0068  memory: 2032  loss: 2.3257\n",
      "11/24 00:30:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 240/1563]  lr: 1.0000e-03  eta: 0:01:08  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3250\n",
      "11/24 00:30:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 250/1563]  lr: 1.0000e-03  eta: 0:01:07  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3185\n",
      "11/24 00:30:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 260/1563]  lr: 1.0000e-03  eta: 0:01:07  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3376\n",
      "11/24 00:30:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 270/1563]  lr: 1.0000e-03  eta: 0:01:07  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3277\n",
      "11/24 00:30:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 280/1563]  lr: 1.0000e-03  eta: 0:01:07  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3464\n",
      "11/24 00:30:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 290/1563]  lr: 1.0000e-03  eta: 0:01:07  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3129\n",
      "11/24 00:30:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 300/1563]  lr: 1.0000e-03  eta: 0:01:06  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3375\n",
      "11/24 00:30:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 310/1563]  lr: 1.0000e-03  eta: 0:01:06  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3277\n",
      "11/24 00:30:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:30:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 320/1563]  lr: 1.0000e-03  eta: 0:01:06  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3328\n",
      "11/24 00:30:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 330/1563]  lr: 1.0000e-03  eta: 0:01:06  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3314\n",
      "11/24 00:30:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 340/1563]  lr: 1.0000e-03  eta: 0:01:05  time: 0.0234  data_time: 0.0069  memory: 2032  loss: 2.3302\n",
      "11/24 00:30:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 350/1563]  lr: 1.0000e-03  eta: 0:01:05  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3359\n",
      "11/24 00:30:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 360/1563]  lr: 1.0000e-03  eta: 0:01:05  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3215\n",
      "11/24 00:30:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 370/1563]  lr: 1.0000e-03  eta: 0:01:05  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3115\n",
      "11/24 00:30:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 380/1563]  lr: 1.0000e-03  eta: 0:01:04  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3237\n",
      "11/24 00:30:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 390/1563]  lr: 1.0000e-03  eta: 0:01:04  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3143\n",
      "11/24 00:30:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 400/1563]  lr: 1.0000e-03  eta: 0:01:04  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3333\n",
      "11/24 00:30:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 410/1563]  lr: 1.0000e-03  eta: 0:01:04  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3213\n",
      "11/24 00:30:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 420/1563]  lr: 1.0000e-03  eta: 0:01:03  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3059\n",
      "11/24 00:30:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 430/1563]  lr: 1.0000e-03  eta: 0:01:03  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3308\n",
      "11/24 00:30:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 440/1563]  lr: 1.0000e-03  eta: 0:01:03  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3192\n",
      "11/24 00:30:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 450/1563]  lr: 1.0000e-03  eta: 0:01:03  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3320\n",
      "11/24 00:30:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 460/1563]  lr: 1.0000e-03  eta: 0:01:02  time: 0.0238  data_time: 0.0071  memory: 2032  loss: 2.3228\n",
      "11/24 00:30:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 470/1563]  lr: 1.0000e-03  eta: 0:01:02  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3282\n",
      "11/24 00:30:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 480/1563]  lr: 1.0000e-03  eta: 0:01:02  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3309\n",
      "11/24 00:30:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 490/1563]  lr: 1.0000e-03  eta: 0:01:02  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3335\n",
      "11/24 00:30:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 500/1563]  lr: 1.0000e-03  eta: 0:01:01  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.2920\n",
      "11/24 00:30:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 510/1563]  lr: 1.0000e-03  eta: 0:01:01  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3413\n",
      "11/24 00:30:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 520/1563]  lr: 1.0000e-03  eta: 0:01:01  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3209\n",
      "11/24 00:30:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 530/1563]  lr: 1.0000e-03  eta: 0:01:01  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3270\n",
      "11/24 00:30:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 540/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0226  data_time: 0.0067  memory: 2032  loss: 2.3411\n",
      "11/24 00:30:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 550/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3327\n",
      "11/24 00:30:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 560/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3346\n",
      "11/24 00:30:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 570/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0241  data_time: 0.0070  memory: 2032  loss: 2.3214\n",
      "11/24 00:30:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 580/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3238\n",
      "11/24 00:30:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 590/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3291\n",
      "11/24 00:30:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 600/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3239\n",
      "11/24 00:30:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 610/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3170\n",
      "11/24 00:30:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 620/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3331\n",
      "11/24 00:30:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 630/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3229\n",
      "11/24 00:30:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 640/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3404\n",
      "11/24 00:30:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 650/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3453\n",
      "11/24 00:30:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 660/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3110\n",
      "11/24 00:30:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 670/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0229  data_time: 0.0068  memory: 2032  loss: 2.3269\n",
      "11/24 00:30:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 680/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3405\n",
      "11/24 00:30:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 690/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0231  data_time: 0.0069  memory: 2032  loss: 2.3558\n",
      "11/24 00:30:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 700/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3240\n",
      "11/24 00:30:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 710/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3253\n",
      "11/24 00:30:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 720/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3087\n",
      "11/24 00:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 730/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0225  data_time: 0.0066  memory: 2032  loss: 2.3138\n",
      "11/24 00:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 740/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0232  data_time: 0.0067  memory: 2032  loss: 2.3369\n",
      "11/24 00:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 750/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3319\n",
      "11/24 00:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 760/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3204\n",
      "11/24 00:30:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 770/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3367\n",
      "11/24 00:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 780/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3147\n",
      "11/24 00:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 790/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3239\n",
      "11/24 00:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 800/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0248  data_time: 0.0073  memory: 2032  loss: 2.3187\n",
      "11/24 00:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 810/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3207\n",
      "11/24 00:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 820/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3059\n",
      "11/24 00:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 830/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3089\n",
      "11/24 00:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 840/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3130\n",
      "11/24 00:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 850/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3470\n",
      "11/24 00:30:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 860/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3392\n",
      "11/24 00:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 870/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3303\n",
      "11/24 00:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 880/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3160\n",
      "11/24 00:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 890/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3084\n",
      "11/24 00:30:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 900/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3430\n",
      "11/24 00:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 910/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0234  data_time: 0.0069  memory: 2032  loss: 2.3252\n",
      "11/24 00:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 920/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3380\n",
      "11/24 00:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 930/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3176\n",
      "11/24 00:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 940/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3538\n",
      "11/24 00:30:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 950/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3171\n",
      "11/24 00:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 960/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3378\n",
      "11/24 00:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 970/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3086\n",
      "11/24 00:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 980/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3314\n",
      "11/24 00:30:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][ 990/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3279\n",
      "11/24 00:30:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1000/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3325\n",
      "11/24 00:30:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1010/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3354\n",
      "11/24 00:30:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1020/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0236  data_time: 0.0071  memory: 2032  loss: 2.3224\n",
      "11/24 00:30:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1030/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3343\n",
      "11/24 00:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1040/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3364\n",
      "11/24 00:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1050/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3343\n",
      "11/24 00:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1060/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3348\n",
      "11/24 00:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1070/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0221  data_time: 0.0063  memory: 2032  loss: 2.3090\n",
      "11/24 00:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1080/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3302\n",
      "11/24 00:30:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1090/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3481\n",
      "11/24 00:30:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1100/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3235\n",
      "11/24 00:30:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1110/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3255\n",
      "11/24 00:30:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1120/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3416\n",
      "11/24 00:30:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1130/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0234  data_time: 0.0071  memory: 2032  loss: 2.3196\n",
      "11/24 00:30:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1140/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0227  data_time: 0.0066  memory: 2032  loss: 2.3326\n",
      "11/24 00:30:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1150/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3384\n",
      "11/24 00:30:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1160/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3199\n",
      "11/24 00:30:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1170/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3278\n",
      "11/24 00:31:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1180/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3231\n",
      "11/24 00:31:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1190/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3303\n",
      "11/24 00:31:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1200/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3291\n",
      "11/24 00:31:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1210/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0225  data_time: 0.0066  memory: 2032  loss: 2.3249\n",
      "11/24 00:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1220/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3278\n",
      "11/24 00:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1230/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0237  data_time: 0.0072  memory: 2032  loss: 2.3257\n",
      "11/24 00:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1240/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3335\n",
      "11/24 00:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1250/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3302\n",
      "11/24 00:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1260/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3251\n",
      "11/24 00:31:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1270/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3217\n",
      "11/24 00:31:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1280/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3135\n",
      "11/24 00:31:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1290/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3042\n",
      "11/24 00:31:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1300/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3176\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1310/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3258\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1320/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3247\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1330/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0232  data_time: 0.0068  memory: 2032  loss: 2.3306\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1340/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3271\n",
      "11/24 00:31:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1350/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3190\n",
      "11/24 00:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1360/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3063\n",
      "11/24 00:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1370/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3342\n",
      "11/24 00:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1380/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3260\n",
      "11/24 00:31:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1390/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3354\n",
      "11/24 00:31:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1400/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3348\n",
      "11/24 00:31:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1410/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3222\n",
      "11/24 00:31:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1420/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3061\n",
      "11/24 00:31:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1430/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3206\n",
      "11/24 00:31:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1440/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3416\n",
      "11/24 00:31:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1450/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3283\n",
      "11/24 00:31:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1460/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3353\n",
      "11/24 00:31:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1470/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0224  data_time: 0.0065  memory: 2032  loss: 2.3230\n",
      "11/24 00:31:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1480/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3285\n",
      "11/24 00:31:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1490/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3177\n",
      "11/24 00:31:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1500/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3376\n",
      "11/24 00:31:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1510/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0228  data_time: 0.0067  memory: 2032  loss: 2.3315\n",
      "11/24 00:31:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1520/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3091\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1530/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3214\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1540/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0239  data_time: 0.0071  memory: 2032  loss: 2.3240\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1550/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3296\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [4][1560/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3222\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:31:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 10/313]    eta: 0:00:02  time: 0.0079  data_time: 0.0050  memory: 2032  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 20/313]    eta: 0:00:02  time: 0.0113  data_time: 0.0082  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 30/313]    eta: 0:00:02  time: 0.0080  data_time: 0.0050  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 40/313]    eta: 0:00:02  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 50/313]    eta: 0:00:02  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 60/313]    eta: 0:00:02  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 70/313]    eta: 0:00:01  time: 0.0080  data_time: 0.0048  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 80/313]    eta: 0:00:01  time: 0.0075  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][ 90/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][100/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:31:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][110/313]    eta: 0:00:01  time: 0.0090  data_time: 0.0053  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][120/313]    eta: 0:00:01  time: 0.0074  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][130/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][140/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][150/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][160/313]    eta: 0:00:01  time: 0.0078  data_time: 0.0048  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][170/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][180/313]    eta: 0:00:01  time: 0.0078  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][190/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][200/313]    eta: 0:00:00  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][210/313]    eta: 0:00:00  time: 0.0087  data_time: 0.0053  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][220/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][230/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][240/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][250/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][260/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][270/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][280/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][290/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][300/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][310/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][313/313]    accuracy: 10.0000  data_time: 0.0045  time: 0.0075\n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  10/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0253  data_time: 0.0087  memory: 2032  loss: 2.3346\n",
      "11/24 00:31:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  20/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3280\n",
      "11/24 00:31:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  30/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3218\n",
      "11/24 00:31:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  40/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3384\n",
      "11/24 00:31:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  50/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3127\n",
      "11/24 00:31:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  60/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0218  data_time: 0.0060  memory: 2032  loss: 2.3261\n",
      "11/24 00:31:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  70/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0218  data_time: 0.0060  memory: 2032  loss: 2.3285\n",
      "11/24 00:31:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  80/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3354\n",
      "11/24 00:31:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][  90/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3215\n",
      "11/24 00:31:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 100/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0220  data_time: 0.0062  memory: 2032  loss: 2.3049\n",
      "11/24 00:31:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 110/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0230  data_time: 0.0068  memory: 2032  loss: 2.3076\n",
      "11/24 00:31:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 120/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3316\n",
      "11/24 00:31:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 130/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3278\n",
      "11/24 00:31:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 140/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3392\n",
      "11/24 00:31:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 150/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3142\n",
      "11/24 00:31:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 160/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3277\n",
      "11/24 00:31:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 170/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3227\n",
      "11/24 00:31:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 180/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3247\n",
      "11/24 00:31:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 190/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3223\n",
      "11/24 00:31:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 200/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0232  data_time: 0.0069  memory: 2032  loss: 2.3295\n",
      "11/24 00:31:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 210/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3322\n",
      "11/24 00:31:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 220/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3195\n",
      "11/24 00:31:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 230/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3166\n",
      "11/24 00:31:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 240/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3119\n",
      "11/24 00:31:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 250/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3332\n",
      "11/24 00:31:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 260/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3180\n",
      "11/24 00:31:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 270/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3130\n",
      "11/24 00:31:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 280/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3171\n",
      "11/24 00:31:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 290/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0229  data_time: 0.0068  memory: 2032  loss: 2.3165\n",
      "11/24 00:31:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 300/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3119\n",
      "11/24 00:31:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 310/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3254\n",
      "11/24 00:31:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 320/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3256\n",
      "11/24 00:31:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 330/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3341\n",
      "11/24 00:31:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 340/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3254\n",
      "11/24 00:31:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 350/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3109\n",
      "11/24 00:31:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 360/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3186\n",
      "11/24 00:31:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 370/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3296\n",
      "11/24 00:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 380/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0234  data_time: 0.0072  memory: 2032  loss: 2.3110\n",
      "11/24 00:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 390/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3245\n",
      "11/24 00:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 400/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3085\n",
      "11/24 00:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 410/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0229  data_time: 0.0068  memory: 2032  loss: 2.3264\n",
      "11/24 00:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 420/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3236\n",
      "11/24 00:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 430/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3259\n",
      "11/24 00:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 440/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.3094\n",
      "11/24 00:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 450/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3263\n",
      "11/24 00:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 460/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3290\n",
      "11/24 00:31:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 470/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0235  data_time: 0.0069  memory: 2032  loss: 2.3261\n",
      "11/24 00:31:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 480/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3335\n",
      "11/24 00:31:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 490/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0225  data_time: 0.0064  memory: 2032  loss: 2.3315\n",
      "11/24 00:31:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 500/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3231\n",
      "11/24 00:31:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 510/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3264\n",
      "11/24 00:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 520/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3365\n",
      "11/24 00:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 530/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3411\n",
      "11/24 00:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 540/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3300\n",
      "11/24 00:31:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 550/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3206\n",
      "11/24 00:31:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 560/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0233  data_time: 0.0071  memory: 2032  loss: 2.3310\n",
      "11/24 00:31:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 570/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3263\n",
      "11/24 00:31:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 580/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3199\n",
      "11/24 00:31:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 590/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0239  data_time: 0.0077  memory: 2032  loss: 2.3455\n",
      "11/24 00:31:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 600/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3228\n",
      "11/24 00:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 610/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3475\n",
      "11/24 00:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 620/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3187\n",
      "11/24 00:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 630/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3479\n",
      "11/24 00:31:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 640/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3167\n",
      "11/24 00:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 650/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0235  data_time: 0.0069  memory: 2032  loss: 2.3305\n",
      "11/24 00:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 660/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3227\n",
      "11/24 00:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 670/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0225  data_time: 0.0063  memory: 2032  loss: 2.3033\n",
      "11/24 00:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 680/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0225  data_time: 0.0065  memory: 2032  loss: 2.3335\n",
      "11/24 00:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 690/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3096\n",
      "11/24 00:31:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 700/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3289\n",
      "11/24 00:31:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 710/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3115\n",
      "11/24 00:31:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 720/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3154\n",
      "11/24 00:31:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 730/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3162\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 740/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0236  data_time: 0.0073  memory: 2032  loss: 2.3379\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 750/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3083\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 760/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0226  data_time: 0.0066  memory: 2032  loss: 2.3256\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 770/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3332\n",
      "11/24 00:31:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 780/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3314\n",
      "11/24 00:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 790/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3371\n",
      "11/24 00:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 800/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3233\n",
      "11/24 00:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 810/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0223  data_time: 0.0064  memory: 2032  loss: 2.3371\n",
      "11/24 00:31:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 820/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0233  data_time: 0.0068  memory: 2032  loss: 2.3385\n",
      "11/24 00:31:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 830/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3228\n",
      "11/24 00:31:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 840/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3260\n",
      "11/24 00:31:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 850/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0224  data_time: 0.0064  memory: 2032  loss: 2.3218\n",
      "11/24 00:31:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 860/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3308\n",
      "11/24 00:31:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 870/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3343\n",
      "11/24 00:31:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 880/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3295\n",
      "11/24 00:31:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 890/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3130\n",
      "11/24 00:31:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 900/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3233\n",
      "11/24 00:31:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 910/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3368\n",
      "11/24 00:31:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 920/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3131\n",
      "11/24 00:31:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 930/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3415\n",
      "11/24 00:31:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 940/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0234  data_time: 0.0073  memory: 2032  loss: 2.3094\n",
      "11/24 00:31:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 950/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3363\n",
      "11/24 00:31:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 960/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3103\n",
      "11/24 00:31:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 970/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3323\n",
      "11/24 00:31:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 980/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0223  data_time: 0.0061  memory: 2032  loss: 2.3247\n",
      "11/24 00:31:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][ 990/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0240  data_time: 0.0078  memory: 2032  loss: 2.3212\n",
      "11/24 00:31:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1000/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0231  data_time: 0.0065  memory: 2032  loss: 2.3149\n",
      "11/24 00:31:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1010/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3184\n",
      "11/24 00:31:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1020/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0226  data_time: 0.0065  memory: 2032  loss: 2.3163\n",
      "11/24 00:31:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1030/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3119\n",
      "11/24 00:31:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1040/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3101\n",
      "11/24 00:31:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1050/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0219  data_time: 0.0061  memory: 2032  loss: 2.3435\n",
      "11/24 00:31:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1060/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3183\n",
      "11/24 00:31:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1070/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0224  data_time: 0.0063  memory: 2032  loss: 2.3238\n",
      "11/24 00:31:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1080/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3276\n",
      "11/24 00:31:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1090/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0235  data_time: 0.0070  memory: 2032  loss: 2.3305\n",
      "11/24 00:31:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1100/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3175\n",
      "11/24 00:31:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1110/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3265\n",
      "11/24 00:31:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1120/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3226\n",
      "11/24 00:31:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1130/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3303\n",
      "11/24 00:31:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1140/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3400\n",
      "11/24 00:31:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1150/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0222  data_time: 0.0063  memory: 2032  loss: 2.3107\n",
      "11/24 00:31:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1160/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3264\n",
      "11/24 00:31:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1170/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0223  data_time: 0.0062  memory: 2032  loss: 2.3165\n",
      "11/24 00:31:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1180/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0234  data_time: 0.0069  memory: 2032  loss: 2.3028\n",
      "11/24 00:31:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1190/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3176\n",
      "11/24 00:31:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1200/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3339\n",
      "11/24 00:31:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1210/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3190\n",
      "11/24 00:31:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1220/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3201\n",
      "11/24 00:31:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1230/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3187\n",
      "11/24 00:31:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1240/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3203\n",
      "11/24 00:31:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1250/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0219  data_time: 0.0060  memory: 2032  loss: 2.3278\n",
      "11/24 00:31:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1260/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3174\n",
      "11/24 00:31:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1270/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0232  data_time: 0.0070  memory: 2032  loss: 2.3255\n",
      "11/24 00:31:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1280/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3102\n",
      "11/24 00:31:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1290/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3085\n",
      "11/24 00:31:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1300/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3133\n",
      "11/24 00:31:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1310/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0225  data_time: 0.0063  memory: 2032  loss: 2.3224\n",
      "11/24 00:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1320/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3156\n",
      "11/24 00:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1330/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3119\n",
      "11/24 00:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1340/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3247\n",
      "11/24 00:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1350/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0235  data_time: 0.0069  memory: 2032  loss: 2.3312\n",
      "11/24 00:31:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1360/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.3313\n",
      "11/24 00:31:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1370/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3244\n",
      "11/24 00:31:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1380/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0220  data_time: 0.0060  memory: 2032  loss: 2.3300\n",
      "11/24 00:31:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1390/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3224\n",
      "11/24 00:31:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1400/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3328\n",
      "11/24 00:31:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1410/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3222\n",
      "11/24 00:31:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1420/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3255\n",
      "11/24 00:31:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1430/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0234  data_time: 0.0068  memory: 2032  loss: 2.3286\n",
      "11/24 00:31:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1440/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3251\n",
      "11/24 00:31:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1450/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0224  data_time: 0.0061  memory: 2032  loss: 2.3106\n",
      "11/24 00:31:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1460/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3209\n",
      "11/24 00:31:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1470/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3243\n",
      "11/24 00:31:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1480/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3209\n",
      "11/24 00:31:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1490/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0220  data_time: 0.0061  memory: 2032  loss: 2.3308\n",
      "11/24 00:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1500/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0221  data_time: 0.0062  memory: 2032  loss: 2.2979\n",
      "11/24 00:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1510/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0236  data_time: 0.0071  memory: 2032  loss: 2.3436\n",
      "11/24 00:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1520/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0221  data_time: 0.0061  memory: 2032  loss: 2.3366\n",
      "11/24 00:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1530/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3220\n",
      "11/24 00:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1540/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0233  data_time: 0.0072  memory: 2032  loss: 2.3479\n",
      "11/24 00:31:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1550/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0222  data_time: 0.0062  memory: 2032  loss: 2.3176\n",
      "11/24 00:31:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [5][1560/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0223  data_time: 0.0063  memory: 2032  loss: 2.3361\n",
      "11/24 00:31:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20231124_002739\n",
      "11/24 00:31:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "11/24 00:32:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 10/313]    eta: 0:00:02  time: 0.0072  data_time: 0.0038  memory: 2032  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 20/313]    eta: 0:00:04  time: 0.0238  data_time: 0.0203  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 30/313]    eta: 0:00:03  time: 0.0069  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 40/313]    eta: 0:00:03  time: 0.0069  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 50/313]    eta: 0:00:02  time: 0.0069  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 60/313]    eta: 0:00:02  time: 0.0073  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 70/313]    eta: 0:00:02  time: 0.0083  data_time: 0.0052  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 80/313]    eta: 0:00:02  time: 0.0073  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][ 90/313]    eta: 0:00:02  time: 0.0070  data_time: 0.0041  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][100/313]    eta: 0:00:01  time: 0.0075  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][110/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][120/313]    eta: 0:00:01  time: 0.0074  data_time: 0.0044  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][130/313]    eta: 0:00:01  time: 0.0084  data_time: 0.0051  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][140/313]    eta: 0:00:01  time: 0.0066  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:32:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][150/313]    eta: 0:00:01  time: 0.0065  data_time: 0.0038  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][160/313]    eta: 0:00:01  time: 0.0065  data_time: 0.0038  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][170/313]    eta: 0:00:01  time: 0.0067  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][180/313]    eta: 0:00:01  time: 0.0065  data_time: 0.0038  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][190/313]    eta: 0:00:00  time: 0.0065  data_time: 0.0038  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][200/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][210/313]    eta: 0:00:00  time: 0.0066  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][220/313]    eta: 0:00:00  time: 0.0065  data_time: 0.0038  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][230/313]    eta: 0:00:00  time: 0.0078  data_time: 0.0047  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][240/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][250/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][260/313]    eta: 0:00:00  time: 0.0070  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][270/313]    eta: 0:00:00  time: 0.0068  data_time: 0.0040  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][280/313]    eta: 0:00:00  time: 0.0067  data_time: 0.0039  memory: 1619  \n",
      "11/24 00:32:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][290/313]    eta: 0:00:00  time: 0.0083  data_time: 0.0047  memory: 1619  \n",
      "11/24 00:32:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][300/313]    eta: 0:00:00  time: 0.0075  data_time: 0.0043  memory: 1619  \n",
      "11/24 00:32:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][310/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0042  memory: 1619  \n",
      "11/24 00:32:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][313/313]    accuracy: 10.0000  data_time: 0.0047  time: 0.0076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMVGG16(\n",
       "  (data_preprocessor): BaseDataPreprocessor()\n",
       "  (resnet): VGG(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MMVGG16(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = torchvision.models.vgg16()\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.resnet(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels\n",
    "\n",
    "a = MMVGG16()\n",
    "a\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "    def process(self, data_batch, data_samples):\n",
    "        score, gt = data_samples\n",
    "        self.results.append({\n",
    "            'batch_size': len(gt),\n",
    "            'correct': (score.argmax(dim=1) == gt).sum().cpu(),\n",
    "        })\n",
    "\n",
    "    def compute_metrics(self, results):\n",
    "        total_correct = sum(item['correct'] for item in results)\n",
    "        total_size = sum(item['batch_size'] for item in results)\n",
    "        return dict(accuracy=100 * total_correct / total_size)\n",
    "\n",
    "\n",
    "norm_cfg = dict(mean=[124.508, 116.050, 106.438], std=[58.577, 57.310, 57.437])\n",
    "train_dataloader = DataLoader(batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              dataset=torchvision.datasets.CIFAR10(\n",
    "                                  'data/cifar10',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(**norm_cfg)\n",
    "                                  ])))\n",
    "\n",
    "val_dataloader = DataLoader(batch_size=32,\n",
    "                            shuffle=False,\n",
    "                            dataset=torchvision.datasets.CIFAR10(\n",
    "                                'data/cifar10',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(**norm_cfg)\n",
    "                                ])))\n",
    "\n",
    "runner = Runner(\n",
    "    model=MMVGG16(),\n",
    "    work_dir='./work_dir',\n",
    "    train_dataloader=train_dataloader,\n",
    "    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n",
    "    train_cfg=dict(by_epoch=True, max_epochs=5, val_interval=1),\n",
    "    val_dataloader=val_dataloader,\n",
    "    val_cfg=dict(),\n",
    "    val_evaluator=dict(type=Accuracy),\n",
    ")\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "60e31b73-5484-43c5-a44a-8fb0e83c3cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/vgg/vgg19_alzheimer_axial_view.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/vgg/vgg19_alzheimer_axial_view.py\n",
    "_base_ = [\n",
    "    '../_base_/models/vgg16.py',\n",
    "    '../_base_/schedules/imagenet_bs1024_adamw_conformer.py',\n",
    "    # '../_base_/schedules/imagenet_bs256.py',\n",
    "    '../_base_/default_runtime.py'\n",
    "]\n",
    "\n",
    "pretrained = 'https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth'\n",
    "\n",
    "model = dict(\n",
    "    type='ImageClassifier', \n",
    "    \n",
    "    backbone=dict(\n",
    "        type='VGG',\n",
    "        depth=19,\n",
    "        frozen_stages=-1,\n",
    "        num_classes=3,\n",
    "        norm_eval=True,\n",
    "        # style='pytorch',\n",
    "        init_cfg = dict(\n",
    "            type='Pretrained', \n",
    "            checkpoint=pretrained,\n",
    "            prefix='backbone',\n",
    "        )\n",
    "    ),\n",
    "    neck=None,\n",
    "    head=dict(\n",
    "        # type='MultiLabelClsHead',\n",
    "        # loss=dict(type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
    "        topk = (1, 3),\n",
    "        # cal_acc=True,\n",
    "    ))\n",
    "\n",
    "dataset_type = 'CustomDataset'\n",
    "data_preprocessor = dict(\n",
    "     mean=[124.508, 116.050, 106.438],\n",
    "     std=[58.577, 57.310, 57.437],\n",
    "     to_rgb=False)\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),     # read image\n",
    "    dict(type='ResizeEdge', scale=256),  \n",
    "    dict(type='RandomResizedCrop', scale=224),     # Random scaling and cropping\n",
    "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),   # random horizontal flip\n",
    "    dict(type='PackInputs'),         # prepare images and labels\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),     # read image\n",
    "    dict(type='ResizeEdge', scale=256),  \n",
    "    dict(type='CenterCrop', crop_size=244),     # center crop\n",
    "    dict(type='PackInputs'),                 # prepare images and labels\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    num_workers=5,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='data',\n",
    "        data_prefix='YoriDataset_vgg/train',\n",
    "        classes='data/classes.txt',\n",
    "        ann_file='train_ann.txt',\n",
    "        with_label=True,\n",
    "        pipeline=train_pipeline\n",
    "    ),\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    num_workers=5,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='data',\n",
    "        data_prefix='YoriDataset_vgg/validation',\n",
    "        classes='data/classes.txt',\n",
    "        ann_file='val_ann.txt',\n",
    "        with_label=True,\n",
    "        pipeline=test_pipeline\n",
    "    ),\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    num_workers=5,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='data',\n",
    "        data_prefix='YoriDataset_vgg/test',\n",
    "        classes='data/classes.txt',\n",
    "        ann_file='test_ann.txt',\n",
    "        with_label=True,\n",
    "        pipeline=test_pipeline\n",
    "    ),    \n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_evaluator = dict(type='Accuracy', topk=(1, 3))\n",
    "\n",
    "test_evaluator = val_evaluator \n",
    "\n",
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "        type='AdamW',\n",
    "        # for batch in each gpu is 128, 8 gpu\n",
    "        # lr = 5e-4 * 128 * 8 / 512 = 0.001\n",
    "        lr=5e-4 * 128 * 8 / 512,\n",
    "        weight_decay=0.05,\n",
    "        eps=1e-8,\n",
    "        betas=(0.9, 0.999)),\n",
    "    paramwise_cfg=dict(\n",
    "        norm_decay_mult=0.0,\n",
    "        bias_decay_mult=0.0,\n",
    "        custom_keys={\n",
    "            '.cls_token': dict(decay_mult=0.0),\n",
    "        }),\n",
    ")\n",
    "\n",
    "# learning policy\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR',\n",
    "        start_factor=1e-3,\n",
    "        by_epoch=True,\n",
    "        begin=0,\n",
    "        end=5,\n",
    "        convert_to_iter_based=True),\n",
    "    dict(\n",
    "        type='CosineAnnealingLR',\n",
    "        T_max=295,\n",
    "        eta_min=1e-5,\n",
    "        by_epoch=True,\n",
    "        begin=5,\n",
    "        end=300)\n",
    "]\n",
    "\n",
    "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
    "val_cfg = dict()\n",
    "test_cfg = dict()\n",
    "\n",
    "auto_scale_lr = dict(base_batch_size=256)\n",
    "\n",
    "default_scope = 'mmpretrain'\n",
    "\n",
    "# configure default hooks\n",
    "default_hooks = dict(\n",
    "    # record the time of every iteration.\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "\n",
    "    # print log every 100 iterations.\n",
    "    logger=dict(type='LoggerHook', interval=100),\n",
    "\n",
    "    # enable the parameter scheduler.\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "\n",
    "    # save checkpoint per epoch.\n",
    "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
    "\n",
    "    # set sampler seed in a distributed environment.\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "\n",
    "    # validation results visualization, set True to enable it.\n",
    "    visualization=dict(type='VisualizationHook', enable=False),\n",
    ")\n",
    "\n",
    "env_cfg = dict(\n",
    "    # whether to enable cudnn benchmark\n",
    "    cudnn_benchmark=False,\n",
    "\n",
    "    # set multi-process parameters\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "\n",
    "    # set distributed parameters\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "# set visualizer\n",
    "vis_backends = [dict(type='LocalVisBackend')]  # use local HDD backend\n",
    "visualizer = dict(type='UniversalVisualizer', vis_backends=vis_backends, name='visualizer')\n",
    "\n",
    "# set log level\n",
    "log_level = 'INFO'\n",
    "\n",
    "# load from which checkpoint\n",
    "load_from = None\n",
    "\n",
    "# whether to resume training from the loaded checkpoint\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5cf77-2b0c-4780-9015-01389e8e752f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e952c46b-6dfb-49be-bced-2fd1a5a43f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/vgg/vgg19_alzheimer_axial_view.py\n",
      "11/26 01:21:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1629880455\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda-11.4\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.100\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.10.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.9.0\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1629880455\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "11/26 01:21:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=256)\n",
      "data_preprocessor = dict(\n",
      "    mean=[\n",
      "        124.508,\n",
      "        116.05,\n",
      "        106.438,\n",
      "    ],\n",
      "    std=[\n",
      "        58.577,\n",
      "        57.31,\n",
      "        57.437,\n",
      "    ],\n",
      "    to_rgb=False)\n",
      "dataset_type = 'CustomDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=100, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(enable=False, type='VisualizationHook'))\n",
      "default_scope = 'mmpretrain'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "launcher = 'none'\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=19,\n",
      "        frozen_stages=-1,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth',\n",
      "            prefix='backbone',\n",
      "            type='Pretrained'),\n",
      "        norm_eval=True,\n",
      "        num_classes=3,\n",
      "        type='VGG'),\n",
      "    head=dict(\n",
      "        loss=dict(loss_weight=1.0, type='CrossEntropyLoss'),\n",
      "        topk=(\n",
      "            1,\n",
      "            3,\n",
      "        ),\n",
      "        type='ClsHead'),\n",
      "    neck=None,\n",
      "    type='ImageClassifier')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ),\n",
      "        eps=1e-08,\n",
      "        lr=0.001,\n",
      "        type='AdamW',\n",
      "        weight_decay=0.05),\n",
      "    paramwise_cfg=dict(\n",
      "        bias_decay_mult=0.0,\n",
      "        custom_keys=dict({'.cls_token': dict(decay_mult=0.0)}),\n",
      "        norm_decay_mult=0.0))\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        convert_to_iter_based=True,\n",
      "        end=5,\n",
      "        start_factor=0.001,\n",
      "        type='LinearLR'),\n",
      "    dict(\n",
      "        T_max=295,\n",
      "        begin=5,\n",
      "        by_epoch=True,\n",
      "        end=300,\n",
      "        eta_min=1e-05,\n",
      "        type='CosineAnnealingLR'),\n",
      "]\n",
      "pretrained = 'https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth'\n",
      "randomness = dict(deterministic=False, seed=None)\n",
      "resume = False\n",
      "test_cfg = dict()\n",
      "test_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        ann_file='test_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='YoriDataset_vgg/test',\n",
      "        data_root='data',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=256, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    topk=(\n",
      "        1,\n",
      "        3,\n",
      "    ), type='Accuracy')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=256, type='ResizeEdge'),\n",
      "    dict(crop_size=244, type='CenterCrop'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        ann_file='train_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='YoriDataset_vgg/train',\n",
      "        data_root='data',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=256, type='ResizeEdge'),\n",
      "            dict(scale=224, type='RandomResizedCrop'),\n",
      "            dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=256, type='ResizeEdge'),\n",
      "    dict(scale=224, type='RandomResizedCrop'),\n",
      "    dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "val_cfg = dict()\n",
      "val_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        ann_file='val_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='YoriDataset_vgg/validation',\n",
      "        data_root='data',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=256, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=5,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    topk=(\n",
      "        1,\n",
      "        3,\n",
      "    ), type='Accuracy')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='UniversalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = 'work_dirs/alzheimer/axial/vgg16'\n",
      "\n",
      "11/26 01:21:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "11/26 01:21:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.0.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.1.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.3.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.4.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.6.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.7.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.8.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.9.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.11.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.12.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.13.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.14.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.16.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.17.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.18.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.19.conv.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.0.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.3.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.6.bias:weight_decay=0.0\n",
      "11/26 01:21:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load backbone in model from: https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth\n",
      "Downloading: \"https://download.openmmlab.com/mmclassification/v0/vgg/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth\" to /home/user/.cache/torch/hub/checkpoints/vgg19_bn_batch256_imagenet_20210208-da620c4f.pth\n",
      "100%|| 548M/548M [00:34<00:00, 16.7MB/s]\n",
      "11/26 01:22:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for classifier.6.weight: copying a param with shape torch.Size([1000, 4096]) from checkpoint, the shape in current model is torch.Size([3, 4096]).\n",
      "size mismatch for classifier.6.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "unexpected key in source state_dict: features.0.bn.weight, features.0.bn.bias, features.0.bn.running_mean, features.0.bn.running_var, features.0.bn.num_batches_tracked, features.1.bn.weight, features.1.bn.bias, features.1.bn.running_mean, features.1.bn.running_var, features.1.bn.num_batches_tracked, features.3.bn.weight, features.3.bn.bias, features.3.bn.running_mean, features.3.bn.running_var, features.3.bn.num_batches_tracked, features.4.bn.weight, features.4.bn.bias, features.4.bn.running_mean, features.4.bn.running_var, features.4.bn.num_batches_tracked, features.6.bn.weight, features.6.bn.bias, features.6.bn.running_mean, features.6.bn.running_var, features.6.bn.num_batches_tracked, features.7.bn.weight, features.7.bn.bias, features.7.bn.running_mean, features.7.bn.running_var, features.7.bn.num_batches_tracked, features.8.bn.weight, features.8.bn.bias, features.8.bn.running_mean, features.8.bn.running_var, features.8.bn.num_batches_tracked, features.9.bn.weight, features.9.bn.bias, features.9.bn.running_mean, features.9.bn.running_var, features.9.bn.num_batches_tracked, features.11.bn.weight, features.11.bn.bias, features.11.bn.running_mean, features.11.bn.running_var, features.11.bn.num_batches_tracked, features.12.bn.weight, features.12.bn.bias, features.12.bn.running_mean, features.12.bn.running_var, features.12.bn.num_batches_tracked, features.13.bn.weight, features.13.bn.bias, features.13.bn.running_mean, features.13.bn.running_var, features.13.bn.num_batches_tracked, features.14.bn.weight, features.14.bn.bias, features.14.bn.running_mean, features.14.bn.running_var, features.14.bn.num_batches_tracked, features.16.bn.weight, features.16.bn.bias, features.16.bn.running_mean, features.16.bn.running_var, features.16.bn.num_batches_tracked, features.17.bn.weight, features.17.bn.bias, features.17.bn.running_mean, features.17.bn.running_var, features.17.bn.num_batches_tracked, features.18.bn.weight, features.18.bn.bias, features.18.bn.running_mean, features.18.bn.running_var, features.18.bn.num_batches_tracked, features.19.bn.weight, features.19.bn.bias, features.19.bn.running_mean, features.19.bn.running_var, features.19.bn.num_batches_tracked\n",
      "\n",
      "11/26 01:22:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "11/26 01:22:21 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "11/26 01:22:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/user/mmpretrain/work_dirs/alzheimer/axial/vgg16.\n",
      "11/26 01:22:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:22:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][38/38]  base_lr: 1.9657e-04 lr: 1.9657e-04  eta: 0:26:04  time: 0.1421  data_time: 0.0004  memory: 4922  loss: 1.1033\n",
      "11/26 01:22:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "11/26 01:24:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][9/9]    accuracy/top1: 34.8485  accuracy/top3: 100.0000  data_time: 1.6571  time: 3.0041\n",
      "11/26 01:24:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:24:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][38/38]  base_lr: 3.9743e-04 lr: 3.9743e-04  eta: 0:20:12  time: 0.1394  data_time: 0.0004  memory: 4922  loss: 1.1070\n",
      "11/26 01:24:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "11/26 01:25:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 1.0875  time: 1.5438\n",
      "11/26 01:25:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:25:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][38/38]  base_lr: 5.9829e-04 lr: 5.9829e-04  eta: 0:17:11  time: 0.1397  data_time: 0.0004  memory: 4922  loss: 1.1162\n",
      "11/26 01:25:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "11/26 01:26:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 1.1536  time: 1.3941\n",
      "11/26 01:26:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:26:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][38/38]  base_lr: 7.9914e-04 lr: 7.9914e-04  eta: 0:15:09  time: 0.1387  data_time: 0.0004  memory: 4922  loss: 1.0965\n",
      "11/26 01:26:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "11/26 01:27:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.6163  time: 0.7788\n",
      "11/26 01:27:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:27:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][38/38]  base_lr: 1.0000e-03 lr: 1.0000e-03  eta: 0:14:43  time: 0.1398  data_time: 0.0004  memory: 4922  loss: 1.1010\n",
      "11/26 01:27:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "11/26 01:27:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.2407  time: 0.3117\n",
      "11/26 01:27:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:27:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][38/38]  base_lr: 1.0000e-03 lr: 1.0000e-03  eta: 0:13:44  time: 0.1395  data_time: 0.0004  memory: 4922  loss: 1.1200\n",
      "11/26 01:27:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 6 epochs\n",
      "11/26 01:27:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [6][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1802  time: 0.2426\n",
      "11/26 01:28:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:28:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][38/38]  base_lr: 9.9997e-04 lr: 9.9997e-04  eta: 0:13:06  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1244\n",
      "11/26 01:28:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 7 epochs\n",
      "11/26 01:28:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [7][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1932  time: 0.2724\n",
      "11/26 01:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][38/38]  base_lr: 9.9989e-04 lr: 9.9989e-04  eta: 0:12:34  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1029\n",
      "11/26 01:28:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 8 epochs\n",
      "11/26 01:28:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [8][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.1222  time: 0.1947\n",
      "11/26 01:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [9][38/38]  base_lr: 9.9975e-04 lr: 9.9975e-04  eta: 0:12:19  time: 0.1387  data_time: 0.0003  memory: 4922  loss: 1.1379\n",
      "11/26 01:28:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 9 epochs\n",
      "11/26 01:29:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [9][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0364  time: 0.0926\n",
      "11/26 01:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [10][38/38]  base_lr: 9.9955e-04 lr: 9.9955e-04  eta: 0:11:51  time: 0.1386  data_time: 0.0002  memory: 4922  loss: 1.1068\n",
      "11/26 01:29:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n",
      "11/26 01:29:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0440  time: 0.1059\n",
      "11/26 01:29:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:29:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [11][38/38]  base_lr: 9.9930e-04 lr: 9.9930e-04  eta: 0:11:27  time: 0.1386  data_time: 0.0002  memory: 4922  loss: 1.1071\n",
      "11/26 01:29:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 11 epochs\n",
      "11/26 01:30:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [11][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0411  time: 0.1020\n",
      "11/26 01:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [12][38/38]  base_lr: 9.9899e-04 lr: 9.9899e-04  eta: 0:11:07  time: 0.1389  data_time: 0.0002  memory: 4922  loss: 1.1050\n",
      "11/26 01:30:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 12 epochs\n",
      "11/26 01:30:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [12][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0450  time: 0.1034\n",
      "11/26 01:30:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:30:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [13][38/38]  base_lr: 9.9863e-04 lr: 9.9863e-04  eta: 0:10:49  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1112\n",
      "11/26 01:30:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 13 epochs\n",
      "11/26 01:30:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [13][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1081  time: 0.1723\n",
      "11/26 01:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [14][38/38]  base_lr: 9.9820e-04 lr: 9.9820e-04  eta: 0:10:32  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.0944\n",
      "11/26 01:30:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 14 epochs\n",
      "11/26 01:31:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [14][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0388  time: 0.0969\n",
      "11/26 01:31:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:31:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [15][38/38]  base_lr: 9.9773e-04 lr: 9.9773e-04  eta: 0:10:17  time: 0.1389  data_time: 0.0002  memory: 4922  loss: 1.1023\n",
      "11/26 01:31:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 15 epochs\n",
      "11/26 01:31:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [15][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0515  time: 0.1089\n",
      "11/26 01:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [16][38/38]  base_lr: 9.9720e-04 lr: 9.9720e-04  eta: 0:10:03  time: 0.1387  data_time: 0.0002  memory: 4922  loss: 1.1013\n",
      "11/26 01:31:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 16 epochs\n",
      "11/26 01:31:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [16][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0425  time: 0.1037\n",
      "11/26 01:31:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:31:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [17][38/38]  base_lr: 9.9661e-04 lr: 9.9661e-04  eta: 0:09:51  time: 0.1384  data_time: 0.0002  memory: 4922  loss: 1.1041\n",
      "11/26 01:31:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 17 epochs\n",
      "11/26 01:32:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [17][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0469  time: 0.1065\n",
      "11/26 01:32:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:32:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [18][38/38]  base_lr: 9.9596e-04 lr: 9.9596e-04  eta: 0:09:38  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1117\n",
      "11/26 01:32:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 18 epochs\n",
      "11/26 01:32:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [18][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0571  time: 0.1190\n",
      "11/26 01:32:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:32:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [19][38/38]  base_lr: 9.9526e-04 lr: 9.9526e-04  eta: 0:09:27  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.0995\n",
      "11/26 01:32:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 19 epochs\n",
      "11/26 01:33:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [19][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1863  time: 0.2559\n",
      "11/26 01:33:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:33:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [20][38/38]  base_lr: 9.9451e-04 lr: 9.9451e-04  eta: 0:09:17  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.0999\n",
      "11/26 01:33:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 20 epochs\n",
      "11/26 01:33:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.2834  time: 0.3698\n",
      "11/26 01:33:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:33:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [21][38/38]  base_lr: 9.9370e-04 lr: 9.9370e-04  eta: 0:09:25  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1082\n",
      "11/26 01:33:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 21 epochs\n",
      "11/26 01:33:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [21][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0405  time: 0.0997\n",
      "11/26 01:34:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:34:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [22][38/38]  base_lr: 9.9283e-04 lr: 9.9283e-04  eta: 0:09:15  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1034\n",
      "11/26 01:34:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 22 epochs\n",
      "11/26 01:34:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [22][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.3347  time: 0.4333\n",
      "11/26 01:34:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:34:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [23][38/38]  base_lr: 9.9191e-04 lr: 9.9191e-04  eta: 0:09:24  time: 0.2988  data_time: 0.1608  memory: 4922  loss: 1.1060\n",
      "11/26 01:34:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 23 epochs\n",
      "11/26 01:35:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [23][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0626  time: 0.1287\n",
      "11/26 01:35:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:35:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [24][38/38]  base_lr: 9.9093e-04 lr: 9.9093e-04  eta: 0:09:16  time: 0.1387  data_time: 0.0005  memory: 4922  loss: 1.0987\n",
      "11/26 01:35:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 24 epochs\n",
      "11/26 01:35:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [24][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1448  time: 0.2206\n",
      "11/26 01:35:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:35:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [25][38/38]  base_lr: 9.8990e-04 lr: 9.8990e-04  eta: 0:09:06  time: 0.1383  data_time: 0.0003  memory: 4922  loss: 1.0964\n",
      "11/26 01:35:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 25 epochs\n",
      "11/26 01:35:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [25][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0491  time: 0.1118\n",
      "11/26 01:36:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:36:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [26][38/38]  base_lr: 9.8881e-04 lr: 9.8881e-04  eta: 0:08:55  time: 0.1394  data_time: 0.0005  memory: 4922  loss: 1.1079\n",
      "11/26 01:36:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 26 epochs\n",
      "11/26 01:36:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [26][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.1482  time: 0.2143\n",
      "11/26 01:36:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:36:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:36:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [27][38/38]  base_lr: 9.8767e-04 lr: 9.8767e-04  eta: 0:08:52  time: 0.1394  data_time: 0.0003  memory: 4922  loss: 1.1082\n",
      "11/26 01:36:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 27 epochs\n",
      "11/26 01:36:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [27][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1967  time: 0.2605\n",
      "11/26 01:37:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:37:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [28][38/38]  base_lr: 9.8648e-04 lr: 9.8648e-04  eta: 0:08:50  time: 0.1392  data_time: 0.0003  memory: 4922  loss: 1.1019\n",
      "11/26 01:37:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 28 epochs\n",
      "11/26 01:37:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [28][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0725  time: 0.1547\n",
      "11/26 01:37:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:37:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [29][38/38]  base_lr: 9.8523e-04 lr: 9.8523e-04  eta: 0:08:40  time: 0.1393  data_time: 0.0003  memory: 4922  loss: 1.1013\n",
      "11/26 01:37:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 29 epochs\n",
      "11/26 01:37:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [29][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0924  time: 0.1539\n",
      "11/26 01:37:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:37:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [30][38/38]  base_lr: 9.8392e-04 lr: 9.8392e-04  eta: 0:08:30  time: 0.1389  data_time: 0.0005  memory: 4922  loss: 1.0972\n",
      "11/26 01:37:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 30 epochs\n",
      "11/26 01:38:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0857  time: 0.1451\n",
      "11/26 01:38:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:38:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [31][38/38]  base_lr: 9.8256e-04 lr: 9.8256e-04  eta: 0:08:21  time: 0.1385  data_time: 0.0003  memory: 4922  loss: 1.1064\n",
      "11/26 01:38:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 31 epochs\n",
      "11/26 01:38:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [31][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.1052  time: 0.1642\n",
      "11/26 01:38:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:38:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [32][38/38]  base_lr: 9.8115e-04 lr: 9.8115e-04  eta: 0:08:10  time: 0.1382  data_time: 0.0002  memory: 4922  loss: 1.0999\n",
      "11/26 01:38:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 32 epochs\n",
      "11/26 01:39:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [32][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.1065  time: 0.1704\n",
      "11/26 01:39:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:39:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [33][38/38]  base_lr: 9.7968e-04 lr: 9.7968e-04  eta: 0:08:02  time: 0.1385  data_time: 0.0003  memory: 4922  loss: 1.0977\n",
      "11/26 01:39:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 33 epochs\n",
      "11/26 01:39:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [33][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0685  time: 0.1253\n",
      "11/26 01:39:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:39:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [34][38/38]  base_lr: 9.7816e-04 lr: 9.7816e-04  eta: 0:07:53  time: 0.1383  data_time: 0.0003  memory: 4922  loss: 1.0990\n",
      "11/26 01:39:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 34 epochs\n",
      "11/26 01:39:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [34][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0426  time: 0.0992\n",
      "11/26 01:40:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:40:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [35][38/38]  base_lr: 9.7658e-04 lr: 9.7658e-04  eta: 0:07:44  time: 0.1383  data_time: 0.0003  memory: 4922  loss: 1.1031\n",
      "11/26 01:40:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 35 epochs\n",
      "11/26 01:40:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [35][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0338  time: 0.0932\n",
      "11/26 01:40:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:40:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [36][38/38]  base_lr: 9.7495e-04 lr: 9.7495e-04  eta: 0:07:34  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1050\n",
      "11/26 01:40:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 36 epochs\n",
      "11/26 01:40:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [36][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0390  time: 0.0973\n",
      "11/26 01:40:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:40:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [37][38/38]  base_lr: 9.7327e-04 lr: 9.7327e-04  eta: 0:07:26  time: 0.1384  data_time: 0.0002  memory: 4922  loss: 1.1012\n",
      "11/26 01:40:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 37 epochs\n",
      "11/26 01:41:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [37][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0608  time: 0.1178\n",
      "11/26 01:41:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:41:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [38][38/38]  base_lr: 9.7153e-04 lr: 9.7153e-04  eta: 0:07:16  time: 0.1384  data_time: 0.0002  memory: 4922  loss: 1.1024\n",
      "11/26 01:41:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 38 epochs\n",
      "11/26 01:41:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [38][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0262  time: 0.0806\n",
      "11/26 01:41:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:41:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [39][38/38]  base_lr: 9.6975e-04 lr: 9.6975e-04  eta: 0:07:08  time: 0.1383  data_time: 0.0003  memory: 4922  loss: 1.1008\n",
      "11/26 01:41:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 39 epochs\n",
      "11/26 01:41:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [39][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0611  time: 0.1144\n",
      "11/26 01:41:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:41:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [40][38/38]  base_lr: 9.6790e-04 lr: 9.6790e-04  eta: 0:06:59  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1056\n",
      "11/26 01:41:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 40 epochs\n",
      "11/26 01:42:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0417  time: 0.1006\n",
      "11/26 01:42:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:42:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [41][38/38]  base_lr: 9.6601e-04 lr: 9.6601e-04  eta: 0:06:50  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1031\n",
      "11/26 01:42:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 41 epochs\n",
      "11/26 01:42:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [41][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0202  time: 0.0756\n",
      "11/26 01:42:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:42:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [42][38/38]  base_lr: 9.6407e-04 lr: 9.6407e-04  eta: 0:06:42  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1011\n",
      "11/26 01:42:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 42 epochs\n",
      "11/26 01:42:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [42][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0270  time: 0.0855\n",
      "11/26 01:42:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:42:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [43][38/38]  base_lr: 9.6207e-04 lr: 9.6207e-04  eta: 0:06:34  time: 0.1385  data_time: 0.0003  memory: 4922  loss: 1.1036\n",
      "11/26 01:42:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 43 epochs\n",
      "11/26 01:43:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [43][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0454  time: 0.1045\n",
      "11/26 01:43:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:43:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [44][38/38]  base_lr: 9.6002e-04 lr: 9.6002e-04  eta: 0:06:26  time: 0.1387  data_time: 0.0003  memory: 4922  loss: 1.1018\n",
      "11/26 01:43:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 44 epochs\n",
      "11/26 01:43:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [44][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0696  time: 0.1260\n",
      "11/26 01:43:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:43:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [45][38/38]  base_lr: 9.5792e-04 lr: 9.5792e-04  eta: 0:06:18  time: 0.1389  data_time: 0.0002  memory: 4922  loss: 1.1063\n",
      "11/26 01:43:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 45 epochs\n",
      "11/26 01:43:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [45][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0993  time: 0.1592\n",
      "11/26 01:44:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:44:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [46][38/38]  base_lr: 9.5576e-04 lr: 9.5576e-04  eta: 0:06:10  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1018\n",
      "11/26 01:44:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 46 epochs\n",
      "11/26 01:44:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [46][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0423  time: 0.0966\n",
      "11/26 01:44:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:44:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [47][38/38]  base_lr: 9.5356e-04 lr: 9.5356e-04  eta: 0:06:02  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.0998\n",
      "11/26 01:44:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 47 epochs\n",
      "11/26 01:44:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [47][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0285  time: 0.0882\n",
      "11/26 01:44:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:44:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [48][38/38]  base_lr: 9.5131e-04 lr: 9.5131e-04  eta: 0:05:54  time: 0.1387  data_time: 0.0003  memory: 4922  loss: 1.0993\n",
      "11/26 01:44:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 48 epochs\n",
      "11/26 01:45:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [48][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0293  time: 0.0875\n",
      "11/26 01:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [49][38/38]  base_lr: 9.4900e-04 lr: 9.4900e-04  eta: 0:05:46  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1037\n",
      "11/26 01:45:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 49 epochs\n",
      "11/26 01:45:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [49][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0320  time: 0.0880\n",
      "11/26 01:45:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:45:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [50][38/38]  base_lr: 9.4664e-04 lr: 9.4664e-04  eta: 0:05:38  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.1024\n",
      "11/26 01:45:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 50 epochs\n",
      "11/26 01:45:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [50][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0370  time: 0.0896\n",
      "11/26 01:45:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:45:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [51][38/38]  base_lr: 9.4424e-04 lr: 9.4424e-04  eta: 0:05:31  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.0949\n",
      "11/26 01:45:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 51 epochs\n",
      "11/26 01:46:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [51][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0287  time: 0.0842\n",
      "11/26 01:46:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:46:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [52][38/38]  base_lr: 9.4178e-04 lr: 9.4178e-04  eta: 0:05:23  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.1047\n",
      "11/26 01:46:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 52 epochs\n",
      "11/26 01:46:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [52][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0492  time: 0.1057\n",
      "11/26 01:46:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:46:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:46:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [53][38/38]  base_lr: 9.3928e-04 lr: 9.3928e-04  eta: 0:05:16  time: 0.1387  data_time: 0.0003  memory: 4922  loss: 1.0996\n",
      "11/26 01:46:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 53 epochs\n",
      "11/26 01:46:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [53][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0601  time: 0.1162\n",
      "11/26 01:47:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:47:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [54][38/38]  base_lr: 9.3672e-04 lr: 9.3672e-04  eta: 0:05:09  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1007\n",
      "11/26 01:47:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 54 epochs\n",
      "11/26 01:47:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [54][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0487  time: 0.1096\n",
      "11/26 01:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [55][38/38]  base_lr: 9.3412e-04 lr: 9.3412e-04  eta: 0:05:02  time: 0.1389  data_time: 0.0002  memory: 4922  loss: 1.1038\n",
      "11/26 01:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 55 epochs\n",
      "11/26 01:47:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [55][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0291  time: 0.0881\n",
      "11/26 01:47:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:47:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [56][38/38]  base_lr: 9.3147e-04 lr: 9.3147e-04  eta: 0:04:54  time: 0.1385  data_time: 0.0003  memory: 4922  loss: 1.1101\n",
      "11/26 01:47:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 56 epochs\n",
      "11/26 01:48:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [56][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0489  time: 0.1058\n",
      "11/26 01:48:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:48:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [57][38/38]  base_lr: 9.2877e-04 lr: 9.2877e-04  eta: 0:04:47  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1104\n",
      "11/26 01:48:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 57 epochs\n",
      "11/26 01:48:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [57][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0297  time: 0.0874\n",
      "11/26 01:48:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:48:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [58][38/38]  base_lr: 9.2602e-04 lr: 9.2602e-04  eta: 0:04:40  time: 0.1393  data_time: 0.0003  memory: 4922  loss: 1.0989\n",
      "11/26 01:48:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 58 epochs\n",
      "11/26 01:48:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [58][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0698  time: 0.1472\n",
      "11/26 01:49:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:49:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [59][38/38]  base_lr: 9.2322e-04 lr: 9.2322e-04  eta: 0:04:36  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.0990\n",
      "11/26 01:49:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 59 epochs\n",
      "11/26 01:49:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [59][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0639  time: 0.1270\n",
      "11/26 01:49:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:49:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [60][38/38]  base_lr: 9.2038e-04 lr: 9.2038e-04  eta: 0:04:31  time: 0.1386  data_time: 0.0003  memory: 4922  loss: 1.1054\n",
      "11/26 01:49:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 60 epochs\n",
      "11/26 01:49:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [60][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0880  time: 0.1574\n",
      "11/26 01:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [61][38/38]  base_lr: 9.1749e-04 lr: 9.1749e-04  eta: 0:04:24  time: 0.1385  data_time: 0.0003  memory: 4922  loss: 1.1028\n",
      "11/26 01:49:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 61 epochs\n",
      "11/26 01:50:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [61][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0465  time: 0.1042\n",
      "11/26 01:50:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:50:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [62][38/38]  base_lr: 9.1455e-04 lr: 9.1455e-04  eta: 0:04:16  time: 0.1385  data_time: 0.0002  memory: 4922  loss: 1.1057\n",
      "11/26 01:50:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 62 epochs\n",
      "11/26 01:50:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [62][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0432  time: 0.0956\n",
      "11/26 01:50:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:50:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [63][38/38]  base_lr: 9.1157e-04 lr: 9.1157e-04  eta: 0:04:09  time: 0.1384  data_time: 0.0003  memory: 4922  loss: 1.1024\n",
      "11/26 01:50:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 63 epochs\n",
      "11/26 01:51:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [63][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0427  time: 0.1041\n",
      "11/26 01:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [64][38/38]  base_lr: 9.0854e-04 lr: 9.0854e-04  eta: 0:04:02  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1042\n",
      "11/26 01:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 64 epochs\n",
      "11/26 01:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [64][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0416  time: 0.0993\n",
      "11/26 01:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [65][38/38]  base_lr: 9.0546e-04 lr: 9.0546e-04  eta: 0:03:55  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.1063\n",
      "11/26 01:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 65 epochs\n",
      "11/26 01:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [65][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0410  time: 0.0982\n",
      "11/26 01:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [66][38/38]  base_lr: 9.0234e-04 lr: 9.0234e-04  eta: 0:03:48  time: 0.1390  data_time: 0.0003  memory: 4922  loss: 1.1044\n",
      "11/26 01:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 66 epochs\n",
      "11/26 01:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [66][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0413  time: 0.1005\n",
      "11/26 01:52:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:52:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [67][38/38]  base_lr: 8.9918e-04 lr: 8.9918e-04  eta: 0:03:41  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1004\n",
      "11/26 01:52:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 67 epochs\n",
      "11/26 01:52:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [67][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0411  time: 0.0945\n",
      "11/26 01:52:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:52:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [68][38/38]  base_lr: 8.9597e-04 lr: 8.9597e-04  eta: 0:03:33  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.1012\n",
      "11/26 01:52:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 68 epochs\n",
      "11/26 01:52:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [68][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0300  time: 0.0832\n",
      "11/26 01:53:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:53:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [69][38/38]  base_lr: 8.9271e-04 lr: 8.9271e-04  eta: 0:03:26  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.1008\n",
      "11/26 01:53:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 69 epochs\n",
      "11/26 01:53:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [69][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0196  time: 0.0778\n",
      "11/26 01:53:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:53:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [70][38/38]  base_lr: 8.8941e-04 lr: 8.8941e-04  eta: 0:03:19  time: 0.1387  data_time: 0.0003  memory: 4922  loss: 1.1032\n",
      "11/26 01:53:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 70 epochs\n",
      "11/26 01:53:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [70][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0627  time: 0.1348\n",
      "11/26 01:53:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:53:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [71][38/38]  base_lr: 8.8607e-04 lr: 8.8607e-04  eta: 0:03:12  time: 0.1390  data_time: 0.0003  memory: 4922  loss: 1.1000\n",
      "11/26 01:53:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 71 epochs\n",
      "11/26 01:54:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [71][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0351  time: 0.0911\n",
      "11/26 01:54:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:54:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [72][38/38]  base_lr: 8.8268e-04 lr: 8.8268e-04  eta: 0:03:06  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1046\n",
      "11/26 01:54:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 72 epochs\n",
      "11/26 01:54:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [72][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0308  time: 0.0897\n",
      "11/26 01:54:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:54:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [73][38/38]  base_lr: 8.7925e-04 lr: 8.7925e-04  eta: 0:02:59  time: 0.1389  data_time: 0.0003  memory: 4922  loss: 1.1040\n",
      "11/26 01:54:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 73 epochs\n",
      "11/26 01:54:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [73][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0474  time: 0.1071\n",
      "11/26 01:55:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:55:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [74][38/38]  base_lr: 8.7578e-04 lr: 8.7578e-04  eta: 0:02:52  time: 0.1390  data_time: 0.0003  memory: 4922  loss: 1.0997\n",
      "11/26 01:55:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 74 epochs\n",
      "11/26 01:55:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [74][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0238  time: 0.0795\n",
      "11/26 01:55:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:55:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [75][38/38]  base_lr: 8.7227e-04 lr: 8.7227e-04  eta: 0:02:45  time: 0.1388  data_time: 0.0003  memory: 4922  loss: 1.0988\n",
      "11/26 01:55:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 75 epochs\n",
      "11/26 01:55:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [75][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0785  time: 0.1511\n",
      "11/26 01:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [76][38/38]  base_lr: 8.6871e-04 lr: 8.6871e-04  eta: 0:02:38  time: 0.1388  data_time: 0.0002  memory: 4922  loss: 1.1010\n",
      "11/26 01:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 76 epochs\n",
      "11/26 01:56:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [76][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0418  time: 0.1021\n",
      "11/26 01:56:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:56:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [77][38/38]  base_lr: 8.6512e-04 lr: 8.6512e-04  eta: 0:02:31  time: 0.1377  data_time: 0.0003  memory: 4922  loss: 1.1009\n",
      "11/26 01:56:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 77 epochs\n",
      "11/26 01:56:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [77][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0550  time: 0.1192\n",
      "11/26 01:56:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:56:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [78][38/38]  base_lr: 8.6148e-04 lr: 8.6148e-04  eta: 0:02:25  time: 0.1376  data_time: 0.0002  memory: 4922  loss: 1.0984\n",
      "11/26 01:56:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 78 epochs\n",
      "11/26 01:56:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [78][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0474  time: 0.1067\n",
      "11/26 01:56:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:56:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:56:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [79][38/38]  base_lr: 8.5780e-04 lr: 8.5780e-04  eta: 0:02:18  time: 0.1374  data_time: 0.0003  memory: 4922  loss: 1.1069\n",
      "11/26 01:56:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 79 epochs\n",
      "11/26 01:57:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [79][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0228  time: 0.0811\n",
      "11/26 01:57:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:57:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [80][38/38]  base_lr: 8.5408e-04 lr: 8.5408e-04  eta: 0:02:11  time: 0.1381  data_time: 0.0003  memory: 4922  loss: 1.0941\n",
      "11/26 01:57:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 80 epochs\n",
      "11/26 01:57:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [80][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0746  time: 0.1410\n",
      "11/26 01:57:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:57:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [81][38/38]  base_lr: 8.5033e-04 lr: 8.5033e-04  eta: 0:02:04  time: 0.1374  data_time: 0.0002  memory: 4922  loss: 1.0992\n",
      "11/26 01:57:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 81 epochs\n",
      "11/26 01:58:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [81][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0343  time: 0.0957\n",
      "11/26 01:58:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:58:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [82][38/38]  base_lr: 8.4653e-04 lr: 8.4653e-04  eta: 0:01:58  time: 0.1373  data_time: 0.0003  memory: 4922  loss: 1.0994\n",
      "11/26 01:58:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 82 epochs\n",
      "11/26 01:58:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [82][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0515  time: 0.1141\n",
      "11/26 01:58:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:58:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [83][38/38]  base_lr: 8.4270e-04 lr: 8.4270e-04  eta: 0:01:51  time: 0.1372  data_time: 0.0002  memory: 4922  loss: 1.0985\n",
      "11/26 01:58:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 83 epochs\n",
      "11/26 01:58:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [83][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0393  time: 0.0978\n",
      "11/26 01:58:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:58:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [84][38/38]  base_lr: 8.3882e-04 lr: 8.3882e-04  eta: 0:01:44  time: 0.1373  data_time: 0.0003  memory: 4922  loss: 1.1145\n",
      "11/26 01:58:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 84 epochs\n",
      "11/26 01:59:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [84][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0510  time: 0.1115\n",
      "11/26 01:59:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:59:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [85][38/38]  base_lr: 8.3491e-04 lr: 8.3491e-04  eta: 0:01:38  time: 0.1375  data_time: 0.0003  memory: 4922  loss: 1.1009\n",
      "11/26 01:59:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 85 epochs\n",
      "11/26 01:59:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [85][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0462  time: 0.1048\n",
      "11/26 01:59:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 01:59:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [86][38/38]  base_lr: 8.3096e-04 lr: 8.3096e-04  eta: 0:01:31  time: 0.1375  data_time: 0.0003  memory: 4922  loss: 1.0984\n",
      "11/26 01:59:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 86 epochs\n",
      "11/26 02:00:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [86][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0303  time: 0.0881\n",
      "11/26 02:00:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:00:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [87][38/38]  base_lr: 8.2698e-04 lr: 8.2698e-04  eta: 0:01:24  time: 0.1373  data_time: 0.0003  memory: 4922  loss: 1.1035\n",
      "11/26 02:00:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 87 epochs\n",
      "11/26 02:00:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [87][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0285  time: 0.0885\n",
      "11/26 02:00:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:00:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [88][38/38]  base_lr: 8.2296e-04 lr: 8.2296e-04  eta: 0:01:18  time: 0.1375  data_time: 0.0003  memory: 4922  loss: 1.1034\n",
      "11/26 02:00:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 88 epochs\n",
      "11/26 02:00:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [88][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0304  time: 0.0880\n",
      "11/26 02:00:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:00:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [89][38/38]  base_lr: 8.1890e-04 lr: 8.1890e-04  eta: 0:01:11  time: 0.1371  data_time: 0.0002  memory: 4922  loss: 1.1047\n",
      "11/26 02:00:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 89 epochs\n",
      "11/26 02:01:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [89][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0263  time: 0.0851\n",
      "11/26 02:01:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:01:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [90][38/38]  base_lr: 8.1480e-04 lr: 8.1480e-04  eta: 0:01:05  time: 0.1375  data_time: 0.0003  memory: 4922  loss: 1.1007\n",
      "11/26 02:01:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 90 epochs\n",
      "11/26 02:01:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [90][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0285  time: 0.0849\n",
      "11/26 02:01:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:01:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [91][38/38]  base_lr: 8.1067e-04 lr: 8.1067e-04  eta: 0:00:58  time: 0.1373  data_time: 0.0003  memory: 4922  loss: 1.1028\n",
      "11/26 02:01:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 91 epochs\n",
      "11/26 02:01:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [91][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0546  time: 0.1115\n",
      "11/26 02:01:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:01:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [92][38/38]  base_lr: 8.0651e-04 lr: 8.0651e-04  eta: 0:00:51  time: 0.1377  data_time: 0.0003  memory: 4922  loss: 1.1031\n",
      "11/26 02:01:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 92 epochs\n",
      "11/26 02:02:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [92][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0294  time: 0.0881\n",
      "11/26 02:02:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:02:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [93][38/38]  base_lr: 8.0231e-04 lr: 8.0231e-04  eta: 0:00:45  time: 0.1373  data_time: 0.0002  memory: 4922  loss: 1.1069\n",
      "11/26 02:02:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 93 epochs\n",
      "11/26 02:02:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [93][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0404  time: 0.0974\n",
      "11/26 02:02:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:02:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [94][38/38]  base_lr: 7.9808e-04 lr: 7.9808e-04  eta: 0:00:38  time: 0.1375  data_time: 0.0003  memory: 4922  loss: 1.1018\n",
      "11/26 02:02:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 94 epochs\n",
      "11/26 02:02:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [94][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0336  time: 0.0859\n",
      "11/26 02:02:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:02:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [95][38/38]  base_lr: 7.9382e-04 lr: 7.9382e-04  eta: 0:00:32  time: 0.1374  data_time: 0.0003  memory: 4922  loss: 1.1044\n",
      "11/26 02:02:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 95 epochs\n",
      "11/26 02:03:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [95][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0263  time: 0.0818\n",
      "11/26 02:03:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:03:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [96][38/38]  base_lr: 7.8952e-04 lr: 7.8952e-04  eta: 0:00:25  time: 0.1374  data_time: 0.0003  memory: 4922  loss: 1.1025\n",
      "11/26 02:03:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 96 epochs\n",
      "11/26 02:03:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [96][9/9]    accuracy/top1: 31.8182  accuracy/top3: 100.0000  data_time: 0.0234  time: 0.0829\n",
      "11/26 02:03:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:03:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [97][38/38]  base_lr: 7.8519e-04 lr: 7.8519e-04  eta: 0:00:19  time: 0.1373  data_time: 0.0003  memory: 4922  loss: 1.1006\n",
      "11/26 02:03:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 97 epochs\n",
      "11/26 02:03:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [97][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0248  time: 0.0792\n",
      "11/26 02:04:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:04:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [98][38/38]  base_lr: 7.8083e-04 lr: 7.8083e-04  eta: 0:00:12  time: 0.1374  data_time: 0.0003  memory: 4922  loss: 1.0960\n",
      "11/26 02:04:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 98 epochs\n",
      "11/26 02:04:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [98][9/9]    accuracy/top1: 34.0909  accuracy/top3: 100.0000  data_time: 0.0332  time: 0.0853\n",
      "11/26 02:04:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg19_alzheimer_axial_view_20231126_012119\n",
      "11/26 02:04:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [99][38/38]  base_lr: 7.7644e-04 lr: 7.7644e-04  eta: 0:00:06  time: 0.1376  data_time: 0.0003  memory: 4922  loss: 1.1001\n",
      "11/26 02:04:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 99 epochs\n",
      "Traceback (most recent call last):\n",
      "  File \"./tools/train.py\", line 162, in <module>\n",
      "    main()\n",
      "  File \"./tools/train.py\", line 158, in main\n",
      "    runner.train()\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/runner.py\", line 1777, in train\n",
      "    model = self.train_loop.run()  # type: ignore\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/loops.py\", line 96, in run\n",
      "    self.run_epoch()\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/loops.py\", line 114, in run_epoch\n",
      "    self.runner.call_hook('after_train_epoch')\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/runner.py\", line 1839, in call_hook\n",
      "    getattr(hook, fn_name)(self, **kwargs)\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/hooks/checkpoint_hook.py\", line 345, in after_train_epoch\n",
      "    self._save_checkpoint(runner)\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/hooks/checkpoint_hook.py\", line 476, in _save_checkpoint\n",
      "    self._save_checkpoint_with_step(runner, step, meta=meta)\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/hooks/checkpoint_hook.py\", line 443, in _save_checkpoint_with_step\n",
      "    runner.save_checkpoint(\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/dist/utils.py\", line 412, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/runner.py\", line 2271, in save_checkpoint\n",
      "    save_checkpoint(\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/runner/checkpoint.py\", line 793, in save_checkpoint\n",
      "    file_backend.put(f.getvalue(), filename)\n",
      "  File \"/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmengine/fileio/backends/local_backend.py\", line 78, in put\n",
      "    f.write(obj)\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    }
   ],
   "source": [
    "!python ./tools/train.py \\\n",
    "    configs/vgg/vgg19_alzheimer_axial_view.py \\\n",
    "    --work-dir work_dirs/alzheimer/axial/vgg16 \\\n",
    "    # --auto-scale-lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59733eda-e41b-43ab-84ec-55551d094d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/mmpretrain\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "080a64c9-ce85-4ebe-b3f0-1c071962b988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14 22:59:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 248044611\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda-11.4\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.100\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.10.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.9.0\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 248044611\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "11/14 22:59:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=256)\n",
      "data_preprocessor = dict(\n",
      "    mean=[\n",
      "        124.508,\n",
      "        116.05,\n",
      "        106.438,\n",
      "    ],\n",
      "    std=[\n",
      "        58.577,\n",
      "        57.31,\n",
      "        57.437,\n",
      "    ],\n",
      "    to_rgb=False)\n",
      "dataset_type = 'CustomDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=100, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(enable=False, type='VisualizationHook'))\n",
      "default_scope = 'mmpretrain'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "launcher = 'none'\n",
      "load_from = 'work_dirs/alzheimer/axial/vgg16/epoch_3.pth'\n",
      "log_level = 'INFO'\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=16,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_batch256_imagenet_20210208-db26f1a5.pth',\n",
      "            prefix='backbone',\n",
      "            type='Pretrained'),\n",
      "        num_classes=3,\n",
      "        type='VGG'),\n",
      "    head=dict(\n",
      "        cal_acc=True,\n",
      "        loss=dict(loss_weight=1.0, type='CrossEntropyLoss'),\n",
      "        topk=(1, ),\n",
      "        type='ClsHead'),\n",
      "    neck=None,\n",
      "    type='ImageClassifier')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ),\n",
      "        eps=1e-08,\n",
      "        lr=0.001,\n",
      "        type='AdamW',\n",
      "        weight_decay=0.05),\n",
      "    paramwise_cfg=dict(\n",
      "        bias_decay_mult=0.0,\n",
      "        custom_keys=dict({'.cls_token': dict(decay_mult=0.0)}),\n",
      "        norm_decay_mult=0.0))\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        convert_to_iter_based=True,\n",
      "        end=5,\n",
      "        start_factor=0.001,\n",
      "        type='LinearLR'),\n",
      "    dict(\n",
      "        T_max=295,\n",
      "        begin=5,\n",
      "        by_epoch=True,\n",
      "        end=300,\n",
      "        eta_min=1e-05,\n",
      "        type='CosineAnnealingLR'),\n",
      "]\n",
      "randomness = dict(deterministic=False, seed=None)\n",
      "resume = False\n",
      "test_cfg = dict()\n",
      "test_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        ann_file='data/test_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/test',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True)\n",
      "test_evaluator = dict(topk=(1, ), type='Accuracy')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=244, type='ResizeEdge'),\n",
      "    dict(crop_size=244, type='CenterCrop'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "train_cfg = dict(by_epoch=True, max_epochs=3, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        ann_file='data/train_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/train',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=224, type='RandomResizedCrop'),\n",
      "            dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=224, type='RandomResizedCrop'),\n",
      "    dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "val_cfg = dict()\n",
      "val_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        ann_file='data/val_ann.txt',\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/validation',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset',\n",
      "        with_label=True),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True)\n",
      "val_evaluator = dict(topk=(1, ), type='Accuracy')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='UniversalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './work_dirs/vgg16_alzheimer_axial_view'\n",
      "\n",
      "11/14 22:59:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "11/14 22:59:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_3.pth\n",
      "11/14 22:59:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from work_dirs/alzheimer/axial/vgg16/epoch_3.pth\n",
      "11/14 22:59:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [1/1]    accuracy/top1: 33.3333  data_time: 0.2301  time: 1.0877\n"
     ]
    }
   ],
   "source": [
    "!python tools/test.py configs/vgg/vgg16_alzheimer_axial_view.py work_dirs/alzheimer/axial/vgg16/epoch_3.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b1fa47a-017d-4d6a-bc22-cef52faaa4d0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/14 23:50:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 96908555\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda-11.4\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.100\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.10.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.9.0\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 96908555\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "11/14 23:50:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=256)\n",
      "data_preprocessor = dict(\n",
      "    mean=[\n",
      "        124.508,\n",
      "        116.05,\n",
      "        106.438,\n",
      "    ],\n",
      "    std=[\n",
      "        58.577,\n",
      "        57.31,\n",
      "        57.437,\n",
      "    ],\n",
      "    to_rgb=False)\n",
      "dataset_type = 'CustomDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=100, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(enable=False, type='VisualizationHook'))\n",
      "default_scope = 'mmpretrain'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = 'work_dirs/alzheimer/axial/vgg16/epoch_100.pth'\n",
      "log_level = 'INFO'\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=16,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_batch256_imagenet_20210208-db26f1a5.pth',\n",
      "            prefix='backbone',\n",
      "            type='Pretrained'),\n",
      "        num_classes=3,\n",
      "        type='VGG'),\n",
      "    head=dict(\n",
      "        cal_acc=True,\n",
      "        loss=dict(loss_weight=1.0, type='CrossEntropyLoss'),\n",
      "        topk=(1, ),\n",
      "        type='ClsHead'),\n",
      "    neck=None,\n",
      "    type='ImageClassifier')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=0.1, momentum=0.9, type='SGD', weight_decay=0.0001))\n",
      "param_scheduler = dict(\n",
      "    by_epoch=True, gamma=0.1, milestones=[\n",
      "        30,\n",
      "        60,\n",
      "        90,\n",
      "    ], type='MultiStepLR')\n",
      "randomness = dict(deterministic=False, seed=None)\n",
      "resume = False\n",
      "test_cfg = dict()\n",
      "test_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/test',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True)\n",
      "test_evaluator = dict(type='ConfusionMatrix')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=244, type='ResizeEdge'),\n",
      "    dict(crop_size=244, type='CenterCrop'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=16,\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/train',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=224, type='RandomResizedCrop'),\n",
      "            dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=224, type='RandomResizedCrop'),\n",
      "    dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "val_cfg = dict()\n",
      "val_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/YoriDataset_vgg/validation',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True)\n",
      "val_evaluator = dict(topk=(1, ), type='Accuracy')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='UniversalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = '/tmp/tmpave2deov'\n",
      "\n",
      "11/14 23:50:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "11/14 23:50:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "11/14 23:50:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Load checkpoint from work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "11/14 23:50:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(test) [1/1]    confusion_matrix/result: \n",
      "tensor([[10,  0,  0],\n",
      "        [10,  0,  0],\n",
      "        [10,  0,  0]])\n",
      "  data_time: 0.2769  time: 1.1460\n",
      "Figure(1000x1000)\n"
     ]
    }
   ],
   "source": [
    "!python tools/analysis_tools/confusion_matrix.py \\\n",
    "    configs/vgg/vgg16_alzheimer_axial_view.py \\\n",
    "    work_dirs/alzheimer/axial/vgg16/epoch_100.pth \\\n",
    "    --show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7b322-bae6-4a9c-b6f0-81f51a2edd09",
   "metadata": {},
   "source": [
    "# Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6731a1c4-8f16-456f-b9a6-e2198e4d87d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/vgg_heat/validation/NC/heat_53x-133.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "1\n",
      "data/vgg_heat/validation/NC/heat_11x-119.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "2\n",
      "data/vgg_heat/validation/NC/heat_60x-131.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "3\n",
      "data/vgg_heat/validation/NC/heat_30x-133.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "4\n",
      "data/vgg_heat/validation/NC/heat_3x-141.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "5\n",
      "data/vgg_heat/validation/NC/heat_9x-122.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "6\n",
      "data/vgg_heat/validation/NC/heat_77x-129.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "7\n",
      "data/vgg_heat/validation/NC/heat_16x-119.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "8\n",
      "data/vgg_heat/validation/NC/heat_29x-159.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "9\n",
      "data/vgg_heat/validation/NC/heat_42x-138.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "10\n",
      "data/vgg_heat/validation/NC/heat_1x-128.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "11\n",
      "data/vgg_heat/validation/NC/heat_37x-158.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "12\n",
      "data/vgg_heat/validation/NC/heat_28x-158.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "13\n",
      "data/vgg_heat/validation/NC/heat_70x-131.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "14\n",
      "data/vgg_heat/validation/NC/heat_81x-121.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "15\n",
      "data/vgg_heat/validation/NC/heat_18x-128.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "16\n",
      "data/vgg_heat/validation/NC/heat_63x-120.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "17\n",
      "data/vgg_heat/validation/NC/heat_15x-133.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "18\n",
      "data/vgg_heat/validation/NC/heat_13x-114.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "19\n",
      "data/vgg_heat/validation/NC/heat_1x-120.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "20\n",
      "data/vgg_heat/validation/NC/heat_2x-139.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "21\n",
      "data/vgg_heat/validation/NC/heat_41x-130.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "22\n",
      "data/vgg_heat/validation/NC/heat_43x-126.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "23\n",
      "data/vgg_heat/validation/NC/heat_8x-120.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "24\n",
      "data/vgg_heat/validation/NC/heat_50x-124.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "25\n",
      "data/vgg_heat/validation/NC/heat_49x-135.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "26\n",
      "data/vgg_heat/validation/NC/heat_78x-129.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "27\n",
      "data/vgg_heat/validation/NC/heat_20x-121.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "28\n",
      "data/vgg_heat/validation/NC/heat_6x-119.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "29\n",
      "data/vgg_heat/validation/NC/heat_32x-151.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "30\n",
      "data/vgg_heat/validation/NC/heat_26x-108.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "31\n",
      "data/vgg_heat/validation/NC/heat_38x-155.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "32\n",
      "data/vgg_heat/validation/NC/heat_20x-125.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "33\n",
      "data/vgg_heat/validation/NC/heat_39x-125.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "34\n",
      "data/vgg_heat/validation/NC/heat_10x-138.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "35\n",
      "data/vgg_heat/validation/NC/heat_35x-151.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "36\n",
      "data/vgg_heat/validation/NC/heat_29x-151.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "37\n",
      "data/vgg_heat/validation/NC/heat_59x-124.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "38\n",
      "data/vgg_heat/validation/NC/heat_76x-126.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "39\n",
      "data/vgg_heat/validation/NC/heat_46x-144.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "40\n",
      "data/vgg_heat/validation/NC/heat_33x-127.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "41\n",
      "data/vgg_heat/validation/NC/heat_9x-128.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "42\n",
      "data/vgg_heat/validation/NC/heat_64x-132.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "43\n",
      "data/vgg_heat/validation/NC/heat_22x-89.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "44\n",
      "data/vgg_heat/validation/NC/heat_31x-154.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "45\n",
      "data/vgg_heat/validation/NC/heat_73x-123.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "46\n",
      "data/vgg_heat/validation/NC/heat_43x-130.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "47\n",
      "data/vgg_heat/validation/NC/heat_75x-137.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "48\n",
      "data/vgg_heat/validation/NC/heat_71x-133.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "49\n",
      "data/vgg_heat/validation/NC/heat_34x-151.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "50\n",
      "data/vgg_heat/validation/NC/heat_34x-147.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "51\n",
      "data/vgg_heat/validation/NC/heat_10x-142.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "52\n",
      "data/vgg_heat/validation/NC/heat_2x-143.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "53\n",
      "data/vgg_heat/validation/NC/heat_54x-148.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "54\n",
      "data/vgg_heat/validation/NC/heat_6x-123.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "55\n",
      "data/vgg_heat/validation/NC/heat_19x-127.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "56\n",
      "data/vgg_heat/validation/NC/heat_15x-137.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "57\n",
      "data/vgg_heat/validation/NC/heat_50x-132.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "58\n",
      "data/vgg_heat/validation/NC/heat_29x-155.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "59\n",
      "data/vgg_heat/validation/NC/heat_79x-118.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "60\n",
      "data/vgg_heat/validation/NC/heat_16x-115.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "61\n",
      "data/vgg_heat/validation/NC/heat_57x-149.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "62\n",
      "data/vgg_heat/validation/NC/heat_72x-137.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "63\n",
      "data/vgg_heat/validation/NC/heat_35x-145.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "64\n",
      "data/vgg_heat/validation/NC/heat_23x-91.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "65\n",
      "data/vgg_heat/validation/NC/heat_44x-122.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "66\n",
      "data/vgg_heat/validation/NC/heat_11x-111.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "67\n",
      "data/vgg_heat/validation/NC/heat_21x-83.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "68\n",
      "data/vgg_heat/validation/NC/heat_61x-129.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "69\n",
      "data/vgg_heat/validation/NC/heat_15x-141.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "70\n",
      "data/vgg_heat/validation/NC/heat_38x-159.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "71\n",
      "data/vgg_heat/validation/NC/heat_25x-102.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "72\n",
      "data/vgg_heat/validation/NC/heat_68x-118.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "73\n",
      "data/vgg_heat/validation/NC/heat_1x-124.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "74\n",
      "data/vgg_heat/validation/NC/heat_40x-138.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "75\n",
      "data/vgg_heat/validation/NC/heat_11x-115.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "76\n",
      "data/vgg_heat/validation/NC/heat_5x-137.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "77\n",
      "data/vgg_heat/validation/NC/heat_25x-98.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "78\n",
      "data/vgg_heat/validation/NC/heat_39x-129.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "79\n",
      "data/vgg_heat/validation/NC/heat_33x-123.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "80\n",
      "data/vgg_heat/validation/NC/heat_7x-140.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "81\n",
      "data/vgg_heat/validation/NC/heat_24x-103.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "82\n",
      "data/vgg_heat/validation/NC/heat_67x-128.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "83\n",
      "data/vgg_heat/validation/NC/heat_76x-122.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "84\n",
      "data/vgg_heat/validation/NC/heat_28x-154.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "85\n",
      "data/vgg_heat/validation/NC/heat_19x-131.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "86\n",
      "data/vgg_heat/validation/NC/heat_24x-107.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "87\n",
      "data/vgg_heat/validation/NC/heat_4x-125.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "88\n",
      "data/vgg_heat/validation/NC/heat_48x-135.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "89\n",
      "data/vgg_heat/validation/NC/heat_7x-136.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "13\n",
      "Automatically choose the last norm layer \"backbone.features.16.bn\" as the target layer.\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "train_AD = 'data/YoriDataset_vgg/train/AD'\n",
    "train_MCI = 'data/YoriDataset_vgg/train/MCI'\n",
    "train_NC = 'data/YoriDataset_vgg/train/NC'\n",
    "\n",
    "val_AD = 'data/YoriDataset_vgg/validation/AD'\n",
    "val_MCI = 'data/YoriDataset_vgg/validation/MCI'\n",
    "val_NC = 'data/YoriDataset_vgg/validation/NC'\n",
    "\n",
    "test_AD = 'data/YoriDataset_vgg/test/AD'\n",
    "test_MCI = 'data/YoriDataset_vgg/test/MCI'\n",
    "test_NC = 'data/YoriDataset_vgg/test/NC'\n",
    "\n",
    "\n",
    "trainheat_AD = 'data/vgg_heat/train/AD'\n",
    "trainheat_MCI = 'data/vgg_heat/train/MCI'\n",
    "trainheat_NC = 'data/vgg_heat/train/NC'\n",
    "\n",
    "valheat_AD = 'data/vgg_heat/validation/AD'\n",
    "valheat_MCI = 'data/vgg_heat/validation/MCI'\n",
    "valheat_NC = 'data/vgg_heat/validation/NC'\n",
    "\n",
    "testheat_AD = 'data/vgg_heat/test/AD'\n",
    "testheat_MCI = 'data/vgg_heat/test/MCI'\n",
    "testheat_NC = 'data/vgg_heat/test/NC'\n",
    "\n",
    "count = 0\n",
    "for i in os.listdir(val_NC):\n",
    "    count+=1\n",
    "    temp = os.path.join(val_NC,i)\n",
    "    out = valheat_NC + '/heat_'+i\n",
    "    print(out)  \n",
    "    subprocess.run([\"python\", \"tools/visualization/vis_cam.py\", \n",
    "                temp,\n",
    "                 \"configs/vgg/vgg16_alzheimer_axial_view.py\",\n",
    "                 \"work_dirs/alzheimer/axial/vgg16/epoch_100.pth\",\n",
    "                 # \"--target-layers\", \"backbone.features.16\",\n",
    "                 \"--method\", \"GradCAM\",\n",
    "                 \"--save-path\", out,\n",
    "                 \"--eigen-smooth\", \"--aug-smooth\"\n",
    "                 ], text=True)\n",
    "    print(count)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c79f45cd-fe6c-4315-9cce-a025efff35d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: work_dirs/alzheimer/axial/vgg16/epoch_100.pth\n",
      "backbone VGG(\n",
      "  (features): Sequential(\n",
      "    (0): ConvModule(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): ConvModule(\n",
      "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ConvModule(\n",
      "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): ConvModule(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): ConvModule(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (7): ConvModule(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (8): ConvModule(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): ConvModule(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (11): ConvModule(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (12): ConvModule(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): ConvModule(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (15): ConvModule(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (16): ConvModule(\n",
      "      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (activate): ReLU(inplace=True)\n",
      "    )\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "init_cfg={'type': 'Pretrained', 'checkpoint': 'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_batch256_imagenet_20210208-db26f1a5.pth', 'prefix': 'backbone'}\n",
      "backbone.features Sequential(\n",
      "  (0): ConvModule(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): ConvModule(\n",
      "    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (3): ConvModule(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): ConvModule(\n",
      "    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (6): ConvModule(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (7): ConvModule(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (8): ConvModule(\n",
      "    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): ConvModule(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (11): ConvModule(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (12): ConvModule(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (14): ConvModule(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (15): ConvModule(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (16): ConvModule(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (activate): ReLU(inplace=True)\n",
      "  )\n",
      "  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "backbone.features.0 ConvModule(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.0.conv Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.0.activate ReLU(inplace=True)\n",
      "backbone.features.1 ConvModule(\n",
      "  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.1.activate ReLU(inplace=True)\n",
      "backbone.features.2 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "backbone.features.3 ConvModule(\n",
      "  (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.3.conv Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.3.activate ReLU(inplace=True)\n",
      "backbone.features.4 ConvModule(\n",
      "  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.4.conv Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.4.activate ReLU(inplace=True)\n",
      "backbone.features.5 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "backbone.features.6 ConvModule(\n",
      "  (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.6.conv Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.6.activate ReLU(inplace=True)\n",
      "backbone.features.7 ConvModule(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.7.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.7.activate ReLU(inplace=True)\n",
      "backbone.features.8 ConvModule(\n",
      "  (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.8.conv Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.8.activate ReLU(inplace=True)\n",
      "backbone.features.9 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "backbone.features.10 ConvModule(\n",
      "  (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.10.conv Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.10.activate ReLU(inplace=True)\n",
      "backbone.features.11 ConvModule(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.11.conv Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.11.activate ReLU(inplace=True)\n",
      "backbone.features.12 ConvModule(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.12.conv Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.12.activate ReLU(inplace=True)\n",
      "backbone.features.13 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "backbone.features.14 ConvModule(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.14.conv Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.14.activate ReLU(inplace=True)\n",
      "backbone.features.15 ConvModule(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.15.conv Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.15.activate ReLU(inplace=True)\n",
      "backbone.features.16 ConvModule(\n",
      "  (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (activate): ReLU(inplace=True)\n",
      ")\n",
      "backbone.features.16.conv Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "backbone.features.16.activate ReLU(inplace=True)\n",
      "backbone.features.17 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "backbone.classifier Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=3, bias=True)\n",
      ")\n",
      "backbone.classifier.0 Linear(in_features=25088, out_features=4096, bias=True)\n",
      "backbone.classifier.1 ReLU(inplace=True)\n",
      "backbone.classifier.2 Dropout(p=0.5, inplace=False)\n",
      "backbone.classifier.3 Linear(in_features=4096, out_features=4096, bias=True)\n",
      "backbone.classifier.4 ReLU(inplace=True)\n",
      "backbone.classifier.5 Dropout(p=0.5, inplace=False)\n",
      "backbone.classifier.6 Linear(in_features=4096, out_features=3, bias=True)\n",
      "0\n",
      "Traceback (most recent call last):\n",
      "  File \"tools/visualization/vis_cam.py\", line 277, in <module>\n",
      "    main()\n",
      "  File \"tools/visualization/vis_cam.py\", line 247, in main\n",
      "    target_layers = get_default_target_layers(model, args)\n",
      "  File \"tools/visualization/vis_cam.py\", line 215, in get_default_target_layers\n",
      "    name, layer = norm_layers[-1]\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "!python tools/visualization/vis_cam.py \\\n",
    "    data/YoriDataset_vgg/train/MCI/100x-103.png \\\n",
    "    configs/vgg/vgg16_alzheimer_axial_view.py \\\n",
    "    work_dirs/alzheimer/axial/vgg16/epoch_100.pth \\\n",
    "    # --target-layers backbone.features.17 \\\n",
    "    --method GradCAM\n",
    "    # GradCAM++, XGradCAM, EigenCAM, EigenGradCAM, LayerCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a621014a-910a-4e88-be16-670f99118714",
   "metadata": {},
   "source": [
    "## Train Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "41879dc4-5030-4751-8f34-34c56dd1fc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/vgg/vgg16_alzheimer_axial_view_heat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/vgg/vgg16_alzheimer_axial_view_heat.py\n",
    "_base_ = [\n",
    "    '../_base_/models/vgg16bn.py',\n",
    "    '../_base_/datasets/imagenet_bs32_pil_resize.py',\n",
    "    '../_base_/schedules/imagenet_bs1024_adamw_conformer.py',\n",
    "    # '../_base_/schedules/imagenet_bs256.py',\n",
    "    '../_base_/default_runtime.py'\n",
    "]\n",
    "\n",
    "pretrained = 'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29.pth'\n",
    "\n",
    "model = dict(\n",
    "    type='ImageClassifier', \n",
    "    \n",
    "    backbone=dict(\n",
    "        type='VGG',\n",
    "        depth=16,\n",
    "        frozen_stages=-1,\n",
    "        num_classes=3,\n",
    "        norm_eval=True,\n",
    "        init_cfg = dict(\n",
    "            type='Pretrained', \n",
    "            checkpoint=pretrained,\n",
    "            prefix='backbone',\n",
    "        )\n",
    "    ),\n",
    "    neck=None,\n",
    "    head=dict(\n",
    "        topk = (1, ),\n",
    "        # cal_acc=True,\n",
    "    ))\n",
    "\n",
    "dataset_type = 'CustomDataset'\n",
    "data_preprocessor = dict(\n",
    "     mean=[124.508, 116.050, 106.438],\n",
    "     std=[58.577, 57.310, 57.437],\n",
    "     to_rgb=False)\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),     # read image\n",
    "    # dict(type='ResizeEdge', scale=244),  \n",
    "    \n",
    "    dict(type='RandomResizedCrop', scale=224),     # Random scaling and cropping\n",
    "    dict(type='RandomFlip', prob=0.5, direction='horizontal'),   # random horizontal flip\n",
    "    dict(type='Normalize', **data_preprocessor),\n",
    "    dict(type='PackInputs'),         # prepare images and labels\n",
    "]\n",
    "\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),     # read image\n",
    "    dict(type='ResizeEdge', scale=244),  \n",
    "    dict(type='CenterCrop', crop_size=244),     # center crop\n",
    "    dict(type='Normalize', **data_preprocessor),\n",
    "    dict(type='PackInputs'),                 # prepare images and labels\n",
    "]\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=16,\n",
    "    num_workers=1,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='',\n",
    "        data_prefix='data/vgg_heat/train',\n",
    "        classes='data/classes.txt',\n",
    "        # ann_file='data/train_ann.txt',\n",
    "        pipeline=train_pipeline\n",
    "    ),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='',\n",
    "        data_prefix='data/vgg_heat/validation',\n",
    "        classes='data/classes.txt',\n",
    "        # ann_file='data/val_ann.txt',\n",
    "        pipeline=test_pipeline\n",
    "    ),\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "test_dataloader = dict(\n",
    "    batch_size=32,\n",
    "    num_workers=1,\n",
    "    dataset=dict(\n",
    "        type='CustomDataset',\n",
    "        data_root='',\n",
    "        data_prefix='data/vgg_heat/test',\n",
    "        classes='data/classes.txt',\n",
    "        # ann_file='data/test_ann.txt',\n",
    "        pipeline=test_pipeline\n",
    "    ),    \n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "val_evaluator = dict(type='Accuracy', topk=(1, ))\n",
    "test_evaluator = val_evaluator \n",
    "\n",
    "# optimizer\n",
    "# optim_wrapper = dict(\n",
    "#     optimizer=dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001))\n",
    "\n",
    "# # learning policy\n",
    "# param_scheduler = dict(\n",
    "#     type='MultiStepLR', by_epoch=True, milestones=[30, 60, 90], gamma=0.1)\n",
    "\n",
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "        type='AdamW',\n",
    "        # for batch in each gpu is 128, 8 gpu\n",
    "        # lr = 5e-4 * 128 * 8 / 512 = 0.001\n",
    "        lr=5e-4 * 128 * 8 / 512,\n",
    "        weight_decay=0.05,\n",
    "        eps=1e-8,\n",
    "        betas=(0.9, 0.999)),\n",
    "    paramwise_cfg=dict(\n",
    "        norm_decay_mult=0.0,\n",
    "        bias_decay_mult=0.0,\n",
    "        custom_keys={\n",
    "            '.cls_token': dict(decay_mult=0.0),\n",
    "        }),\n",
    ")\n",
    "\n",
    "# learning policy\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='LinearLR',\n",
    "        start_factor=1e-3,\n",
    "        by_epoch=True,\n",
    "        begin=0,\n",
    "        end=5,\n",
    "        convert_to_iter_based=True),\n",
    "    dict(\n",
    "        type='CosineAnnealingLR',\n",
    "        T_max=295,\n",
    "        eta_min=1e-5,\n",
    "        by_epoch=True,\n",
    "        begin=5,\n",
    "        end=300)\n",
    "]\n",
    "\n",
    "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
    "val_cfg = dict()\n",
    "test_cfg = dict()\n",
    "\n",
    "auto_scale_lr = dict(base_batch_size=128)\n",
    "\n",
    "default_scope = 'mmpretrain'\n",
    "\n",
    "# configure default hooks\n",
    "default_hooks = dict(\n",
    "    # record the time of every iteration.\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "\n",
    "    # print log every 100 iterations.\n",
    "    logger=dict(type='LoggerHook', interval=100),\n",
    "\n",
    "    # enable the parameter scheduler.\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "\n",
    "    # save checkpoint per epoch.\n",
    "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
    "\n",
    "    # set sampler seed in a distributed environment.\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "\n",
    "    # validation results visualization, set True to enable it.\n",
    "    visualization=dict(type='VisualizationHook', enable=False),\n",
    ")\n",
    "\n",
    "env_cfg = dict(\n",
    "    # whether to enable cudnn benchmark\n",
    "    cudnn_benchmark=False,\n",
    "\n",
    "    # set multi-process parameters\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "\n",
    "    # set distributed parameters\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "# set visualizer\n",
    "vis_backends = [dict(type='LocalVisBackend')]  # use local HDD backend\n",
    "visualizer = dict(type='UniversalVisualizer', vis_backends=vis_backends, name='visualizer')\n",
    "\n",
    "# set log level\n",
    "log_level = 'INFO'\n",
    "\n",
    "# load from which checkpoint\n",
    "load_from = None\n",
    "\n",
    "# whether to resume training from the loaded checkpoint\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ab3788dd-c70c-4221-af8e-5d57d62f1fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs/vgg/vgg16_alzheimer_axial_view_heat.py\n",
      "11/16 17:20:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.8.18 (default, Sep 11 2023, 13:40:15) [GCC 11.2.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 41050527\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda-11.4\n",
      "    NVCC: Cuda compilation tools, release 11.4, V11.4.100\n",
      "    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "    PyTorch: 1.10.1\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2023.1-Product Build 20230303 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2\n",
      "    OpenCV: 4.8.1\n",
      "    MMEngine: 0.9.0\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 41050527\n",
      "    deterministic: False\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "11/16 17:20:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=128, enable=True)\n",
      "data_preprocessor = dict(\n",
      "    mean=[\n",
      "        124.508,\n",
      "        116.05,\n",
      "        106.438,\n",
      "    ],\n",
      "    num_classes=1000,\n",
      "    std=[\n",
      "        58.577,\n",
      "        57.31,\n",
      "        57.437,\n",
      "    ],\n",
      "    to_rgb=False)\n",
      "dataset_type = 'CustomDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(interval=1, type='CheckpointHook'),\n",
      "    logger=dict(interval=100, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(enable=False, type='VisualizationHook'))\n",
      "default_scope = 'mmpretrain'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "launcher = 'none'\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        depth=16,\n",
      "        frozen_stages=-1,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29.pth',\n",
      "            prefix='backbone',\n",
      "            type='Pretrained'),\n",
      "        norm_cfg=dict(type='BN'),\n",
      "        norm_eval=True,\n",
      "        num_classes=3,\n",
      "        type='VGG'),\n",
      "    head=dict(\n",
      "        loss=dict(loss_weight=1.0, type='CrossEntropyLoss'),\n",
      "        topk=(1, ),\n",
      "        type='ClsHead'),\n",
      "    neck=None,\n",
      "    type='ImageClassifier')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(\n",
      "        betas=(\n",
      "            0.9,\n",
      "            0.999,\n",
      "        ),\n",
      "        eps=1e-08,\n",
      "        lr=0.001,\n",
      "        type='AdamW',\n",
      "        weight_decay=0.05),\n",
      "    paramwise_cfg=dict(\n",
      "        bias_decay_mult=0.0,\n",
      "        custom_keys=dict({'.cls_token': dict(decay_mult=0.0)}),\n",
      "        norm_decay_mult=0.0))\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        convert_to_iter_based=True,\n",
      "        end=5,\n",
      "        start_factor=0.001,\n",
      "        type='LinearLR'),\n",
      "    dict(\n",
      "        T_max=295,\n",
      "        begin=5,\n",
      "        by_epoch=True,\n",
      "        end=300,\n",
      "        eta_min=1e-05,\n",
      "        type='CosineAnnealingLR'),\n",
      "]\n",
      "pretrained = 'https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29.pth'\n",
      "randomness = dict(deterministic=False, seed=None)\n",
      "resume = False\n",
      "test_cfg = dict()\n",
      "test_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/vgg_heat/test',\n",
      "        data_root='',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(\n",
      "                mean=[\n",
      "                    124.508,\n",
      "                    116.05,\n",
      "                    106.438,\n",
      "                ],\n",
      "                std=[\n",
      "                    58.577,\n",
      "                    57.31,\n",
      "                    57.437,\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                type='Normalize'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "            ], type='ImageToTensor'),\n",
      "            dict(keys=[\n",
      "                'gt_label',\n",
      "            ], type='ToTensor'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "                'gt_label',\n",
      "            ], type='Collect'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='val',\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(topk=(1, ), type='Accuracy')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=244, type='ResizeEdge'),\n",
      "    dict(crop_size=244, type='CenterCrop'),\n",
      "    dict(\n",
      "        mean=[\n",
      "            124.508,\n",
      "            116.05,\n",
      "            106.438,\n",
      "        ],\n",
      "        std=[\n",
      "            58.577,\n",
      "            57.31,\n",
      "            57.437,\n",
      "        ],\n",
      "        to_rgb=False,\n",
      "        type='Normalize'),\n",
      "    dict(keys=[\n",
      "        'img',\n",
      "    ], type='ImageToTensor'),\n",
      "    dict(keys=[\n",
      "        'gt_label',\n",
      "    ], type='ToTensor'),\n",
      "    dict(keys=[\n",
      "        'img',\n",
      "        'gt_label',\n",
      "    ], type='Collect'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "train_cfg = dict(by_epoch=True, max_epochs=100, val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_size=16,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/vgg_heat/train',\n",
      "        data_root='',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=224, type='RandomResizedCrop'),\n",
      "            dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "            dict(\n",
      "                mean=[\n",
      "                    124.508,\n",
      "                    116.05,\n",
      "                    106.438,\n",
      "                ],\n",
      "                std=[\n",
      "                    58.577,\n",
      "                    57.31,\n",
      "                    57.437,\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                type='Normalize'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "            ], type='ImageToTensor'),\n",
      "            dict(keys=[\n",
      "                'gt_label',\n",
      "            ], type='ToTensor'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "                'gt_label',\n",
      "            ], type='Collect'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='train',\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(scale=224, type='RandomResizedCrop'),\n",
      "    dict(direction='horizontal', prob=0.5, type='RandomFlip'),\n",
      "    dict(\n",
      "        mean=[\n",
      "            124.508,\n",
      "            116.05,\n",
      "            106.438,\n",
      "        ],\n",
      "        std=[\n",
      "            58.577,\n",
      "            57.31,\n",
      "            57.437,\n",
      "        ],\n",
      "        to_rgb=False,\n",
      "        type='Normalize'),\n",
      "    dict(keys=[\n",
      "        'img',\n",
      "    ], type='ImageToTensor'),\n",
      "    dict(keys=[\n",
      "        'gt_label',\n",
      "    ], type='ToTensor'),\n",
      "    dict(keys=[\n",
      "        'img',\n",
      "        'gt_label',\n",
      "    ], type='Collect'),\n",
      "    dict(type='PackInputs'),\n",
      "]\n",
      "val_cfg = dict()\n",
      "val_dataloader = dict(\n",
      "    batch_size=32,\n",
      "    collate_fn=dict(type='default_collate'),\n",
      "    dataset=dict(\n",
      "        classes='data/classes.txt',\n",
      "        data_prefix='data/vgg_heat/validation',\n",
      "        data_root='',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(scale=244, type='ResizeEdge'),\n",
      "            dict(crop_size=244, type='CenterCrop'),\n",
      "            dict(\n",
      "                mean=[\n",
      "                    124.508,\n",
      "                    116.05,\n",
      "                    106.438,\n",
      "                ],\n",
      "                std=[\n",
      "                    58.577,\n",
      "                    57.31,\n",
      "                    57.437,\n",
      "                ],\n",
      "                to_rgb=False,\n",
      "                type='Normalize'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "            ], type='ImageToTensor'),\n",
      "            dict(keys=[\n",
      "                'gt_label',\n",
      "            ], type='ToTensor'),\n",
      "            dict(keys=[\n",
      "                'img',\n",
      "                'gt_label',\n",
      "            ], type='Collect'),\n",
      "            dict(type='PackInputs'),\n",
      "        ],\n",
      "        split='val',\n",
      "        type='CustomDataset'),\n",
      "    num_workers=1,\n",
      "    persistent_workers=True,\n",
      "    pin_memory=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(topk=(1, ), type='Accuracy')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='UniversalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = 'work_dirs/alzheimer/axial/vgg16_heat'\n",
      "\n",
      "/home/user/miniconda3/envs/open-mmlab/lib/python3.8/site-packages/mmcv/cnn/bricks/conv_module.py:207: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) VisualizationHook                  \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.0.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.0.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.0.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.1.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.1.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.1.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.3.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.3.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.3.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.4.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.4.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.4.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.6.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.6.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.6.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.7.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.7.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.7.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.8.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.8.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.8.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.10.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.10.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.10.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.11.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.11.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.11.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.12.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.12.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.12.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.14.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.14.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.14.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.15.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.15.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.15.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.16.conv.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.16.bn.weight:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.features.16.bn.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.0.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.3.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - paramwise_options -- backbone.classifier.6.bias:weight_decay=0.0\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - LR is set based on batch size of 128 and the current batch size is 16. Scaling the original LR by 0.125.\n",
      "11/16 17:20:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - load backbone in model from: https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmclassification/v0/vgg/vgg16_bn_batch256_imagenet_20210208-7e55cd29.pth\n",
      "11/16 17:20:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for classifier.6.weight: copying a param with shape torch.Size([1000, 4096]) from checkpoint, the shape in current model is torch.Size([3, 4096]).\n",
      "size mismatch for classifier.6.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "11/16 17:20:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "11/16 17:20:23 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "11/16 17:20:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/user/mmpretrain/work_dirs/alzheimer/axial/vgg16_heat.\n",
      "11/16 17:20:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [1][100/150]  base_lr: 1.3304e-04 lr: 1.6631e-05  eta: 0:23:06  time: 0.0830  data_time: 0.0006  memory: 4400  loss: 1.0948\n",
      "11/16 17:20:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:20:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "11/16 17:20:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][17/17]    accuracy/top1: 31.8182  data_time: 0.0962  time: 0.1501\n",
      "11/16 17:21:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [2][100/150]  base_lr: 3.3311e-04 lr: 4.1639e-05  eta: 0:26:53  time: 0.0946  data_time: 0.0134  memory: 4400  loss: 1.1083\n",
      "11/16 17:21:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:21:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "11/16 17:21:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][17/17]    accuracy/top1: 34.0909  data_time: 0.0610  time: 0.1136\n",
      "11/16 17:21:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [3][100/150]  base_lr: 5.3318e-04 lr: 6.6647e-05  eta: 0:27:12  time: 0.0950  data_time: 0.0138  memory: 4400  loss: 1.1111\n",
      "11/16 17:21:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:21:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 3 epochs\n",
      "11/16 17:21:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [3][17/17]    accuracy/top1: 34.0909  data_time: 0.0505  time: 0.1030\n",
      "11/16 17:22:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [4][100/150]  base_lr: 7.3324e-04 lr: 9.1656e-05  eta: 0:27:35  time: 0.1095  data_time: 0.0285  memory: 4400  loss: 1.1133\n",
      "11/16 17:22:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:22:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 4 epochs\n",
      "11/16 17:22:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [4][17/17]    accuracy/top1: 34.0909  data_time: 0.0467  time: 0.0996\n",
      "11/16 17:22:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [5][100/150]  base_lr: 9.3331e-04 lr: 1.1666e-04  eta: 0:27:37  time: 0.0896  data_time: 0.0080  memory: 4400  loss: 1.1038\n",
      "11/16 17:22:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:22:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 5 epochs\n",
      "11/16 17:22:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [5][17/17]    accuracy/top1: 34.0909  data_time: 0.0479  time: 0.1004\n",
      "11/16 17:23:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [6][100/150]  base_lr: 1.0000e-03 lr: 1.2500e-04  eta: 0:27:25  time: 0.0855  data_time: 0.0039  memory: 4400  loss: 1.0995\n",
      "11/16 17:23:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:23:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 6 epochs\n",
      "11/16 17:23:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [6][17/17]    accuracy/top1: 34.0909  data_time: 0.0651  time: 0.1178\n",
      "11/16 17:23:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:23:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [7][100/150]  base_lr: 9.9997e-04 lr: 1.2500e-04  eta: 0:27:05  time: 0.0821  data_time: 0.0003  memory: 4400  loss: 1.1043\n",
      "11/16 17:23:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:23:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 7 epochs\n",
      "11/16 17:23:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [7][17/17]    accuracy/top1: 34.0909  data_time: 0.0503  time: 0.1029\n",
      "11/16 17:24:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [8][100/150]  base_lr: 9.9989e-04 lr: 1.2499e-04  eta: 0:26:49  time: 0.0877  data_time: 0.0063  memory: 4400  loss: 1.0873\n",
      "11/16 17:24:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:24:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 8 epochs\n",
      "11/16 17:24:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [8][17/17]    accuracy/top1: 31.8182  data_time: 0.0567  time: 0.1090\n",
      "11/16 17:24:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)   [9][100/150]  base_lr: 9.9975e-04 lr: 1.2497e-04  eta: 0:26:41  time: 0.0915  data_time: 0.0103  memory: 4400  loss: 1.1022\n",
      "11/16 17:24:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:24:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 9 epochs\n",
      "11/16 17:24:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [9][17/17]    accuracy/top1: 34.0909  data_time: 0.0652  time: 0.1181\n",
      "11/16 17:25:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [10][100/150]  base_lr: 9.9955e-04 lr: 1.2495e-04  eta: 0:26:34  time: 0.1095  data_time: 0.0282  memory: 4400  loss: 1.1037\n",
      "11/16 17:25:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:25:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 10 epochs\n",
      "11/16 17:25:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [10][17/17]    accuracy/top1: 34.0909  data_time: 0.0498  time: 0.1023\n",
      "11/16 17:25:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [11][100/150]  base_lr: 9.9930e-04 lr: 1.2492e-04  eta: 0:26:15  time: 0.0849  data_time: 0.0031  memory: 4400  loss: 1.0997\n",
      "11/16 17:25:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:25:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 11 epochs\n",
      "11/16 17:25:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [11][17/17]    accuracy/top1: 34.0909  data_time: 0.0599  time: 0.1128\n",
      "11/16 17:26:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [12][100/150]  base_lr: 9.9899e-04 lr: 1.2488e-04  eta: 0:26:00  time: 0.1052  data_time: 0.0238  memory: 4400  loss: 1.0964\n",
      "11/16 17:26:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:26:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 12 epochs\n",
      "11/16 17:26:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [12][17/17]    accuracy/top1: 34.0909  data_time: 0.0611  time: 0.1136\n",
      "11/16 17:26:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [13][100/150]  base_lr: 9.9863e-04 lr: 1.2484e-04  eta: 0:25:46  time: 0.0850  data_time: 0.0031  memory: 4400  loss: 1.0961\n",
      "11/16 17:26:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:26:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 13 epochs\n",
      "11/16 17:26:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [13][17/17]    accuracy/top1: 34.0909  data_time: 0.0503  time: 0.1028\n",
      "11/16 17:26:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:27:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [14][100/150]  base_lr: 9.9820e-04 lr: 1.2479e-04  eta: 0:25:29  time: 0.0865  data_time: 0.0047  memory: 4400  loss: 1.1012\n",
      "11/16 17:27:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:27:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 14 epochs\n",
      "11/16 17:27:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [14][17/17]    accuracy/top1: 34.0909  data_time: 0.0680  time: 0.1204\n",
      "11/16 17:27:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [15][100/150]  base_lr: 9.9773e-04 lr: 1.2474e-04  eta: 0:25:13  time: 0.0830  data_time: 0.0010  memory: 4400  loss: 1.1068\n",
      "11/16 17:27:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:27:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 15 epochs\n",
      "11/16 17:27:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [15][17/17]    accuracy/top1: 34.0909  data_time: 0.0636  time: 0.1161\n",
      "11/16 17:28:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [16][100/150]  base_lr: 9.9720e-04 lr: 1.2467e-04  eta: 0:24:53  time: 0.0824  data_time: 0.0004  memory: 4400  loss: 1.1147\n",
      "11/16 17:28:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:28:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 16 epochs\n",
      "11/16 17:28:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [16][17/17]    accuracy/top1: 31.8182  data_time: 0.0670  time: 0.1195\n",
      "11/16 17:28:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [17][100/150]  base_lr: 9.9661e-04 lr: 1.2461e-04  eta: 0:24:32  time: 0.0848  data_time: 0.0031  memory: 4400  loss: 1.0954\n",
      "11/16 17:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:28:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 17 epochs\n",
      "11/16 17:28:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [17][17/17]    accuracy/top1: 34.0909  data_time: 0.0618  time: 0.1141\n",
      "11/16 17:28:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [18][100/150]  base_lr: 9.9596e-04 lr: 1.2453e-04  eta: 0:24:14  time: 0.1005  data_time: 0.0193  memory: 4400  loss: 1.0976\n",
      "11/16 17:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:29:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 18 epochs\n",
      "11/16 17:29:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [18][17/17]    accuracy/top1: 34.0909  data_time: 0.0531  time: 0.1057\n",
      "11/16 17:29:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [19][100/150]  base_lr: 9.9526e-04 lr: 1.2445e-04  eta: 0:23:57  time: 0.0914  data_time: 0.0099  memory: 4400  loss: 1.0997\n",
      "11/16 17:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:29:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 19 epochs\n",
      "11/16 17:29:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [19][17/17]    accuracy/top1: 34.0909  data_time: 0.0587  time: 0.1112\n",
      "11/16 17:29:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [20][100/150]  base_lr: 9.9451e-04 lr: 1.2436e-04  eta: 0:23:39  time: 0.0916  data_time: 0.0102  memory: 4400  loss: 1.1040\n",
      "11/16 17:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:30:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 20 epochs\n",
      "11/16 17:30:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [20][17/17]    accuracy/top1: 34.0909  data_time: 0.0523  time: 0.1046\n",
      "11/16 17:30:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [21][100/150]  base_lr: 9.9370e-04 lr: 1.2427e-04  eta: 0:23:23  time: 0.0904  data_time: 0.0089  memory: 4400  loss: 1.1142\n",
      "11/16 17:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:30:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 21 epochs\n",
      "11/16 17:30:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [21][17/17]    accuracy/top1: 34.0909  data_time: 0.0431  time: 0.0955\n",
      "11/16 17:30:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [22][100/150]  base_lr: 9.9283e-04 lr: 1.2417e-04  eta: 0:23:05  time: 0.0869  data_time: 0.0058  memory: 4400  loss: 1.1013\n",
      "11/16 17:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:31:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 22 epochs\n",
      "11/16 17:31:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [22][17/17]    accuracy/top1: 34.0909  data_time: 0.0531  time: 0.1055\n",
      "11/16 17:31:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [23][100/150]  base_lr: 9.9191e-04 lr: 1.2406e-04  eta: 0:22:48  time: 0.0960  data_time: 0.0151  memory: 4400  loss: 1.1098\n",
      "11/16 17:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:31:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 23 epochs\n",
      "11/16 17:31:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [23][17/17]    accuracy/top1: 34.0909  data_time: 0.0542  time: 0.1065\n",
      "11/16 17:31:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [24][100/150]  base_lr: 9.9093e-04 lr: 1.2395e-04  eta: 0:22:28  time: 0.0821  data_time: 0.0005  memory: 4400  loss: 1.0999\n",
      "11/16 17:31:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:31:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 24 epochs\n",
      "11/16 17:32:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [24][17/17]    accuracy/top1: 34.0909  data_time: 0.0521  time: 0.1049\n",
      "11/16 17:32:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [25][100/150]  base_lr: 9.8990e-04 lr: 1.2383e-04  eta: 0:22:09  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1002\n",
      "11/16 17:32:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:32:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 25 epochs\n",
      "11/16 17:32:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [25][17/17]    accuracy/top1: 34.0909  data_time: 0.0449  time: 0.0971\n",
      "11/16 17:32:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [26][100/150]  base_lr: 9.8881e-04 lr: 1.2370e-04  eta: 0:21:48  time: 0.0820  data_time: 0.0003  memory: 4400  loss: 1.1100\n",
      "11/16 17:32:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:32:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 26 epochs\n",
      "11/16 17:33:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [26][17/17]    accuracy/top1: 34.0909  data_time: 0.0668  time: 0.1191\n",
      "11/16 17:33:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:33:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [27][100/150]  base_lr: 9.8767e-04 lr: 1.2357e-04  eta: 0:21:27  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1003\n",
      "11/16 17:33:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:33:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 27 epochs\n",
      "11/16 17:33:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [27][17/17]    accuracy/top1: 34.0909  data_time: 0.0387  time: 0.0912\n",
      "11/16 17:33:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [28][100/150]  base_lr: 9.8648e-04 lr: 1.2343e-04  eta: 0:21:08  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1056\n",
      "11/16 17:33:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:33:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 28 epochs\n",
      "11/16 17:34:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [28][17/17]    accuracy/top1: 34.0909  data_time: 0.0445  time: 0.0970\n",
      "11/16 17:34:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [29][100/150]  base_lr: 9.8523e-04 lr: 1.2328e-04  eta: 0:20:50  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0983\n",
      "11/16 17:34:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:34:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 29 epochs\n",
      "11/16 17:34:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [29][17/17]    accuracy/top1: 34.0909  data_time: 0.0656  time: 0.1178\n",
      "11/16 17:34:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [30][100/150]  base_lr: 9.8392e-04 lr: 1.2313e-04  eta: 0:20:30  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1051\n",
      "11/16 17:34:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:34:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 30 epochs\n",
      "11/16 17:35:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [30][17/17]    accuracy/top1: 34.0909  data_time: 0.0532  time: 0.1056\n",
      "11/16 17:35:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [31][100/150]  base_lr: 9.8256e-04 lr: 1.2297e-04  eta: 0:20:11  time: 0.0822  data_time: 0.0004  memory: 4400  loss: 1.1007\n",
      "11/16 17:35:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:35:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 31 epochs\n",
      "11/16 17:35:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [31][17/17]    accuracy/top1: 34.0909  data_time: 0.0551  time: 0.1077\n",
      "11/16 17:35:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [32][100/150]  base_lr: 9.8115e-04 lr: 1.2281e-04  eta: 0:19:51  time: 0.0823  data_time: 0.0008  memory: 4400  loss: 1.0958\n",
      "11/16 17:35:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:35:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 32 epochs\n",
      "11/16 17:35:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [32][17/17]    accuracy/top1: 34.0909  data_time: 0.0474  time: 0.0997\n",
      "11/16 17:36:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [33][100/150]  base_lr: 9.7968e-04 lr: 1.2264e-04  eta: 0:19:33  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0986\n",
      "11/16 17:36:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:36:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 33 epochs\n",
      "11/16 17:36:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [33][17/17]    accuracy/top1: 34.0909  data_time: 0.0647  time: 0.1170\n",
      "11/16 17:36:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:36:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [34][100/150]  base_lr: 9.7816e-04 lr: 1.2246e-04  eta: 0:19:13  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0963\n",
      "11/16 17:36:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:36:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 34 epochs\n",
      "11/16 17:36:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [34][17/17]    accuracy/top1: 34.0909  data_time: 0.0706  time: 0.1229\n",
      "11/16 17:37:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [35][100/150]  base_lr: 9.7658e-04 lr: 1.2228e-04  eta: 0:18:54  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0934\n",
      "11/16 17:37:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:37:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 35 epochs\n",
      "11/16 17:37:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [35][17/17]    accuracy/top1: 34.0909  data_time: 0.0477  time: 0.1002\n",
      "11/16 17:37:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [36][100/150]  base_lr: 9.7495e-04 lr: 1.2209e-04  eta: 0:18:36  time: 0.0822  data_time: 0.0004  memory: 4400  loss: 1.1010\n",
      "11/16 17:37:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:37:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 36 epochs\n",
      "11/16 17:37:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [36][17/17]    accuracy/top1: 34.0909  data_time: 0.0515  time: 0.1040\n",
      "11/16 17:38:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [37][100/150]  base_lr: 9.7327e-04 lr: 1.2189e-04  eta: 0:18:18  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0995\n",
      "11/16 17:38:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:38:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 37 epochs\n",
      "11/16 17:38:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [37][17/17]    accuracy/top1: 34.0909  data_time: 0.0592  time: 0.1117\n",
      "11/16 17:38:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [38][100/150]  base_lr: 9.7153e-04 lr: 1.2169e-04  eta: 0:18:00  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.1001\n",
      "11/16 17:38:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:38:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 38 epochs\n",
      "11/16 17:38:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [38][17/17]    accuracy/top1: 34.0909  data_time: 0.0473  time: 0.0998\n",
      "11/16 17:38:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [39][100/150]  base_lr: 9.6975e-04 lr: 1.2149e-04  eta: 0:17:41  time: 0.0823  data_time: 0.0007  memory: 4400  loss: 1.0961\n",
      "11/16 17:39:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:39:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 39 epochs\n",
      "11/16 17:39:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [39][17/17]    accuracy/top1: 34.0909  data_time: 0.0494  time: 0.1021\n",
      "11/16 17:39:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [40][100/150]  base_lr: 9.6790e-04 lr: 1.2127e-04  eta: 0:17:24  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0989\n",
      "11/16 17:39:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:39:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 40 epochs\n",
      "11/16 17:39:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [40][17/17]    accuracy/top1: 34.0909  data_time: 0.0531  time: 0.1055\n",
      "11/16 17:39:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [41][100/150]  base_lr: 9.6601e-04 lr: 1.2105e-04  eta: 0:17:06  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.1027\n",
      "11/16 17:39:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:39:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 41 epochs\n",
      "11/16 17:40:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [41][17/17]    accuracy/top1: 31.8182  data_time: 0.0430  time: 0.0954\n",
      "11/16 17:40:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [42][100/150]  base_lr: 9.6407e-04 lr: 1.2083e-04  eta: 0:16:47  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0938\n",
      "11/16 17:40:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:40:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 42 epochs\n",
      "11/16 17:40:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [42][17/17]    accuracy/top1: 31.8182  data_time: 0.0511  time: 0.1037\n",
      "11/16 17:40:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [43][100/150]  base_lr: 9.6207e-04 lr: 1.2059e-04  eta: 0:16:29  time: 0.0847  data_time: 0.0035  memory: 4400  loss: 1.0994\n",
      "11/16 17:40:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:40:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 43 epochs\n",
      "11/16 17:41:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [43][17/17]    accuracy/top1: 31.8182  data_time: 0.0374  time: 0.0902\n",
      "11/16 17:41:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [44][100/150]  base_lr: 9.6002e-04 lr: 1.2036e-04  eta: 0:16:11  time: 0.0822  data_time: 0.0005  memory: 4400  loss: 1.1031\n",
      "11/16 17:41:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:41:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 44 epochs\n",
      "11/16 17:41:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [44][17/17]    accuracy/top1: 31.8182  data_time: 0.0444  time: 0.0975\n",
      "11/16 17:41:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [45][100/150]  base_lr: 9.5792e-04 lr: 1.2011e-04  eta: 0:15:54  time: 0.0830  data_time: 0.0015  memory: 4400  loss: 1.0990\n",
      "11/16 17:41:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:41:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 45 epochs\n",
      "11/16 17:42:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [45][17/17]    accuracy/top1: 31.8182  data_time: 0.0481  time: 0.1005\n",
      "11/16 17:42:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [46][100/150]  base_lr: 9.5576e-04 lr: 1.1986e-04  eta: 0:15:36  time: 0.0823  data_time: 0.0005  memory: 4400  loss: 1.0993\n",
      "11/16 17:42:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:42:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 46 epochs\n",
      "11/16 17:42:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [46][17/17]    accuracy/top1: 31.8182  data_time: 0.0498  time: 0.1024\n",
      "11/16 17:42:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:42:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [47][100/150]  base_lr: 9.5356e-04 lr: 1.1961e-04  eta: 0:15:18  time: 0.0821  data_time: 0.0005  memory: 4400  loss: 1.0965\n",
      "11/16 17:42:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:42:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 47 epochs\n",
      "11/16 17:43:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [47][17/17]    accuracy/top1: 31.8182  data_time: 0.0476  time: 0.1000\n",
      "11/16 17:43:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [48][100/150]  base_lr: 9.5131e-04 lr: 1.1934e-04  eta: 0:15:01  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0988\n",
      "11/16 17:43:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:43:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 48 epochs\n",
      "11/16 17:43:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [48][17/17]    accuracy/top1: 34.0909  data_time: 0.0534  time: 0.1060\n",
      "11/16 17:43:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [49][100/150]  base_lr: 9.4900e-04 lr: 1.1908e-04  eta: 0:14:43  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0990\n",
      "11/16 17:43:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:43:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 49 epochs\n",
      "11/16 17:43:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [49][17/17]    accuracy/top1: 34.0909  data_time: 0.0473  time: 0.0996\n",
      "11/16 17:44:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [50][100/150]  base_lr: 9.4664e-04 lr: 1.1880e-04  eta: 0:14:26  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0958\n",
      "11/16 17:44:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:44:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 50 epochs\n",
      "11/16 17:44:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [50][17/17]    accuracy/top1: 34.0909  data_time: 0.0468  time: 0.0991\n",
      "11/16 17:44:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [51][100/150]  base_lr: 9.4424e-04 lr: 1.1852e-04  eta: 0:14:08  time: 0.0822  data_time: 0.0005  memory: 4400  loss: 1.1012\n",
      "11/16 17:44:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:44:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 51 epochs\n",
      "11/16 17:44:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [51][17/17]    accuracy/top1: 34.0909  data_time: 0.0527  time: 0.1051\n",
      "11/16 17:45:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [52][100/150]  base_lr: 9.4178e-04 lr: 1.1824e-04  eta: 0:13:50  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1029\n",
      "11/16 17:45:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:45:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 52 epochs\n",
      "11/16 17:45:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [52][17/17]    accuracy/top1: 34.0909  data_time: 0.0453  time: 0.0975\n",
      "11/16 17:45:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [53][100/150]  base_lr: 9.3928e-04 lr: 1.1795e-04  eta: 0:13:33  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.1005\n",
      "11/16 17:45:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:45:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 53 epochs\n",
      "11/16 17:45:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [53][17/17]    accuracy/top1: 34.0909  data_time: 0.0456  time: 0.0982\n",
      "11/16 17:45:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:46:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [54][100/150]  base_lr: 9.3672e-04 lr: 1.1765e-04  eta: 0:13:15  time: 0.0822  data_time: 0.0004  memory: 4400  loss: 1.1016\n",
      "11/16 17:46:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:46:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 54 epochs\n",
      "11/16 17:46:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [54][17/17]    accuracy/top1: 34.0909  data_time: 0.0421  time: 0.0944\n",
      "11/16 17:46:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [55][100/150]  base_lr: 9.3412e-04 lr: 1.1735e-04  eta: 0:12:57  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0967\n",
      "11/16 17:46:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:46:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 55 epochs\n",
      "11/16 17:46:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [55][17/17]    accuracy/top1: 34.0909  data_time: 0.0467  time: 0.0994\n",
      "11/16 17:46:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [56][100/150]  base_lr: 9.3147e-04 lr: 1.1704e-04  eta: 0:12:40  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0987\n",
      "11/16 17:47:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:47:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 56 epochs\n",
      "11/16 17:47:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [56][17/17]    accuracy/top1: 34.0909  data_time: 0.0539  time: 0.1063\n",
      "11/16 17:47:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [57][100/150]  base_lr: 9.2877e-04 lr: 1.1673e-04  eta: 0:12:22  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1000\n",
      "11/16 17:47:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:47:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 57 epochs\n",
      "11/16 17:47:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [57][17/17]    accuracy/top1: 34.0909  data_time: 0.0404  time: 0.0931\n",
      "11/16 17:47:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [58][100/150]  base_lr: 9.2602e-04 lr: 1.1641e-04  eta: 0:12:05  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0984\n",
      "11/16 17:47:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:47:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 58 epochs\n",
      "11/16 17:48:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [58][17/17]    accuracy/top1: 34.0909  data_time: 0.0508  time: 0.1032\n",
      "11/16 17:48:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [59][100/150]  base_lr: 9.2322e-04 lr: 1.1608e-04  eta: 0:11:48  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.1003\n",
      "11/16 17:48:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:48:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 59 epochs\n",
      "11/16 17:48:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [59][17/17]    accuracy/top1: 34.0909  data_time: 0.0514  time: 0.1037\n",
      "11/16 17:48:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [60][100/150]  base_lr: 9.2038e-04 lr: 1.1575e-04  eta: 0:11:30  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0991\n",
      "11/16 17:48:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:48:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 60 epochs\n",
      "11/16 17:49:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [60][17/17]    accuracy/top1: 34.0909  data_time: 0.0356  time: 0.0880\n",
      "11/16 17:49:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [61][100/150]  base_lr: 9.1749e-04 lr: 1.1542e-04  eta: 0:11:13  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0980\n",
      "11/16 17:49:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:49:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 61 epochs\n",
      "11/16 17:49:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [61][17/17]    accuracy/top1: 31.8182  data_time: 0.0528  time: 0.1054\n",
      "11/16 17:49:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [62][100/150]  base_lr: 9.1455e-04 lr: 1.1507e-04  eta: 0:10:56  time: 0.0822  data_time: 0.0004  memory: 4400  loss: 1.0951\n",
      "11/16 17:49:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:49:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 62 epochs\n",
      "11/16 17:50:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [62][17/17]    accuracy/top1: 34.0909  data_time: 0.0598  time: 0.1124\n",
      "11/16 17:50:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [63][100/150]  base_lr: 9.1157e-04 lr: 1.1473e-04  eta: 0:10:38  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0953\n",
      "11/16 17:50:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:50:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 63 epochs\n",
      "11/16 17:50:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [63][17/17]    accuracy/top1: 34.0909  data_time: 0.0469  time: 0.0993\n",
      "11/16 17:50:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [64][100/150]  base_lr: 9.0854e-04 lr: 1.1438e-04  eta: 0:10:21  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0975\n",
      "11/16 17:50:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:50:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 64 epochs\n",
      "11/16 17:51:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [64][17/17]    accuracy/top1: 31.8182  data_time: 0.0615  time: 0.1138\n",
      "11/16 17:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [65][100/150]  base_lr: 9.0546e-04 lr: 1.1402e-04  eta: 0:10:04  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.0995\n",
      "11/16 17:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 65 epochs\n",
      "11/16 17:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [65][17/17]    accuracy/top1: 34.0909  data_time: 0.0447  time: 0.0972\n",
      "11/16 17:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [66][100/150]  base_lr: 9.0234e-04 lr: 1.1366e-04  eta: 0:09:46  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1064\n",
      "11/16 17:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 66 epochs\n",
      "11/16 17:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [66][17/17]    accuracy/top1: 34.0909  data_time: 0.0485  time: 0.1013\n",
      "11/16 17:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [67][100/150]  base_lr: 8.9918e-04 lr: 1.1329e-04  eta: 0:09:29  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1015\n",
      "11/16 17:52:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:52:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 67 epochs\n",
      "11/16 17:52:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [67][17/17]    accuracy/top1: 34.0909  data_time: 0.0491  time: 0.1017\n",
      "11/16 17:52:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [68][100/150]  base_lr: 8.9597e-04 lr: 1.1292e-04  eta: 0:09:12  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0998\n",
      "11/16 17:52:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:52:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 68 epochs\n",
      "11/16 17:52:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [68][17/17]    accuracy/top1: 31.8182  data_time: 0.0398  time: 0.0924\n",
      "11/16 17:53:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [69][100/150]  base_lr: 8.9271e-04 lr: 1.1254e-04  eta: 0:08:55  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0966\n",
      "11/16 17:53:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:53:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 69 epochs\n",
      "11/16 17:53:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [69][17/17]    accuracy/top1: 34.0909  data_time: 0.0648  time: 0.1173\n",
      "11/16 17:53:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [70][100/150]  base_lr: 8.8941e-04 lr: 1.1215e-04  eta: 0:08:38  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0974\n",
      "11/16 17:53:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:53:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 70 epochs\n",
      "11/16 17:53:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [70][17/17]    accuracy/top1: 31.8182  data_time: 0.0635  time: 0.1160\n",
      "11/16 17:54:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [71][100/150]  base_lr: 8.8607e-04 lr: 1.1177e-04  eta: 0:08:20  time: 0.0822  data_time: 0.0005  memory: 4400  loss: 1.1001\n",
      "11/16 17:54:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:54:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 71 epochs\n",
      "11/16 17:54:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [71][17/17]    accuracy/top1: 31.8182  data_time: 0.0571  time: 0.1096\n",
      "11/16 17:54:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [72][100/150]  base_lr: 8.8268e-04 lr: 1.1137e-04  eta: 0:08:03  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0985\n",
      "11/16 17:54:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:54:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 72 epochs\n",
      "11/16 17:54:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [72][17/17]    accuracy/top1: 34.0909  data_time: 0.0492  time: 0.1018\n",
      "11/16 17:55:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [73][100/150]  base_lr: 8.7925e-04 lr: 1.1097e-04  eta: 0:07:46  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1072\n",
      "11/16 17:55:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:55:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 73 epochs\n",
      "11/16 17:55:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [73][17/17]    accuracy/top1: 34.0909  data_time: 0.0461  time: 0.0987\n",
      "11/16 17:55:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:55:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [74][100/150]  base_lr: 8.7578e-04 lr: 1.1057e-04  eta: 0:07:29  time: 0.0821  data_time: 0.0004  memory: 4400  loss: 1.1042\n",
      "11/16 17:55:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:55:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 74 epochs\n",
      "11/16 17:55:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [74][17/17]    accuracy/top1: 34.0909  data_time: 0.0504  time: 0.1026\n",
      "11/16 17:56:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [75][100/150]  base_lr: 8.7227e-04 lr: 1.1016e-04  eta: 0:07:12  time: 0.0822  data_time: 0.0004  memory: 4400  loss: 1.0998\n",
      "11/16 17:56:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:56:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 75 epochs\n",
      "11/16 17:56:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [75][17/17]    accuracy/top1: 34.0909  data_time: 0.0435  time: 0.0960\n",
      "11/16 17:56:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [76][100/150]  base_lr: 8.6871e-04 lr: 1.0975e-04  eta: 0:06:55  time: 0.0820  data_time: 0.0005  memory: 4400  loss: 1.1024\n",
      "11/16 17:56:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:56:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 76 epochs\n",
      "11/16 17:56:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [76][17/17]    accuracy/top1: 31.8182  data_time: 0.0543  time: 0.1065\n",
      "11/16 17:56:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [77][100/150]  base_lr: 8.6512e-04 lr: 1.0933e-04  eta: 0:06:37  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1033\n",
      "11/16 17:57:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:57:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 77 epochs\n",
      "11/16 17:57:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [77][17/17]    accuracy/top1: 31.8182  data_time: 0.0476  time: 0.1000\n",
      "11/16 17:57:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [78][100/150]  base_lr: 8.6148e-04 lr: 1.0891e-04  eta: 0:06:20  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0976\n",
      "11/16 17:57:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:57:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 78 epochs\n",
      "11/16 17:57:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [78][17/17]    accuracy/top1: 31.8182  data_time: 0.0502  time: 0.1030\n",
      "11/16 17:57:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [79][100/150]  base_lr: 8.5780e-04 lr: 1.0848e-04  eta: 0:06:03  time: 0.0818  data_time: 0.0004  memory: 4400  loss: 1.1014\n",
      "11/16 17:58:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:58:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 79 epochs\n",
      "11/16 17:58:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [79][17/17]    accuracy/top1: 31.8182  data_time: 0.0436  time: 0.0961\n",
      "11/16 17:58:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [80][100/150]  base_lr: 8.5408e-04 lr: 1.0805e-04  eta: 0:05:46  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0997\n",
      "11/16 17:58:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:58:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 80 epochs\n",
      "11/16 17:58:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [80][17/17]    accuracy/top1: 31.8182  data_time: 0.0516  time: 0.1039\n",
      "11/16 17:58:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [81][100/150]  base_lr: 8.5033e-04 lr: 1.0761e-04  eta: 0:05:29  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0982\n",
      "11/16 17:58:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:58:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 81 epochs\n",
      "11/16 17:59:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [81][17/17]    accuracy/top1: 31.8182  data_time: 0.0465  time: 0.0988\n",
      "11/16 17:59:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [82][100/150]  base_lr: 8.4653e-04 lr: 1.0717e-04  eta: 0:05:12  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0999\n",
      "11/16 17:59:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:59:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 82 epochs\n",
      "11/16 17:59:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [82][17/17]    accuracy/top1: 31.8182  data_time: 0.0346  time: 0.0870\n",
      "11/16 17:59:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [83][100/150]  base_lr: 8.4270e-04 lr: 1.0673e-04  eta: 0:04:55  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0976\n",
      "11/16 17:59:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 17:59:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 83 epochs\n",
      "11/16 18:00:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [83][17/17]    accuracy/top1: 31.8182  data_time: 0.0266  time: 0.0796\n",
      "11/16 18:00:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [84][100/150]  base_lr: 8.3882e-04 lr: 1.0628e-04  eta: 0:04:38  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.0971\n",
      "11/16 18:00:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:00:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 84 epochs\n",
      "11/16 18:00:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [84][17/17]    accuracy/top1: 31.8182  data_time: 0.0392  time: 0.0924\n",
      "11/16 18:00:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [85][100/150]  base_lr: 8.3491e-04 lr: 1.0582e-04  eta: 0:04:21  time: 0.0818  data_time: 0.0004  memory: 4400  loss: 1.0987\n",
      "11/16 18:00:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:00:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 85 epochs\n",
      "11/16 18:01:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [85][17/17]    accuracy/top1: 34.0909  data_time: 0.0605  time: 0.1129\n",
      "11/16 18:01:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [86][100/150]  base_lr: 8.3096e-04 lr: 1.0536e-04  eta: 0:04:04  time: 0.0818  data_time: 0.0004  memory: 4400  loss: 1.1021\n",
      "11/16 18:01:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:01:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 86 epochs\n",
      "11/16 18:01:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [86][17/17]    accuracy/top1: 31.8182  data_time: 0.0469  time: 0.0992\n",
      "11/16 18:01:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:01:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [87][100/150]  base_lr: 8.2698e-04 lr: 1.0490e-04  eta: 0:03:47  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0999\n",
      "11/16 18:01:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:01:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 87 epochs\n",
      "11/16 18:02:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [87][17/17]    accuracy/top1: 31.8182  data_time: 0.0571  time: 0.1097\n",
      "11/16 18:02:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [88][100/150]  base_lr: 8.2296e-04 lr: 1.0443e-04  eta: 0:03:29  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1013\n",
      "11/16 18:02:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:02:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 88 epochs\n",
      "11/16 18:02:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [88][17/17]    accuracy/top1: 31.8182  data_time: 0.0454  time: 0.0984\n",
      "11/16 18:02:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [89][100/150]  base_lr: 8.1890e-04 lr: 1.0396e-04  eta: 0:03:12  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0982\n",
      "11/16 18:02:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:02:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 89 epochs\n",
      "11/16 18:02:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [89][17/17]    accuracy/top1: 31.8182  data_time: 0.0405  time: 0.0929\n",
      "11/16 18:03:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [90][100/150]  base_lr: 8.1480e-04 lr: 1.0349e-04  eta: 0:02:55  time: 0.0818  data_time: 0.0004  memory: 4400  loss: 1.1017\n",
      "11/16 18:03:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:03:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 90 epochs\n",
      "11/16 18:03:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [90][17/17]    accuracy/top1: 34.0909  data_time: 0.0217  time: 0.0744\n",
      "11/16 18:03:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [91][100/150]  base_lr: 8.1067e-04 lr: 1.0301e-04  eta: 0:02:38  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0992\n",
      "11/16 18:03:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:03:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 91 epochs\n",
      "11/16 18:03:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [91][17/17]    accuracy/top1: 34.0909  data_time: 0.0370  time: 0.0896\n",
      "11/16 18:04:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [92][100/150]  base_lr: 8.0651e-04 lr: 1.0252e-04  eta: 0:02:21  time: 0.0819  data_time: 0.0005  memory: 4400  loss: 1.0970\n",
      "11/16 18:04:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:04:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 92 epochs\n",
      "11/16 18:04:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [92][17/17]    accuracy/top1: 31.8182  data_time: 0.0225  time: 0.0750\n",
      "11/16 18:04:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [93][100/150]  base_lr: 8.0231e-04 lr: 1.0204e-04  eta: 0:02:04  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0948\n",
      "11/16 18:04:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:04:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 93 epochs\n",
      "11/16 18:04:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [93][17/17]    accuracy/top1: 34.0909  data_time: 0.0458  time: 0.0985\n",
      "11/16 18:04:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:04:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [94][100/150]  base_lr: 7.9808e-04 lr: 1.0154e-04  eta: 0:01:47  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1005\n",
      "11/16 18:05:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:05:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 94 epochs\n",
      "11/16 18:05:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [94][17/17]    accuracy/top1: 34.0909  data_time: 0.0408  time: 0.0936\n",
      "11/16 18:05:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [95][100/150]  base_lr: 7.9382e-04 lr: 1.0105e-04  eta: 0:01:30  time: 0.0819  data_time: 0.0005  memory: 4400  loss: 1.1019\n",
      "11/16 18:05:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:05:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 95 epochs\n",
      "11/16 18:05:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [95][17/17]    accuracy/top1: 34.0909  data_time: 0.0383  time: 0.0908\n",
      "11/16 18:05:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [96][100/150]  base_lr: 7.8952e-04 lr: 1.0055e-04  eta: 0:01:13  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.1007\n",
      "11/16 18:05:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:05:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 96 epochs\n",
      "11/16 18:06:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [96][17/17]    accuracy/top1: 34.0909  data_time: 0.0207  time: 0.0734\n",
      "11/16 18:06:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [97][100/150]  base_lr: 7.8519e-04 lr: 1.0005e-04  eta: 0:00:56  time: 0.0818  data_time: 0.0004  memory: 4400  loss: 1.1008\n",
      "11/16 18:06:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:06:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 97 epochs\n",
      "11/16 18:06:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [97][17/17]    accuracy/top1: 34.0909  data_time: 0.0367  time: 0.0894\n",
      "11/16 18:06:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [98][100/150]  base_lr: 7.8083e-04 lr: 9.9541e-05  eta: 0:00:39  time: 0.0819  data_time: 0.0004  memory: 4400  loss: 1.0983\n",
      "11/16 18:06:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:06:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 98 epochs\n",
      "11/16 18:06:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [98][17/17]    accuracy/top1: 34.0909  data_time: 0.0318  time: 0.0842\n",
      "11/16 18:07:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [99][100/150]  base_lr: 7.7644e-04 lr: 9.9030e-05  eta: 0:00:22  time: 0.0820  data_time: 0.0004  memory: 4400  loss: 1.1002\n",
      "11/16 18:07:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:07:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 99 epochs\n",
      "11/16 18:07:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [99][17/17]    accuracy/top1: 34.0909  data_time: 0.0413  time: 0.0938\n",
      "11/16 18:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [100][100/150]  base_lr: 7.7201e-04 lr: 9.8517e-05  eta: 0:00:05  time: 0.0820  data_time: 0.0005  memory: 4400  loss: 1.0980\n",
      "11/16 18:07:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: vgg16_alzheimer_axial_view_heat_20231116_172013\n",
      "11/16 18:07:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 100 epochs\n",
      "11/16 18:07:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [100][17/17]    accuracy/top1: 34.0909  data_time: 0.0312  time: 0.0841\n"
     ]
    }
   ],
   "source": [
    "!python ./tools/train.py \\\n",
    "    configs/vgg/vgg16_alzheimer_axial_view_heat.py \\\n",
    "    --work-dir work_dirs/alzheimer/axial/vgg16_heat \\\n",
    "    --auto-scale-lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b62ec3-c7ad-4036-9ac8-ea5adb04706f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494cc9f2-522d-4601-a733-1eecbaef1d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
